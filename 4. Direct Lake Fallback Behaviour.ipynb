{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3061fb6e-1484-4072-af99-c85b1dd9ae60",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 4: Direct Lake Fallback Behaviour\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lab explores how Direct Lake protects system stability when queries exceed capacity limits. You will observe when and why Direct Lake automatically switches to the SQL Endpoint (fallback), compare the three execution modes (Automatic, DirectLakeOnly, DirectQueryOnly), and learn to diagnose fallback scenarios using trace analysis.\n",
    "\n",
    "### Workshop Flow\n",
    "\n",
    "1. Connect to the big data environment from Lab 2\n",
    "2. Run a query in Automatic mode and observe fallback to the SQL Endpoint\n",
    "3. Switch to DirectLakeOnly mode and observe query failure when limits are exceeded\n",
    "4. Restore Automatic mode and compare performance characteristics\n",
    "5. Review trace output to understand execution mode decisions\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Automatic mode** lets Direct Lake fall back to the SQL Endpoint when guardrails are exceeded, ensuring queries always return results\n",
    "- **DirectLakeOnly mode** disables fallback, meaning queries fail if they exceed capacity limits\n",
    "- **DirectQueryOnly mode** forces all queries through the SQL Endpoint regardless of data size\n",
    "- **Trace analysis** reveals which execution path a query took and its timing characteristics\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand the conditions that trigger fallback from Direct Lake to the SQL Endpoint\n",
    "- Configure and compare the three execution modes\n",
    "- Use trace events to diagnose execution mode decisions\n",
    "- Determine the appropriate mode for different production scenarios\n",
    "\n",
    "**Prerequisites:** Lab 2 completed (big data lakehouse and semantic model with billion-row tables)\n",
    "\n",
    "---\n",
    "\n",
    "*Deutsche Version:*\n",
    "\n",
    "# Lab 4: Direct Lake Fallback-Verhalten\n",
    "\n",
    "## Uebersicht\n",
    "\n",
    "Dieses Lab untersucht, wie Direct Lake die Systemstabilitaet schuetzt, wenn Abfragen die Kapazitaetsgrenzen ueberschreiten. Sie beobachten, wann und warum Direct Lake automatisch zum SQL Endpoint wechselt (Fallback), vergleichen die drei Ausfuehrungsmodi (Automatic, DirectLakeOnly, DirectQueryOnly) und lernen, Fallback-Szenarien mithilfe der Trace-Analyse zu diagnostizieren.\n",
    "\n",
    "### Wichtige Konzepte\n",
    "\n",
    "- **Automatic-Modus** laesst Direct Lake zum SQL Endpoint zurueckfallen, wenn Guardrails ueberschritten werden\n",
    "- **DirectLakeOnly-Modus** deaktiviert den Fallback; Abfragen schlagen fehl, wenn Grenzen ueberschritten werden\n",
    "- **DirectQueryOnly-Modus** erzwingt alle Abfragen ueber den SQL Endpoint\n",
    "- **Trace-Analyse** zeigt, welchen Ausfuehrungspfad eine Abfrage genommen hat\n",
    "\n",
    "**Voraussetzungen:** Lab 2 abgeschlossen (Big-Data-Lakehouse mit Milliarden-Zeilen-Tabellen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f49793d-8e6c-4850-bdd3-7ed9a85ea337",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Prerequisites and Lab Dependencies\n",
    "\n",
    "This lab requires the infrastructure from Lab 2:\n",
    "\n",
    "- **BigData lakehouse** with OneLake shortcuts to the billion-row tables\n",
    "- **BigData_model semantic model** with relationships and measures configured\n",
    "- **Billion-row tables** (fact_myevents_1bln and fact_myevents_2bln) that naturally push against guardrail limits\n",
    "\n",
    "The large table sizes are essential because they create realistic conditions for observing fallback behaviour. The lab follows a structured sequence: establish baseline behaviour, intentionally trigger fallback, test each execution mode, and then analyse the results.\n",
    "\n",
    "---\n",
    "\n",
    "*Dieses Lab erfordert die Infrastruktur aus Lab 2: BigData Lakehouse mit OneLake-Shortcuts, das konfigurierte Semantic Model und die Milliarden-Zeilen-Tabellen. Die grossen Tabellengroessen sind wesentlich, da sie realistische Bedingungen fuer die Beobachtung des Fallback-Verhaltens schaffen.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e5ade-d660-4f81-ad51-3389bfdff8e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "Install the Semantic Link Labs library for semantic model configuration and trace analysis.\n",
    "\n",
    "---\n",
    "\n",
    "*Installieren Sie die Semantic Link Labs-Bibliothek fuer die Konfiguration von Semantic Models und die Trace-Analyse.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b8ffa9-7858-421b-b9c4-e0ea54e6e8f1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2882579-4bac-4b21-99f6-f7b1bb020235",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Configure Environment\n",
    "\n",
    "Import libraries and establish connection to the big data lakehouse and semantic model from Lab 2. This validates that all required artifacts are accessible.\n",
    "\n",
    "---\n",
    "\n",
    "*Importieren Sie Bibliotheken und stellen Sie die Verbindung zum Big-Data-Lakehouse und Semantic Model aus Lab 2 her. Dies validiert, dass alle erforderlichen Artefakte zugaenglich sind.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3dc908-415a-4f5a-83c9-7d15693fb363",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as labs\n",
    "from sempy import fabric\n",
    "import sempy\n",
    "\n",
    "LakehouseName = \"BigData\"\n",
    "lakehouses = labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "for l in lakehouses:\n",
    "    if l.startswith(\"Big\"):\n",
    "        LakehouseName = l\n",
    "\n",
    "SemanticModelName = f\"{LakehouseName}_model\"\n",
    "\n",
    "lakehouses=labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "if LakehouseName in lakehouses.values:\n",
    "    lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]\n",
    "else:\n",
    "    print(\"You need to complete Lab 2 to create the required lakehouse for this lab\")\n",
    "\n",
    "workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "workspaceName = sempy.fabric.resolve_workspace_name(workspaceId)\n",
    "print(f\"WorkspaceId = {workspaceId}, LakehouseID = {lakehouseId}, Workspace Name = {workspaceName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2215d098-a518-442e-9f42-ff0bfac5c586",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Create Trace Function for Fallback Analysis\n",
    "\n",
    "Define a tracing function that captures query execution events, including whether Direct Lake mode was used or whether the query fell back to the SQL Endpoint.\n",
    "\n",
    "---\n",
    "\n",
    "*Definieren Sie eine Tracing-Funktion, die Abfrageausfuehrungsereignisse erfasst, einschliesslich ob der Direct Lake-Modus verwendet wurde oder ob die Abfrage zum SQL Endpoint zurueckgefallen ist.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39653352-568d-4144-88d4-9393f8e4a6b3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "import pandas\n",
    "\n",
    "#### Generate Unique Trace Name - Start ####\n",
    "import json, base64, re\n",
    "token = notebookutils.credentials.getToken(\"pbi\")\n",
    "payload = token.split(\".\")[1]\n",
    "payload += \"=\" * (4 - len(payload) % 4)\n",
    "upn = json.loads(base64.b64decode(payload)).get(\"upn\")\n",
    "\n",
    "# Extract just the user part (e.g. \"SQLKDL.user39\")\n",
    "user_id = upn.split(\"@\")[0]\n",
    "lab_number = 4  # set per lab\n",
    "\n",
    "# Remove characters not allowed in trace names: . , ; ' ` : / \\ * | ? \" & % $ ! + = ( ) [ ] { } < >\n",
    "user_id_clean = re.sub(r\"[.,;'`:/\\\\*|?\\\"&%$!+=(){}\\[\\]<>]\", \"_\", user_id)\n",
    "trace_name = f\"Lab{lab_number}_{user_id_clean}\"\n",
    "#### Generate Unique Trace Name - End ####\n",
    "\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)\n",
    "\n",
    "def filter_func(e):\n",
    "    retVal:bool=True\n",
    "    if e.EventSubclass.ToString() == \"VertiPaqScanInternal\":\n",
    "        retVal=False      \n",
    "    #     #if e.EventSubClass.ToString() == \"VertiPaqScanInternal\":\n",
    "    #     retVal=False\n",
    "    return retVal\n",
    "\n",
    "# define events to trace and their corresponding columns\n",
    "def runQueryWithTrace (expr:str,workspaceName:str,SemanticModelName:str,Result:Optional[bool]=True,Trace:Optional[bool]=True,DMV:Optional[bool]=True,ClearCache:Optional[bool]=True) -> pandas.DataFrame :\n",
    "    event_schema = fabric.Trace.get_default_query_trace_schema()\n",
    "    event_schema.update({\"ExecutionMetrics\":[\"EventClass\",\"TextData\"]})\n",
    "    del event_schema['VertiPaqSEQueryBegin']\n",
    "    del event_schema['VertiPaqSEQueryCacheMatch']\n",
    "    del event_schema['DirectQueryBegin']\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    WorkspaceName = workspaceName\n",
    "    SemanticModelName = SemanticModelName\n",
    "\n",
    "    if ClearCache:\n",
    "        labs.clear_cache(SemanticModelName)\n",
    "\n",
    "    with fabric.create_trace_connection(SemanticModelName,WorkspaceName) as trace_connection:\n",
    "        # create trace on server with specified events\n",
    "        with trace_connection.create_trace(\n",
    "            event_schema=event_schema, \n",
    "            name=trace_name,\n",
    "            filter_predicate=filter_func,\n",
    "            stop_event=\"QueryEnd\"\n",
    "            ) as trace:\n",
    "\n",
    "            trace.start()\n",
    "\n",
    "            df=sempy.fabric.evaluate_dax(\n",
    "                dataset=SemanticModelName, \n",
    "                dax_string=expr)\n",
    "\n",
    "            if Result:\n",
    "                displayHTML(f\"<H2>####### DAX QUERY RESULT #######</H2>\")\n",
    "                display(df)\n",
    "\n",
    "            # Wait 5 seconds for trace data to arrive\n",
    "            time.sleep(5)\n",
    "\n",
    "            # stop Trace and collect logs\n",
    "            final_trace_logs = trace.stop()\n",
    "\n",
    "    if Trace:\n",
    "        displayHTML(f\"<H2>####### SERVER TIMINGS #######</H2>\")\n",
    "        display(final_trace_logs)\n",
    "    \n",
    "    if DMV:\n",
    "        displayHTML(f\"<H2>####### SHOW DMV RESULTS #######</H2>\")\n",
    "        runDMV()\n",
    "\n",
    "    return final_trace_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4343642-bc69-4bcb-98f8-a5b3305cf1dd",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5dad7-c8b4-4b96-8974-63ba5f5adfbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 4: Demonstrate Fallback in Automatic Mode\n",
    "\n",
    "Execute a query against a billion-row table in Automatic mode. If the query exceeds Direct Lake guardrails, Direct Lake will automatically fall back to the SQL Endpoint to ensure the query completes successfully.\n",
    "\n",
    "---\n",
    "\n",
    "*Fuehren Sie eine Abfrage gegen eine Milliarden-Zeilen-Tabelle im Automatic-Modus aus. Wenn die Abfrage die Direct Lake-Guardrails ueberschreitet, faellt Direct Lake automatisch zum SQL Endpoint zurueck, um sicherzustellen, dass die Abfrage erfolgreich abgeschlossen wird.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98a66e-a7e3-4300-b094-b69405ac01f3",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "trace1 = runQueryWithTrace(\n",
    "    \"\"\"\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "                dim_Date[FirstDateofMonth] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(fact_myevents_1bln) ,\n",
    "                \"Sum of Sales (1bln)\" , [Sum of Sales (1bln)] ,\n",
    "                \"Sum of Sales (2bln)\" , [Sum of Sales (2bln)]\n",
    "        )\n",
    "        ORDER BY [FirstDateofMonth]\n",
    "    \"\"\" , workspaceName , SemanticModelName\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb53a945-6d3d-4fda-b69e-f6b8cd897b1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 5: Switch to DirectLakeOnly Mode\n",
    "\n",
    "Update the semantic model to DirectLakeOnly mode. In this mode, queries that exceed guardrails will fail rather than falling back to the SQL Endpoint.\n",
    "\n",
    "---\n",
    "\n",
    "*Stellen Sie das Semantic Model auf den DirectLakeOnly-Modus um. In diesem Modus schlagen Abfragen, die Guardrails ueberschreiten, fehl, anstatt zum SQL Endpoint zurueckzufallen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec808ab-a554-4d3c-a28d-56238929e7ad",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "tom = labs.tom.TOMWrapper(dataset=SemanticModelName, workspace=workspaceName, readonly=False)\n",
    "tom.set_direct_lake_behavior(\"DirectLakeOnly\") ##  Can be set to any of ['Automatic', 'DirectLakeOnly', 'DirectQueryOnly'].\n",
    "tom.model.SaveChanges()\n",
    "print(\"Model changed\")\n",
    "fabric.refresh_dataset(refresh_type=\"calculate\",dataset=SemanticModelName)\n",
    "print(\"Model recalculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f661bdb2-1903-4ef6-b766-30b73811d726",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 6: Test Query Failure in DirectLakeOnly Mode\n",
    "\n",
    "Attempt the same billion-row query in DirectLakeOnly mode. This is expected to fail with an error. Read the error message carefully, as it explains the specific guardrail that was exceeded.\n",
    "\n",
    "---\n",
    "\n",
    "*Versuchen Sie dieselbe Milliarden-Zeilen-Abfrage im DirectLakeOnly-Modus. Dies wird voraussichtlich mit einem Fehler fehlschlagen. Lesen Sie die Fehlermeldung sorgfaeltig, da sie die spezifische ueberschrittene Guardrail erklaert.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc20540-3924-43b4-944e-c93e93c43452",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sempy import fabric\n",
    "x = sempy.fabric._client._adomd_connection.FabricAdomdException\n",
    "try:\n",
    "    runQueryWithTrace(\n",
    "        \"\"\"\n",
    "        EVALUATE\n",
    "            SUMMARIZECOLUMNS(\n",
    "                    dim_Date[FirstDateofMonth] ,\n",
    "                    \"Count of Transactions\" , COUNTROWS(fact_myevents_1bln) ,\n",
    "                    \"Sum of Sales (1bln)\" , [Sum of Sales (1bln)] ,\n",
    "                    \"Sum of Sales (2bln)\" , [Sum of Sales (2bln)]\n",
    "            )\n",
    "            ORDER BY [FirstDateofMonth]\n",
    "        \"\"\" , workspaceName , SemanticModelName\n",
    "    )\n",
    "except sempy.fabric._client._adomd_connection.FabricAdomdException as f:\n",
    "    print(f)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643db15-d7a7-4327-b561-e4fac6be6b1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 7: Restore Automatic Mode\n",
    "\n",
    "Switch the semantic model back to Automatic mode, which is the recommended setting for production workloads. This re-enables the intelligent fallback mechanism.\n",
    "\n",
    "---\n",
    "\n",
    "*Stellen Sie das Semantic Model zurueck auf den Automatic-Modus, der fuer Produktionsworkloads empfohlen wird. Dies aktiviert den intelligenten Fallback-Mechanismus erneut.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21878901-92d5-4942-93d6-8f4a32bb1f95",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "tom = labs.tom.TOMWrapper(dataset=SemanticModelName, workspace=workspaceName, readonly=False)\n",
    "tom.set_direct_lake_behavior(\"Automatic\") ##  ['Automatic', 'DirectLakeOnly', 'DirectQueryOnly'].\n",
    "tom.model.SaveChanges()\n",
    "print(\"Model changed\")\n",
    "fabric.refresh_dataset(refresh_type=\"calculate\",dataset=SemanticModelName)\n",
    "print(\"Model recalculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1439dd-d98e-4ec9-92c8-d553c94091b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 8: Run Query with Automatic Fallback Enabled\n",
    "\n",
    "Run a query against the 2 billion row table with Automatic mode restored. The query should complete without error, but check the trace output for a DirectQueryEnd event indicating that fallback to the SQL Endpoint occurred.\n",
    "\n",
    "---\n",
    "\n",
    "*Fuehren Sie eine Abfrage gegen die 2-Milliarden-Zeilen-Tabelle mit wiederhergestelltem Automatic-Modus aus. Die Abfrage sollte fehlerfrei abgeschlossen werden, aber pruefen Sie die Trace-Ausgabe auf ein DirectQueryEnd-Ereignis, das auf einen Fallback zum SQL Endpoint hinweist.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e727b-ebbe-481d-91be-893593089cc0",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "trace2 = runQueryWithTrace(\n",
    "    \"\"\"\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "                dim_Date[FirstDateofMonth] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(fact_myevents_1bln) ,\n",
    "                \"Sum of Sales (1bln)\" , [Sum of Sales (1bln)] ,\n",
    "                \"Sum of Sales (2bln)\" , [Sum of Sales (2bln)]\n",
    "        )\n",
    "        ORDER BY [FirstDateofMonth]\n",
    "    \"\"\" , workspaceName , SemanticModelName, Trace=True, DMV=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d83d43",
   "metadata": {},
   "source": [
    "## Step 9: Stop the Spark Session\n",
    "\n",
    "---\n",
    "\n",
    "*Spark-Sitzung beenden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca18ae",
   "metadata": {},
   "source": [
    "## Lab 4 Summary\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "- **Observed fallback:** Ran queries that triggered automatic fallback from Direct Lake to the SQL Endpoint\n",
    "- **Tested execution modes:** Compared Automatic, DirectLakeOnly, and DirectQueryOnly behaviour\n",
    "- **Validated protection:** Confirmed that DirectLakeOnly mode prevents queries from exceeding memory limits by failing rather than falling back\n",
    "- **Analysed trace output:** Used trace events to identify which execution path each query took\n",
    "\n",
    "### Execution Mode Reference\n",
    "\n",
    "| Mode | Behaviour | Recommended Use |\n",
    "|------|-----------|-----------------|\n",
    "| **Automatic** | Falls back to SQL Endpoint when limits exceeded | Production workloads |\n",
    "| **DirectLakeOnly** | Fails with error when limits exceeded | Performance testing and validation |\n",
    "| **DirectQueryOnly** | Always uses SQL Endpoint | Troubleshooting and comparison |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Automatic mode is the recommended default because it ensures queries always return results, even when the data exceeds Direct Lake guardrails\n",
    "- DirectLakeOnly mode is useful for testing whether a specific query can run entirely in Direct Lake\n",
    "- Trace events provide clear evidence of the execution path, making it straightforward to diagnose fallback\n",
    "- The fallback mechanism is a protective feature that maintains system stability under heavy load\n",
    "\n",
    "### Troubleshooting Workflow\n",
    "\n",
    "1. Use DMVs or trace events to detect whether fallback occurred\n",
    "2. Review the error or trace message to understand the specific guardrail that was exceeded\n",
    "3. Consider optimising the query, the model, or the data to reduce memory demand\n",
    "4. Validate improvements by running the query in DirectLakeOnly mode\n",
    "5. Deploy in Automatic mode for production reliability\n",
    "\n",
    "### Next Lab\n",
    "\n",
    "Continue to **Lab 5** to learn about framing and how Direct Lake synchronises with data changes.\n",
    "\n",
    "---\n",
    "\n",
    "*Deutsche Version:*\n",
    "\n",
    "### Was Sie erreicht haben\n",
    "\n",
    "- **Fallback beobachtet:** Abfragen ausgefuehrt, die den automatischen Fallback von Direct Lake zum SQL Endpoint ausgeloest haben\n",
    "- **Ausfuehrungsmodi getestet:** Automatic, DirectLakeOnly und DirectQueryOnly verglichen\n",
    "- **Schutz validiert:** Bestaetigt, dass DirectLakeOnly-Modus Abfragen am Ueberschreiten von Speichergrenzen hindert\n",
    "- **Trace-Ausgabe analysiert:** Trace-Ereignisse verwendet, um den Ausfuehrungspfad jeder Abfrage zu identifizieren\n",
    "\n",
    "### Wichtige Erkenntnisse\n",
    "\n",
    "- Automatic-Modus ist die empfohlene Standardeinstellung, da Abfragen immer Ergebnisse liefern\n",
    "- DirectLakeOnly-Modus ist nuetzlich zum Testen, ob eine Abfrage vollstaendig im Direct Lake laufen kann\n",
    "- Trace-Ereignisse liefern klare Nachweise des Ausfuehrungspfads\n",
    "- Der Fallback-Mechanismus ist eine Schutzfunktion, die die Systemstabilitaet bei hoher Last aufrechterhaelt\n",
    "\n",
    "### Naechstes Lab\n",
    "\n",
    "Weiter zu **Lab 5**, um Framing und die Synchronisation von Direct Lake mit Datenaenderungen zu lernen."
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
