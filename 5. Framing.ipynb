{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccff19e-c4d4-47d8-b723-103d305a08e0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Goal is to understand framing\n",
    "- Auto framing on/off\n",
    "- Monitor Parquet files after changes\n",
    "- Effect of Overwrite\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef57c0b5-52a1-4a68-816b-a2273b66e688",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Install Semantic Link Labs Python Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cae2d1-1076-45ce-8653-885e75cb12f4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#%pip uninstall -y -q \"builtin/semantic_link_labs-0.9.3-py3-none-any.whl\"\n",
    "#%pip install      -q \"builtin/semantic_link_labs-0.9.3-py3-none-any.whl\"\n",
    "%pip install -q --disable-pip-version-check semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cf5f0-e43b-4875-a665-f5b69e8858f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get Lakehouse and Workspace Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe88dfa-77dc-4763-b330-40921e35ab32",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as labs\n",
    "\n",
    "LakehouseName = \"AdventureWorks\"\n",
    "SemanticModelName = f\"{LakehouseName}_model\"\n",
    "workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110caf3b-b01a-4003-9e0b-71c6e320cb59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Define function to display Framing Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70acccb6-0f67-4fc7-aa0f-39cc09878972",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import pyarrow.parquet as pq\n",
    "from sempy_labs._helper_functions import (\n",
    "    create_abfss_path,\n",
    "    save_as_delta_table,\n",
    "    _get_column_aggregate,\n",
    "    _create_dataframe,\n",
    "    _update_dataframe_datatypes,\n",
    "    resolve_workspace_name_and_id,\n",
    "    resolve_lakehouse_name_and_id,\n",
    "    _read_delta_table,\n",
    "    _delta_table_row_count,\n",
    "    _mount,\n",
    "    _create_spark_session,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from uuid import UUID\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def delta_analyzer_history(\n",
    "    table_name: str,\n",
    "    schema: Optional[str] = None,\n",
    "    lakehouse: Optional[str | UUID] = None,\n",
    "    workspace: Optional[str | UUID] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyzes the transaction log for a specified delta table and shows the results in dataframe.  One row per data modification operation.\n",
    "\n",
    "    Keeps track on the number of Parquet files, rowgroups, file size and #rows impacted by each change.\n",
    "\n",
    "    Incremental Framing effect: 100% = highly effective, 0% = no benefit at all\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_name : str\n",
    "        The delta table name.\n",
    "    schema : str, default=None\n",
    "        The schema name of the delta table.\n",
    "    lakehouse : str | uuid.UUID, default=None\n",
    "        The Fabric lakehouse name or ID.\n",
    "        Defaults to None which resolves to the lakehouse attached to the notebook.\n",
    "    workspace : str | uuid.UUID, default=None\n",
    "        The Fabric workspace name or ID used by the lakehouse.\n",
    "        Defaults to None which resolves to the workspace of the attached lakehouse\n",
    "        or if no lakehouse attached, resolves to the workspace of the notebook.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Displays a gantt visual showing a timeline for individual parquet files.\n",
    "    \"\"\"\n",
    "\n",
    "    import notebookutils\n",
    "\n",
    "    (workspace_name, workspace_id) = resolve_workspace_name_and_id(workspace=workspace)\n",
    "    (lakehouse_name, lakehouse_id) = resolve_lakehouse_name_and_id(\n",
    "        lakehouse=lakehouse, workspace=workspace\n",
    "    )\n",
    "\n",
    "    table_path = create_abfss_path(lakehouse_id, workspace_id, table_name, schema)\n",
    "    local_path = _mount(lakehouse=lakehouse, workspace=workspace)\n",
    "    table_path_local = f\"{local_path}/Tables/{table_name}\"\n",
    "    delta_table_path = f\"{table_path}/_delta_log\"\n",
    "\n",
    "    files = notebookutils.fs.ls(delta_table_path)\n",
    "    json_files = [file.name for file in files if file.name.endswith(\".json\")]\n",
    "\n",
    "    elementVersion = 0\n",
    "    totalSize: int = 0\n",
    "    totalRows: int = 0\n",
    "    totalFiles: int = 0\n",
    "    totalRowgroups: int = 0\n",
    "\n",
    "    changesArray = []\n",
    "    parquetFiles = []\n",
    "\n",
    "    myDateTimeFormat = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "\n",
    "    nowToEpoch = datetime.now().strftime(myDateTimeFormat)\n",
    "\n",
    "    num_latest_files = len(json_files)\n",
    "    for idx, file in enumerate(bar := tqdm(json_files), start=1):\n",
    "\n",
    "        bar.set_description(\n",
    "            f\"Analyzing the '{file}' parquet file ({idx}/{num_latest_files})...\"\n",
    "        )\n",
    "\n",
    "        changeTimestamp = datetime.strptime(\"2001-01-01 12:00:00.000\", myDateTimeFormat)\n",
    "        df = pd.read_json(\n",
    "            f\"{table_path}/_delta_log/{file}\", lines=True\n",
    "        )\n",
    "\n",
    "        rowsAdded: int = 0\n",
    "        sizeAdded: int = 0\n",
    "        rowsDeleted: int = 0\n",
    "        sizeDeleted: int = 0\n",
    "        filesAdded: int = 0\n",
    "        filesRemoved: int = 0\n",
    "\n",
    "        rowGroupsAdded: int = 0\n",
    "        rowGroupsRemoved: int = 0\n",
    "\n",
    "        totalFilesBeforeChange: int = totalFiles\n",
    "        totalRowGroupsBeforeChange: int = totalRowgroups\n",
    "        operation: str = \"\"\n",
    "        predicate: str = \"\"\n",
    "        tags: str = \"\"\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            if df.get(\"add\") is not None:\n",
    "                add_row = row[\"add\"]\n",
    "\n",
    "                if type(add_row) == dict:\n",
    "\n",
    "                    file_name = add_row[\"path\"]\n",
    "                    sizeAdded = sizeAdded + add_row[\"size\"]\n",
    "                    filesAdded = filesAdded + 1\n",
    "\n",
    "                    fileRowsAdded: int = 0\n",
    "\n",
    "                    fs_filename = f\"{table_path}/{file_name}\"\n",
    "\n",
    "                    if notebookutils.fs.exists(fs_filename):\n",
    "                        # parquet_file = pq.ParquetFile(f\"{table_path}/Tables/{table_name}/{file_name}\")\n",
    "                        parquet_file = pq.ParquetFile(\n",
    "                            table_path_local + f\"/{file_name}\"\n",
    "                        )\n",
    "                        for i in range(parquet_file.num_row_groups):\n",
    "                            row_group = parquet_file.metadata.row_group(i)\n",
    "                            num_rows = row_group.num_rows\n",
    "                            fileRowsAdded = fileRowsAdded + num_rows\n",
    "\n",
    "                            rowsAdded = rowsAdded + num_rows\n",
    "\n",
    "                        rowGroupsAdded = rowGroupsAdded + parquet_file.num_row_groups\n",
    "\n",
    "                        start = str(\n",
    "                            datetime.fromtimestamp(add_row[\"modificationTime\"] / 1000.0)\n",
    "                        )\n",
    "                        parquetFiles.append(\n",
    "                            {\n",
    "                                \"file\": file_name,\n",
    "                                \"start\": start,\n",
    "                                \"end\": nowToEpoch,\n",
    "                                \"rows\": fileRowsAdded,\n",
    "                                \"isCurrent\": 1,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            if df.get(\"remove\") is not None:\n",
    "                remove_row = row[\"remove\"]\n",
    "                if type(remove_row) == dict:\n",
    "                    file_name = remove_row[\"path\"]\n",
    "                    ### CHECK IF FILE EXISTS!!!\n",
    "                    fs_filename = f\"{table_path}/{file_name}\"\n",
    "\n",
    "                    if notebookutils.fs.exists(fs_filename):\n",
    "                        # parquet_file = pq.ParquetFile(f\"{table_path}/{file_name}\")\n",
    "                        parquet_file = pq.ParquetFile(\n",
    "                            table_path_local + f\"/{file_name}\"\n",
    "                        )\n",
    "                        for i in range(parquet_file.num_row_groups):\n",
    "                            row_group = parquet_file.metadata.row_group(i)\n",
    "                            num_rows = row_group.num_rows\n",
    "                            rowsDeleted = rowsDeleted + num_rows\n",
    "\n",
    "                        filesRemoved = filesRemoved + 1\n",
    "                        sizeDeleted = sizeDeleted + remove_row[\"size\"]\n",
    "\n",
    "                        rowGroupsRemoved = (\n",
    "                            rowGroupsRemoved + parquet_file.num_row_groups\n",
    "                        )\n",
    "\n",
    "                        result = next(\n",
    "                            (row for row in parquetFiles if row[\"file\"] == file_name),\n",
    "                            None,\n",
    "                        )\n",
    "                        if result is not None:\n",
    "                            result[\"isCurrent\"] = 0\n",
    "                            result[\"end\"] = str(\n",
    "                                datetime.fromtimestamp(\n",
    "                                    remove_row[\"deletionTimestamp\"] / 1000.0\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "            if df.get(\"commitInfo\") is not None:\n",
    "                commit_row = row[\"commitInfo\"]\n",
    "                if type(commit_row) == dict:\n",
    "                    operation = commit_row[\"operation\"]\n",
    "\n",
    "                    if \"tags\" in commit_row:\n",
    "                        tags = commit_row[\"tags\"]\n",
    "\n",
    "                    if \"operationParameters\" in commit_row:\n",
    "                        operationParameters = commit_row[\"operationParameters\"]\n",
    "                        if \"predicate\" in operationParameters:\n",
    "                            predicate = operationParameters[\"predicate\"]\n",
    "\n",
    "                    if operation == \"VACUUM START\":\n",
    "                        totalFiles = totalFiles - int(\n",
    "                            commit_row[\"operationMetrics\"][\"numFilesToDelete\"]\n",
    "                        )\n",
    "                        totalSize = totalSize - int(\n",
    "                            commit_row[\"operationMetrics\"][\"sizeOfDataToDelete\"]\n",
    "                        )\n",
    "\n",
    "                    changeTimestamp = datetime.fromtimestamp(\n",
    "                        commit_row[\"timestamp\"] / 1000.0\n",
    "                    )\n",
    "\n",
    "        totalSize = totalSize + sizeAdded - sizeDeleted\n",
    "        totalRows = totalRows + rowsAdded - rowsDeleted\n",
    "        totalFiles = totalFiles + filesAdded - filesRemoved\n",
    "        totalRowgroups = totalRowgroups + rowGroupsAdded - rowGroupsRemoved\n",
    "\n",
    "        incrementalFramingEffect = 100\n",
    "        if sizeDeleted != 0:\n",
    "            incrementalFramingEffect = int((totalSize - sizeAdded * 1.0) / totalSize * 100000) / 1000\n",
    "            # incrementalFramingEffect = round(\n",
    "            #     (totalSize - sizeAdded * 1.0) / totalSize, 4\n",
    "            # )\n",
    "\n",
    "        changesArray.append(\n",
    "            [\n",
    "                elementVersion,\n",
    "                operation,\n",
    "                predicate,\n",
    "                changeTimestamp,\n",
    "                incrementalFramingEffect,\n",
    "                filesAdded,\n",
    "                filesRemoved,\n",
    "                totalFilesBeforeChange - filesRemoved,\n",
    "                totalFiles,\n",
    "                sizeAdded,\n",
    "                sizeDeleted,\n",
    "                totalSize,\n",
    "                rowGroupsAdded,\n",
    "                rowGroupsRemoved,\n",
    "                totalRowGroupsBeforeChange - rowGroupsRemoved,\n",
    "                totalRowgroups,\n",
    "                rowsAdded,\n",
    "                rowsDeleted,\n",
    "                rowsAdded - rowsDeleted,\n",
    "                totalRows,\n",
    "                tags,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        elementVersion = elementVersion + 1\n",
    "\n",
    "    #  /********************************************************************************************************************\n",
    "    #      Display Gantt Chart of files\n",
    "    #  ********************************************************************************************************************/\n",
    "    spec: str = (\n",
    "        \"\"\"{\n",
    "    \"$$schema\": 'https://vega.github.io/schema/vega-lite/v2.json',\n",
    "    \"description\": \"A simple bar chart with ranged data (aka Gantt Chart).\",\n",
    "    \"width\" : 1024 ,\n",
    "    \"data\": {\n",
    "        \"values\": %s\n",
    "    },\n",
    "    \"layer\":[\n",
    "        {\"mark\": \"bar\"},\n",
    "        {\"mark\": {\n",
    "        \"type\": \"text\",\n",
    "        \"align\": \"center\",\n",
    "        \"baseline\": \"middle\",\n",
    "        \"dx\": 40\n",
    "        },\n",
    "        \"encoding\": {\n",
    "        \"text\": {\"field\": \"rows\", \"type\": \"quantitative\", \"format\":\",\"},\n",
    "        \"color\":{\n",
    "        \"condition\": {\"test\": \"datum['isCurrent'] == 1\", \"value\": \"black\"},\n",
    "        \"value\": \"black\"\n",
    "            }\n",
    "        }\n",
    "        }],\n",
    "    \"encoding\": {\n",
    "        \"y\": {\"field\": \"file\", \"type\": \"ordinal\",\"sort\": \"isCurrent\",\"title\":null,\"axis\":{\"labelPadding\":15,\"labelLimit\":360}},\n",
    "        \"x\": {\"field\": \"start\", \"type\": \"temporal\",\"title\":null},\n",
    "        \"x2\": {\"field\": \"end\", \"type\": \"temporal\",\"title\":null},\n",
    "            \"color\": {\n",
    "            \"field\": \"isCurrent\",\n",
    "            \"scale\": {\"range\": [\"silver\", \"#ca8861\"]}\n",
    "            }\n",
    "    }\n",
    "    }\"\"\"\n",
    "        % (parquetFiles)\n",
    "    )\n",
    "\n",
    "    display(\n",
    "        HTML(\n",
    "            \"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "            <head>\n",
    "                <script src=\"https://cdn.jsdelivr.net/npm/vega@5\"></script>\n",
    "                <script src=\"https://cdn.jsdelivr.net/npm/vega-lite@5\"></script>\n",
    "                <script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"></script>\n",
    "            </head>\n",
    "            <body>\n",
    "                <div id=\"vis\"></div>\n",
    "                <script type=\"text/javascript\">\n",
    "                    var spec = \"\"\"\n",
    "            + spec\n",
    "            + \"\"\";\n",
    "                    var opt = {\"renderer\": \"canvas\", \"actions\": false};\n",
    "                    vegaEmbed(\"#vis\", spec, opt);\n",
    "                </script>\n",
    "            </body>\n",
    "        </html>\"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    changesDF = pd.DataFrame(\n",
    "        changesArray,\n",
    "        columns=[\n",
    "            \"Change Number\",\n",
    "            \"Change Type\",\n",
    "            \"Predicate\",\n",
    "            \"Modification Time\",\n",
    "            \"Incremental Effect\",\n",
    "            \"Files Added\",\n",
    "            \"Files Removed\",\n",
    "            \"Files Preserved\",\n",
    "            \"Files after change\",\n",
    "            \"Size Added\",\n",
    "            \"Sized Removed\",\n",
    "            \"Size after change\",\n",
    "            \"Rowgroups Added\",\n",
    "            \"Rowgroups Removed\",\n",
    "            \"Rowgroups Preserved\",\n",
    "            \"Rowgroups after change\",\n",
    "            \"Rows Added\",\n",
    "            \"Rows Removed\",\n",
    "            \"Rows Delta\",\n",
    "            \"Rows after change\",\n",
    "            \"Tags\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return changesDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9261c-598d-4591-9a0f-55734693cd0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Show Lakehouse Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b9316-66e3-49dc-ad07-ac29546f76eb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "labs.lakehouse.get_lakehouse_tables(LakehouseName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc59957-8148-44f9-8740-a9486f575ed8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Show history details for **DimDate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbda0ed-b407-4321-a251-666b2ff32be2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "delta_analyzer_history(lakehouse=LakehouseName, table_name=\"DimDate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e120ef6-33e7-4d60-8881-2e4fb7a2fe67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Show history details for **FactInernetSales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff06d8f-5779-44a3-8e91-ea249768ba0a",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "delta_analyzer_history(lakehouse=LakehouseName, table_name=\"FactInternetSales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b408195-a4e9-4cad-87e8-22abd7ddbc53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Insert data to **FactInternetSales** using Append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b66b2-187c-4bc5-802c-17898f171789",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Get one day of data from existing table\n",
    "from pyspark.sql.functions import lit, min, max ,count\n",
    "df1 = spark.read.load(f\"abfss://{workspaceId}@onelake.dfs.fabric.microsoft.com/{lakehouseId}/Tables/FactInternetSales\")\n",
    "\n",
    "# Show Min, MAX and Count of rows\n",
    "df1.agg(\n",
    "    min(\"OrderDateKey\").alias(\"min_OrderDateKey\") ,\n",
    "    max(\"OrderDateKey\").alias(\"max_OrderDateKey\") ,\n",
    "    count(\"*\").alias(\"count_rows\")\n",
    "    ).show()\n",
    "\n",
    "\n",
    "# Create a filtered dataframe to update and then append back onto the original table\n",
    "df2 = df1.filter(\"OrderDateKey='20221204'\")\n",
    "df2 = df2.withColumn(\"OrderDateKey\",lit(20050630))\n",
    "\n",
    "\n",
    "df2.write.mode(\"append\").save(f\"abfss://{workspaceId}@onelake.dfs.fabric.microsoft.com/{lakehouseId}/Tables/FactInternetSales\")\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a80718-6d05-4e5e-9b12-ed4f6f68e2bf",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "delta_analyzer_history(lakehouse=LakehouseName, table_name=\"FactInternetSales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14762a35-868a-477b-8852-1e9059e9a97c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.load(f\"abfss://{workspaceId}@onelake.dfs.fabric.microsoft.com/{lakehouseId}/Tables/FactInternetSales\")\n",
    "\n",
    "# Show Min, MAX and Count of rows\n",
    "df1.agg(\n",
    "        min(\"OrderDateKey\").alias(\"min_OrderDateKey\") ,\n",
    "        max(\"OrderDateKey\").alias(\"max_OrderDateKey\") ,\n",
    "        count(\"*\").alias(\"count_rows\")\n",
    "        ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b0be9b-2097-4a61-88b0-7f743387c957",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load **FactInternetSales** into variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169b7c3-c667-430b-b338-4baee2b63f50",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, f\"abfss://{workspaceId}@onelake.dfs.fabric.microsoft.com/{lakehouseId}/Tables/FactInternetSales\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ae181-a1e1-4001-b419-2c0302688656",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Delete some rows from FactInternetSales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351f70c-d311-43a9-afec-f7b2afd182d4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "deltaTable.delete(\"OrderDateKey = '20050701'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d49b5-7245-4806-b2fa-94e63ffc1fe8",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(delta_analyzer_history(lakehouse=LakehouseName, table_name=\"FactInternetSales\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb786b-07c5-497e-8e1c-95598f32c4ae",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(deltaTable.toDF().groupBy(\"OrderDateKey\").count().sort(\"count\",ascending=False))\n",
    "display(deltaTable.toDF().filter(\"OrderDateKey='20220218'\"))\n",
    "display(deltaTable.toDF())\n",
    "display(deltaTable.history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f2f4d-b56e-4413-ae97-286b85bcd475",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(deltaTable.history())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63034a-240a-4447-8c83-2e2612b177a6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Delete from Partitioned\n",
    "Effect of OPtmise\n",
    "Effect of overwrite? (use copy command from previous)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d5f35-78d2-4d58-93b2-d123a11c5b06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Update all values for 20220218 to be DiscountAmount of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba63b48-38b6-4fbc-94f8-a5141ef6e9eb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "deltaTable.update(\n",
    "    condition= col(\"OrderDateKey\")=='20220218',\n",
    "    set = { \"DiscountAmount\":\"1\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507795e4-b818-4c43-b3fe-bbf2ebe9012e",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(deltaTable.toDF().filter(\"OrderDateKey='20220218'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8421e-ffb5-44b1-88ca-2f7065b88b54",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(deltaTable.restoreToVersion(2))"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
