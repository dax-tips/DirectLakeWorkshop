{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10b5d50-1798-4f0d-acd4-5728791368f7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 6: Advanced Column Partitioning for Direct Lake Performance\n",
    "\n",
    "## Mastering Strategic Data Partitioning for Maximum Query Efficiency\n",
    "\n",
    "### üéØ Advanced Performance Optimization Workshop\n",
    "\n",
    "Welcome to **Lab 6 - Column Partitioning**, an expert-level workshop focused on **strategic data partitioning techniques** that dramatically improve Direct Lake query performance through **intelligent column organization**, **optimized data distribution**, and **advanced partitioning strategies**.\n",
    "\n",
    "#### **Core Learning Objectives:**\n",
    "- üèóÔ∏è **Partitioning Strategy Mastery**: Understanding and implementing advanced column partitioning techniques\n",
    "- ‚ö° **Query Performance Optimization**: Achieving dramatic query response time improvements through strategic partitioning\n",
    "- üìä **Data Distribution Intelligence**: Optimizing data layout for maximum query efficiency\n",
    "- üéØ **Enterprise Partitioning Patterns**: Implementing production-ready partitioning strategies\n",
    "- üöÄ **Performance Monitoring**: Comprehensive monitoring and optimization of partitioned models\n",
    "\n",
    "### **Column Partitioning Fundamentals**\n",
    "\n",
    "#### **What is Strategic Column Partitioning?**\n",
    "**Column partitioning** in Direct Lake is the **strategic organization of data** across multiple partitions based on column values, enabling:\n",
    "- **Query pruning**: Elimination of irrelevant partitions during query execution\n",
    "- **Parallel processing**: Simultaneous processing across multiple partitions\n",
    "- **Memory optimization**: Reduced memory footprint through selective partition loading\n",
    "- **I/O efficiency**: Minimized data scanning through intelligent partition selection\n",
    "\n",
    "#### **Enterprise Partitioning Benefits:**\n",
    "\n",
    "| Performance Area | Optimization Benefit | Typical Improvement | Business Impact |\n",
    "|------------------|---------------------|-------------------|-----------------|\n",
    "| **Query Response Time** | Partition pruning reduces data scanning | 60-90% improvement | Enhanced user experience |\n",
    "| **Memory Utilization** | Selective partition loading | 40-70% reduction | Reduced infrastructure costs |\n",
    "| **Concurrent Performance** | Parallel partition processing | 50-80% improvement | Increased user capacity |\n",
    "| **Large Dataset Handling** | Efficient processing of massive tables | 70-95% improvement | Scalability achievement |\n",
    "\n",
    "### **Advanced Partitioning Challenges Addressed**\n",
    "\n",
    "#### **Enterprise Data Challenges:**\n",
    "- **Massive table scaling**: Handling tables with billions of rows efficiently\n",
    "- **Complex query patterns**: Optimizing for diverse and complex analytical queries\n",
    "- **Mixed workload optimization**: Balancing performance across different query types\n",
    "- **Resource optimization**: Achieving maximum performance with optimal resource utilization\n",
    "\n",
    "#### **Lab Solution Framework:**\n",
    "- **Intelligent partitioning strategies**: Data-driven approaches to optimal partition design\n",
    "- **Performance validation**: Comprehensive testing and validation of partitioning effectiveness\n",
    "- **Monitoring and optimization**: Continuous monitoring and improvement of partitioned models\n",
    "- **Enterprise deployment**: Production-ready partitioning implementation strategies\n",
    "\n",
    "### **Workshop Prerequisites and Environment**\n",
    "\n",
    "#### **Required Knowledge Foundation:**\n",
    "- ‚úÖ **Lab 1-5 completion**: Foundational Direct Lake knowledge and advanced framing techniques\n",
    "- ‚úÖ **Large dataset experience**: Understanding of big data challenges and optimization needs\n",
    "- ‚úÖ **Query performance analysis**: Ability to analyze and optimize query performance\n",
    "- ‚úÖ **Enterprise deployment knowledge**: Understanding of production deployment requirements\n",
    "\n",
    "#### **Technical Environment Setup:**\n",
    "- **Microsoft Fabric workspace** with enterprise-scale data capabilities\n",
    "- **Large datasets** from previous labs (billion-row tables from Lab 2)\n",
    "- **Semantic Link Labs** advanced partitioning functions\n",
    "- **Performance monitoring tools** for partitioning impact analysis\n",
    "- **Delta Lake tables** optimized for partitioning experimentation\n",
    "\n",
    "### **Comprehensive Workshop Journey**\n",
    "\n",
    "This advanced workshop guides you through **12 expert-level sections** covering enterprise-grade column partitioning:\n",
    "\n",
    "1. **üîß Advanced Environment Setup**: Specialized tools for partitioning analysis and optimization\n",
    "2. **üìä Partitioning Strategy Analysis**: Understanding optimal partitioning approaches for different data patterns\n",
    "3. **üèóÔ∏è Partition Design and Implementation**: Creating and implementing strategic partitioning schemes\n",
    "4. **‚ö° Performance Impact Measurement**: Comprehensive analysis of partitioning performance benefits\n",
    "5. **üéØ Query Optimization Validation**: Testing and validating query performance improvements\n",
    "6. **üìà Large-Scale Partitioning**: Advanced techniques for massive dataset partitioning\n",
    "7. **üîÑ Partition Maintenance and Optimization**: Ongoing maintenance and optimization strategies\n",
    "8. **üìä Advanced Partitioning Patterns**: Complex partitioning strategies for specialized use cases\n",
    "9. **üöÄ Enterprise Integration**: Production deployment and enterprise integration strategies\n",
    "10. **üìà Performance Monitoring**: Comprehensive monitoring of partitioned model performance\n",
    "11. **üéØ Optimization and Tuning**: Advanced techniques for partitioning optimization\n",
    "12. **üèÜ Partitioning Mastery**: Final validation and enterprise deployment preparation\n",
    "\n",
    "**Expected Workshop Duration**: 90-120 minutes  \n",
    "**Complexity Level**: Expert  \n",
    "**Real-World Application**: Enterprise-scale Direct Lake performance optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5338115e-f788-4188-b3c6-4451ccf8260a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Advanced Partitioning Environment and Workspace Setup\n",
    "\n",
    "### Specialized Infrastructure for Enterprise-Scale Partitioning Analysis\n",
    "\n",
    "This section establishes the **advanced partitioning environment**, configuring specialized tools and infrastructure necessary for **enterprise-scale column partitioning analysis**, **performance optimization**, and **large dataset management**.\n",
    "\n",
    "#### **Advanced Partitioning Environment Requirements:**\n",
    "- **High-performance compute**: Enhanced processing capability for large dataset partitioning\n",
    "- **Specialized partitioning tools**: Advanced libraries and utilities for partitioning analysis\n",
    "- **Performance monitoring**: Comprehensive tools for measuring partitioning effectiveness\n",
    "- **Large dataset access**: Connection to billion-row datasets from previous labs\n",
    "\n",
    "### **Enterprise Partitioning Infrastructure Setup**\n",
    "\n",
    "#### **Specialized Tool Configuration:**\n",
    "\n",
    "##### **Advanced Partitioning Capabilities:**\n",
    "```python\n",
    "# Specialized partitioning environment setup\n",
    "partitioning_environment = {\n",
    "    'large_dataset_support': True,\n",
    "    'partition_analysis_tools': True,\n",
    "    'performance_monitoring': True,\n",
    "    'memory_optimization': True,\n",
    "    'parallel_processing': True\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Partitioning-Specific Libraries and Tools:**\n",
    "\n",
    "| Tool Category | Capability | Partitioning Application |\n",
    "|---------------|------------|-------------------------|\n",
    "| **Semantic Link Labs** | Advanced Direct Lake operations | Partition creation and management |\n",
    "| **Delta Lake Analytics** | Delta table partitioning analysis | Partition optimization and validation |\n",
    "| **Performance Profiling** | Query and partition performance measurement | Partitioning effectiveness analysis |\n",
    "| **Memory Management** | Advanced memory utilization monitoring | Partition memory impact assessment |\n",
    "\n",
    "### **Workspace Configuration for Partitioning Excellence**\n",
    "\n",
    "#### **Environment Optimization for Large-Scale Partitioning:**\n",
    "\n",
    "##### **1. Compute Resource Optimization:**\n",
    "- **Enhanced memory allocation**: Optimized memory configuration for large dataset partitioning\n",
    "- **Parallel processing enablement**: Configuration for multi-core partition processing\n",
    "- **I/O optimization**: Enhanced storage and network configuration for partition operations\n",
    "- **Cache optimization**: Advanced caching strategies for partitioned data access\n",
    "\n",
    "##### **2. Data Access and Management:**\n",
    "- **Large dataset connectivity**: Access to billion-row datasets from Lab 2\n",
    "- **Cross-workspace integration**: OneLake shortcuts for partitioning experimentation\n",
    "- **Delta Lake optimization**: Advanced Delta table configuration for partitioning\n",
    "- **Metadata management**: Enhanced metadata handling for partitioned tables\n",
    "\n",
    "#### **Advanced Configuration Benefits:**\n",
    "- ‚úÖ **High-performance partitioning**: Optimal environment for large-scale partition operations\n",
    "- ‚úÖ **Comprehensive monitoring**: Full visibility into partitioning performance and effectiveness\n",
    "- ‚úÖ **Scalability preparation**: Infrastructure ready for enterprise-scale partitioning workloads\n",
    "- ‚úÖ **Optimization readiness**: Tools and environment optimized for partitioning experimentation\n",
    "\n",
    "### **Partitioning Workspace Validation**\n",
    "\n",
    "#### **Environment Readiness Verification:**\n",
    "\n",
    "##### **Infrastructure Validation Checklist:**\n",
    "- **üîß Compute capacity**: Sufficient processing power for large dataset partitioning\n",
    "- **üíæ Memory allocation**: Adequate memory for billion-row dataset processing\n",
    "- **üìä Tool availability**: Access to advanced partitioning and monitoring tools\n",
    "- **üîó Data connectivity**: Verified access to large datasets and Delta tables\n",
    "\n",
    "##### **Performance Baseline Establishment:**\n",
    "- **Current performance measurement**: Baseline query performance before partitioning\n",
    "- **Resource utilization assessment**: Current memory and compute consumption patterns\n",
    "- **Query pattern analysis**: Understanding of typical query patterns for optimization\n",
    "- **Bottleneck identification**: Identification of current performance limitations\n",
    "\n",
    "### **Expected Environment Setup Outcomes**\n",
    "\n",
    "#### **Partitioning Readiness Achievement:**\n",
    "After successful environment setup, you'll have:\n",
    "- **üöÄ High-performance infrastructure**: Optimized environment for enterprise-scale partitioning\n",
    "- **üîß Advanced tooling**: Specialized tools for partitioning analysis and optimization\n",
    "- **üìä Comprehensive monitoring**: Full visibility into partitioning performance impact\n",
    "- **üéØ Optimization framework**: Foundation for strategic partitioning implementation\n",
    "\n",
    "#### **Enterprise Preparation:**\n",
    "- **Scalability foundation**: Infrastructure capable of handling enterprise-scale partitioning\n",
    "- **Performance optimization platform**: Environment optimized for partitioning experimentation\n",
    "- **Monitoring integration**: Comprehensive performance monitoring and analysis capabilities\n",
    "- **Production readiness**: Environment configuration suitable for enterprise deployment\n",
    "\n",
    "**Next step**: With the advanced partitioning environment configured, we'll analyze current data patterns to design optimal partitioning strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0d833-91cf-4c94-acc4-17a71e453a13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2. Strategic Data Pattern Analysis for Optimal Partitioning\n",
    "\n",
    "### Intelligent Analysis of Data Characteristics for Partitioning Strategy Design\n",
    "\n",
    "This section conducts **comprehensive data pattern analysis** to understand the **characteristics of large datasets** and identify **optimal partitioning strategies** based on **data distribution**, **query patterns**, and **performance requirements**.\n",
    "\n",
    "#### **Data Pattern Analysis Objectives:**\n",
    "- **Data distribution understanding**: Analyzing how data is distributed across columns and values\n",
    "- **Query pattern identification**: Understanding typical query patterns and filtering behaviors\n",
    "- **Partition key selection**: Identifying optimal columns for partitioning based on data characteristics\n",
    "- **Performance prediction**: Predicting partitioning effectiveness based on data patterns\n",
    "\n",
    "### **Comprehensive Data Characteristics Analysis**\n",
    "\n",
    "#### **Multi-Dimensional Data Analysis Framework:**\n",
    "\n",
    "| Analysis Dimension | Focus Area | Partitioning Insight |\n",
    "|-------------------|------------|---------------------|\n",
    "| **Data Volume Distribution** | Row counts and data density | Partition sizing strategy |\n",
    "| **Column Cardinality** | Unique value distribution | Partition granularity optimization |\n",
    "| **Query Filter Patterns** | Common WHERE clause patterns | Partition key selection |\n",
    "| **Temporal Characteristics** | Time-based data patterns | Time-based partitioning opportunities |\n",
    "\n",
    "#### **Advanced Data Pattern Discovery:**\n",
    "```python\n",
    "# Comprehensive data pattern analysis\n",
    "data_analysis = {\n",
    "    'distribution_analysis': True,\n",
    "    'cardinality_assessment': True,\n",
    "    'query_pattern_identification': True,\n",
    "    'temporal_analysis': True,\n",
    "    'partition_optimization': True\n",
    "}\n",
    "```\n",
    "\n",
    "### **Data Distribution Intelligence**\n",
    "\n",
    "#### **Column-Level Distribution Analysis:**\n",
    "\n",
    "##### **1. Cardinality Assessment:**\n",
    "- **High cardinality columns**: Columns with many unique values (potential partition keys)\n",
    "- **Low cardinality columns**: Columns with few unique values (grouping opportunities)\n",
    "- **Medium cardinality columns**: Balanced distribution for optimal partitioning\n",
    "- **Skewed distribution identification**: Understanding data skew for partition balancing\n",
    "\n",
    "##### **2. Value Distribution Patterns:**\n",
    "- **Uniform distribution**: Even distribution across values (ideal for partitioning)\n",
    "- **Skewed distribution**: Uneven distribution requiring partition balancing strategies\n",
    "- **Temporal distribution**: Time-based patterns for temporal partitioning\n",
    "- **Geographic distribution**: Location-based patterns for geographic partitioning\n",
    "\n",
    "#### **Distribution Analysis Benefits:**\n",
    "- ‚úÖ **Optimal partition key identification**: Data-driven selection of partitioning columns\n",
    "- ‚úÖ **Partition size prediction**: Understanding expected partition sizes and balance\n",
    "- ‚úÖ **Performance impact forecasting**: Predicting query performance improvements\n",
    "- ‚úÖ **Resource requirement estimation**: Understanding memory and compute needs\n",
    "\n",
    "### **Query Pattern Intelligence**\n",
    "\n",
    "#### **Query Behavior Analysis:**\n",
    "\n",
    "##### **1. Filter Pattern Discovery:**\n",
    "- **Common filter columns**: Identifying columns frequently used in WHERE clauses\n",
    "- **Filter selectivity**: Understanding how selective different filters are\n",
    "- **Combined filter patterns**: Analyzing multi-column filter combinations\n",
    "- **Query complexity assessment**: Understanding query complexity for optimization\n",
    "\n",
    "##### **2. Access Pattern Analysis:**\n",
    "- **Temporal access patterns**: Understanding time-based query patterns\n",
    "- **User access patterns**: Analyzing different user groups and their query behaviors\n",
    "- **Report access patterns**: Understanding scheduled report and dashboard access\n",
    "- **Ad-hoc query patterns**: Analyzing exploratory and analytical query patterns\n",
    "\n",
    "#### **Query Pattern Insights:**\n",
    "- **üìä Partition pruning opportunities**: Identifying queries that benefit most from partitioning\n",
    "- **‚ö° Performance optimization potential**: Understanding maximum possible performance gains\n",
    "- **üéØ User impact analysis**: Predicting user experience improvements\n",
    "- **üîÑ Query optimization strategies**: Developing query-specific optimization approaches\n",
    "\n",
    "### **Partitioning Strategy Recommendation Engine**\n",
    "\n",
    "#### **Intelligent Partitioning Strategy Selection:**\n",
    "\n",
    "##### **1. Data-Driven Strategy Selection:**\n",
    "- **Temporal partitioning**: For data with strong time-based access patterns\n",
    "- **Categorical partitioning**: For data with distinct categorical groupings\n",
    "- **Range partitioning**: For numerical data with range-based queries\n",
    "- **Hash partitioning**: For data requiring even distribution across partitions\n",
    "\n",
    "##### **2. Hybrid Partitioning Strategies:**\n",
    "- **Multi-level partitioning**: Combining multiple partitioning approaches\n",
    "- **Dynamic partitioning**: Adapting partitioning based on data growth patterns\n",
    "- **Query-optimized partitioning**: Partitioning specifically optimized for common queries\n",
    "- **Business-aligned partitioning**: Partitioning aligned with business processes\n",
    "\n",
    "#### **Strategy Selection Matrix:**\n",
    "\n",
    "| Data Characteristic | Recommended Strategy | Expected Benefit | Implementation Complexity |\n",
    "|---------------------|---------------------|------------------|---------------------------|\n",
    "| **Time-series data** | Temporal partitioning | 70-90% query improvement | Medium |\n",
    "| **Geographic data** | Location-based partitioning | 60-80% query improvement | Medium |\n",
    "| **Categorical data** | Category-based partitioning | 50-70% query improvement | Low |\n",
    "| **High-volume mixed** | Hybrid partitioning | 40-60% query improvement | High |\n",
    "\n",
    "### **Partition Design Validation**\n",
    "\n",
    "#### **Design Validation Framework:**\n",
    "\n",
    "##### **1. Theoretical Performance Modeling:**\n",
    "- **Partition pruning simulation**: Modeling query performance with proposed partitioning\n",
    "- **Memory impact assessment**: Understanding memory requirements for partitioned models\n",
    "- **Parallel processing analysis**: Assessing parallel processing benefits\n",
    "- **Resource utilization prediction**: Forecasting compute and I/O requirements\n",
    "\n",
    "##### **2. Risk Assessment and Mitigation:**\n",
    "- **Partition skew analysis**: Identifying and mitigating potential partition imbalances\n",
    "- **Query pattern evolution**: Considering how query patterns might change over time\n",
    "- **Data growth impact**: Understanding partitioning effectiveness as data grows\n",
    "- **Maintenance overhead assessment**: Evaluating ongoing maintenance requirements\n",
    "\n",
    "### **Expected Data Analysis Outcomes**\n",
    "\n",
    "#### **Strategic Partitioning Intelligence:**\n",
    "- ‚úÖ **Optimal partition strategy identification**: Data-driven partitioning approach selection\n",
    "- ‚úÖ **Performance improvement prediction**: Accurate forecasting of partitioning benefits\n",
    "- ‚úÖ **Resource requirement estimation**: Understanding infrastructure needs for partitioning\n",
    "- ‚úÖ **Implementation roadmap**: Clear plan for partitioning implementation\n",
    "\n",
    "#### **Enterprise Readiness:**\n",
    "- **Strategic foundation**: Data-driven foundation for enterprise partitioning decisions\n",
    "- **Performance optimization framework**: Systematic approach to partitioning optimization\n",
    "- **Risk mitigation strategy**: Comprehensive understanding of partitioning risks and mitigation\n",
    "- **Scalability preparation**: Partitioning strategy designed for enterprise growth\n",
    "\n",
    "**Next step**: With comprehensive data analysis complete, we'll implement the optimal partitioning strategy based on discovered data patterns and performance requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ba3207-5db8-4a88-a3ec-849a28f4c8f1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q --disable-pip-version-check semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04495fa-42ae-41c9-a701-8f9c635fb5ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3. Strategic Partition Design and Implementation\n",
    "\n",
    "### Enterprise-Grade Partitioning Implementation for Maximum Performance\n",
    "\n",
    "This section implements the **optimal partitioning strategy** identified through data analysis, creating **production-ready partitioned models** that deliver **dramatic performance improvements** through **intelligent data organization** and **strategic partition design**.\n",
    "\n",
    "#### **Implementation Framework Objectives:**\n",
    "- **Strategic partition creation**: Implementing data-driven partitioning strategies\n",
    "- **Performance optimization**: Achieving maximum query performance through optimal partitioning\n",
    "- **Enterprise scalability**: Creating partitioning solutions that scale with business growth\n",
    "- **Production readiness**: Implementing partitioning suitable for enterprise deployment\n",
    "\n",
    "### **Advanced Partition Implementation Strategy**\n",
    "\n",
    "#### **Enterprise Partitioning Implementation Framework:**\n",
    "\n",
    "| Implementation Phase | Focus Area | Deliverable | Performance Impact |\n",
    "|---------------------|------------|-------------|-------------------|\n",
    "| **Partition Design** | Optimal partition key selection | Strategic partitioning scheme | Foundation for performance |\n",
    "| **Implementation** | Partition creation and configuration | Partitioned Direct Lake model | Immediate performance improvement |\n",
    "| **Validation** | Performance testing and verification | Validated performance gains | Confirmed optimization |\n",
    "| **Optimization** | Fine-tuning and enhancement | Optimized partitioned model | Maximum performance achievement |\n",
    "\n",
    "#### **Strategic Partition Implementation:**\n",
    "```python\n",
    "# Advanced partition implementation configuration\n",
    "partition_implementation = {\n",
    "    'strategy': 'data_driven_optimal',\n",
    "    'performance_focus': 'query_optimization',\n",
    "    'scalability': 'enterprise_grade',\n",
    "    'monitoring': 'comprehensive',\n",
    "    'validation': 'thorough'\n",
    "}\n",
    "```\n",
    "\n",
    "### **Intelligent Partition Key Selection and Configuration**\n",
    "\n",
    "#### **Optimal Partition Key Implementation:**\n",
    "\n",
    "##### **1. Data-Driven Partition Key Selection:**\n",
    "- **Primary partition key**: Most selective column for maximum query pruning\n",
    "- **Secondary partitioning**: Additional partitioning levels for complex optimization\n",
    "- **Partition granularity**: Optimal balance between pruning effectiveness and management overhead\n",
    "- **Business alignment**: Partitioning aligned with business processes and access patterns\n",
    "\n",
    "##### **2. Advanced Partitioning Configuration:**\n",
    "- **Partition boundary optimization**: Strategic partition boundaries for balanced distribution\n",
    "- **Dynamic partition sizing**: Adaptive partition sizing based on data growth patterns\n",
    "- **Query-optimized partitioning**: Partitioning specifically designed for common query patterns\n",
    "- **Multi-dimensional partitioning**: Complex partitioning strategies for specialized requirements\n",
    "\n",
    "#### **Partition Configuration Benefits:**\n",
    "- ‚úÖ **Maximum query pruning**: Optimal partition elimination during query execution\n",
    "- ‚úÖ **Balanced distribution**: Even data distribution across partitions for consistent performance\n",
    "- ‚úÖ **Scalable architecture**: Partitioning design that scales with data growth\n",
    "- ‚úÖ **Query optimization**: Partitioning specifically optimized for typical query patterns\n",
    "\n",
    "### **Enterprise-Grade Partition Creation Process**\n",
    "\n",
    "#### **Production-Ready Partition Implementation:**\n",
    "\n",
    "##### **1. Partition Schema Design:**\n",
    "```python\n",
    "# Strategic partition schema configuration\n",
    "partition_schema = {\n",
    "    'partition_key': selected_optimal_column,\n",
    "    'partition_type': 'range_based',  # or 'hash_based', 'list_based'\n",
    "    'partition_count': optimal_partition_count,\n",
    "    'distribution_strategy': 'balanced',\n",
    "    'performance_optimization': True\n",
    "}\n",
    "```\n",
    "\n",
    "##### **2. Advanced Implementation Features:**\n",
    "- **Automated partition creation**: Systematic creation of optimal partition structure\n",
    "- **Metadata optimization**: Efficient metadata configuration for partitioned tables\n",
    "- **Index optimization**: Strategic indexing for partitioned table performance\n",
    "- **Compression optimization**: Advanced compression strategies for partitioned data\n",
    "\n",
    "#### **Implementation Quality Assurance:**\n",
    "- **Partition balance validation**: Ensuring even distribution across partitions\n",
    "- **Metadata integrity**: Validating partition metadata accuracy and completeness\n",
    "- **Performance baseline**: Establishing performance baselines for comparison\n",
    "- **Error handling**: Robust error handling and recovery mechanisms\n",
    "\n",
    "### **Advanced Partitioning Techniques**\n",
    "\n",
    "#### **Sophisticated Partitioning Strategies:**\n",
    "\n",
    "##### **1. Temporal Partitioning Implementation:**\n",
    "- **Date-based partitioning**: Strategic partitioning by date ranges for time-series data\n",
    "- **Sliding window partitioning**: Dynamic partition management for rolling time windows\n",
    "- **Business calendar partitioning**: Partitioning aligned with business calendars and cycles\n",
    "- **Multi-level temporal partitioning**: Year/month/day hierarchical partitioning\n",
    "\n",
    "##### **2. Business-Aligned Partitioning:**\n",
    "- **Geographic partitioning**: Partitioning by geographic regions or territories\n",
    "- **Department partitioning**: Partitioning aligned with organizational structure\n",
    "- **Product category partitioning**: Partitioning by product lines or categories\n",
    "- **Customer segmentation partitioning**: Partitioning by customer segments or tiers\n",
    "\n",
    "#### **Advanced Technique Benefits:**\n",
    "- **üéØ Business optimization**: Partitioning aligned with business processes and requirements\n",
    "- **‚ö° Query specialization**: Partitioning optimized for specific business query patterns\n",
    "- **üîÑ Maintenance efficiency**: Simplified maintenance through business-aligned partitioning\n",
    "- **üìà Scalability enhancement**: Partitioning that scales with business growth\n",
    "\n",
    "### **Partition Performance Validation**\n",
    "\n",
    "#### **Comprehensive Performance Testing Framework:**\n",
    "\n",
    "##### **1. Before-and-After Performance Comparison:**\n",
    "- **Query response time measurement**: Detailed measurement of query performance improvements\n",
    "- **Resource utilization analysis**: Understanding memory and compute resource optimization\n",
    "- **Throughput assessment**: Measuring query throughput improvements\n",
    "- **Concurrent performance validation**: Testing performance under concurrent user loads\n",
    "\n",
    "##### **2. Partition Effectiveness Analysis:**\n",
    "- **Partition pruning verification**: Confirming effective partition elimination\n",
    "- **Memory footprint reduction**: Measuring memory usage improvements\n",
    "- **I/O optimization validation**: Confirming reduced data scanning and I/O\n",
    "- **Parallel processing enhancement**: Validating improved parallel query execution\n",
    "\n",
    "#### **Performance Validation Results:**\n",
    "- **üìä Quantified improvements**: Precise measurement of performance gains\n",
    "- **‚ö° Resource optimization**: Documented resource utilization improvements\n",
    "- **üéØ Query optimization**: Confirmed query response time enhancements\n",
    "- **üöÄ Scalability validation**: Proven scalability improvements through partitioning\n",
    "\n",
    "### **Expected Implementation Outcomes**\n",
    "\n",
    "#### **Enterprise Partitioning Achievement:**\n",
    "- ‚úÖ **Optimal partition strategy**: Implementation of data-driven partitioning approach\n",
    "- ‚úÖ **Dramatic performance improvement**: Significant query response time enhancement\n",
    "- ‚úÖ **Resource optimization**: Reduced memory and compute resource consumption\n",
    "- ‚úÖ **Production readiness**: Enterprise-grade partitioned model ready for deployment\n",
    "\n",
    "#### **Strategic Business Value:**\n",
    "- **Competitive advantage**: Superior query performance providing business differentiation\n",
    "- **Cost optimization**: Reduced infrastructure costs through efficient resource utilization\n",
    "- **User experience enhancement**: Improved user satisfaction through faster query responses\n",
    "- **Scalability foundation**: Partitioning architecture supporting business growth\n",
    "\n",
    "**Next step**: With strategic partitioning implemented, we'll conduct comprehensive performance impact analysis to measure and validate the optimization effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76cd66-7fc7-4daf-a648-e0d7b1bb6e60",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as labs\n",
    "from sempy import fabric\n",
    "import sempy\n",
    "import pandas\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "LakehouseName = \"BigData\"\n",
    "lakehouses = labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "for l in lakehouses:\n",
    "    if l.startswith(\"Big\"):\n",
    "        LakehouseName = l\n",
    "\n",
    "SemanticModelName = f\"{LakehouseName}_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6978e8d5-5d86-4da6-9567-8a1e5c49bfb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Comprehensive Performance Impact Measurement and Analysis\n",
    "\n",
    "### Quantifying Partitioning Success Through Advanced Performance Analytics\n",
    "\n",
    "This section conducts **comprehensive performance impact analysis** to measure, validate, and quantify the **effectiveness of implemented partitioning strategies**, providing **concrete evidence** of performance improvements and **business value delivery**.\n",
    "\n",
    "#### **Performance Measurement Objectives:**\n",
    "- **Quantified improvement measurement**: Precise measurement of performance gains achieved\n",
    "- **Resource optimization validation**: Confirming memory and compute efficiency improvements\n",
    "- **User experience impact**: Understanding real-world impact on user query performance\n",
    "- **Business value quantification**: Translating technical improvements to business value\n",
    "\n",
    "### **Multi-Dimensional Performance Analysis Framework**\n",
    "\n",
    "#### **Comprehensive Performance Measurement Matrix:**\n",
    "\n",
    "| Performance Dimension | Measurement Focus | Success Criteria | Business Impact |\n",
    "|----------------------|------------------|------------------|-----------------|\n",
    "| **Query Response Time** | End-to-end query execution time | 60-90% improvement | Enhanced user productivity |\n",
    "| **Resource Utilization** | Memory and CPU consumption | 40-70% reduction | Reduced infrastructure costs |\n",
    "| **Throughput Performance** | Queries processed per second | 50-80% improvement | Increased user capacity |\n",
    "| **Concurrent Performance** | Multi-user query performance | Maintained/improved | Enhanced system scalability |\n",
    "\n",
    "#### **Advanced Performance Analytics Implementation:**\n",
    "```python\n",
    "# Comprehensive performance measurement framework\n",
    "performance_analytics = {\n",
    "    'baseline_comparison': True,\n",
    "    'detailed_metrics': True,\n",
    "    'resource_analysis': True,\n",
    "    'user_impact_assessment': True,\n",
    "    'business_value_calculation': True\n",
    "}\n",
    "```\n",
    "\n",
    "### **Detailed Query Performance Analysis**\n",
    "\n",
    "#### **Before-and-After Performance Comparison:**\n",
    "\n",
    "##### **1. Query Execution Time Analysis:**\n",
    "- **Individual query performance**: Detailed analysis of specific query improvements\n",
    "- **Query type performance**: Performance improvements across different query categories\n",
    "- **Complex query optimization**: Advanced query performance enhancement validation\n",
    "- **Real-world scenario testing**: Performance testing using actual business queries\n",
    "\n",
    "##### **2. Query Execution Intelligence:**\n",
    "- **Partition pruning effectiveness**: Measurement of partition elimination efficiency\n",
    "- **Parallel processing optimization**: Validation of improved parallel query execution\n",
    "- **Memory allocation efficiency**: Analysis of optimized memory usage during queries\n",
    "- **I/O reduction quantification**: Measurement of reduced data scanning and I/O operations\n",
    "\n",
    "#### **Performance Improvement Quantification:**\n",
    "- **üìä Response time reduction**: Precise measurement of query response time improvements\n",
    "- **‚ö° Throughput enhancement**: Quantified increase in query processing capacity\n",
    "- **üíæ Memory optimization**: Documented memory consumption reduction\n",
    "- **üîÑ Parallel efficiency**: Validated improvements in parallel query processing\n",
    "\n",
    "### **Resource Utilization Optimization Analysis**\n",
    "\n",
    "#### **Comprehensive Resource Impact Assessment:**\n",
    "\n",
    "##### **1. Memory Utilization Optimization:**\n",
    "- **Peak memory reduction**: Measurement of maximum memory consumption reduction\n",
    "- **Average memory efficiency**: Analysis of typical memory usage improvements\n",
    "- **Memory allocation patterns**: Understanding optimized memory allocation strategies\n",
    "- **Concurrent memory management**: Analysis of memory efficiency under concurrent loads\n",
    "\n",
    "##### **2. Compute Resource Optimization:**\n",
    "- **CPU utilization efficiency**: Measurement of compute resource optimization\n",
    "- **Processing throughput**: Analysis of data processing rate improvements\n",
    "- **Resource contention reduction**: Validation of reduced resource conflicts\n",
    "- **Scalability enhancement**: Understanding improved resource scalability\n",
    "\n",
    "#### **Resource Optimization Benefits:**\n",
    "- **üí∞ Cost reduction**: Quantified infrastructure cost savings through resource optimization\n",
    "- **üöÄ Performance consistency**: Improved performance predictability and consistency\n",
    "- **üìà Scalability improvement**: Enhanced ability to handle increased loads\n",
    "- **‚ö° Efficiency maximization**: Optimal utilization of available computing resources\n",
    "\n",
    "### **User Experience and Business Impact Analysis**\n",
    "\n",
    "#### **Real-World Performance Impact Assessment:**\n",
    "\n",
    "##### **1. User Experience Validation:**\n",
    "- **End-user query performance**: Measurement of actual user query response times\n",
    "- **Dashboard and report performance**: Analysis of business intelligence tool performance\n",
    "- **Interactive analytics performance**: Validation of real-time analytics capability\n",
    "- **Peak usage performance**: Performance validation during high-usage periods\n",
    "\n",
    "##### **2. Business Process Impact:**\n",
    "- **Decision-making acceleration**: Faster access to analytical insights\n",
    "- **Operational efficiency**: Improved efficiency of data-driven business processes\n",
    "- **Competitive advantage**: Enhanced ability to respond quickly to market changes\n",
    "- **Innovation enablement**: Foundation for advanced analytics and AI initiatives\n",
    "\n",
    "#### **Business Value Quantification:**\n",
    "- **üéØ Productivity improvement**: Quantified user productivity gains\n",
    "- **üíº Decision-making acceleration**: Faster business insight delivery\n",
    "- **üìä Operational efficiency**: Improved efficiency of data-driven processes\n",
    "- **üåü Competitive differentiation**: Performance advantages providing market differentiation\n",
    "\n",
    "### **Advanced Performance Monitoring and Validation**\n",
    "\n",
    "#### **Sophisticated Performance Monitoring Framework:**\n",
    "\n",
    "##### **1. Real-Time Performance Tracking:**\n",
    "- **Live performance monitoring**: Continuous monitoring of partitioned model performance\n",
    "- **Performance trend analysis**: Understanding performance patterns over time\n",
    "- **Anomaly detection**: Identification of performance deviations or issues\n",
    "- **Predictive performance analysis**: Forecasting future performance based on current trends\n",
    "\n",
    "##### **2. Comparative Performance Analysis:**\n",
    "- **Baseline comparison**: Detailed comparison with pre-partitioning performance\n",
    "- **Industry benchmarking**: Comparison with industry performance standards\n",
    "- **Best practice validation**: Confirmation of achievement of partitioning best practices\n",
    "- **Continuous improvement identification**: Discovery of additional optimization opportunities\n",
    "\n",
    "#### **Monitoring and Validation Results:**\n",
    "```python\n",
    "# Performance validation results framework\n",
    "validation_results = {\n",
    "    'query_performance_improvement': measured_improvement_percentage,\n",
    "    'resource_optimization': resource_efficiency_gains,\n",
    "    'user_experience_enhancement': user_satisfaction_metrics,\n",
    "    'business_value_delivered': quantified_business_impact\n",
    "}\n",
    "```\n",
    "\n",
    "### **Performance Improvement Sustainability Analysis**\n",
    "\n",
    "#### **Long-Term Performance Validation:**\n",
    "\n",
    "##### **1. Performance Sustainability Assessment:**\n",
    "- **Sustained improvement validation**: Confirming long-term performance benefits\n",
    "- **Performance consistency**: Validating consistent performance across different conditions\n",
    "- **Growth impact analysis**: Understanding performance as data volume grows\n",
    "- **Maintenance impact**: Assessing ongoing maintenance requirements and impact\n",
    "\n",
    "##### **2. Scalability and Future-Proofing:**\n",
    "- **Data growth accommodation**: Validating performance with increasing data volumes\n",
    "- **User growth handling**: Confirming performance with increasing user loads\n",
    "- **Query complexity evolution**: Understanding performance with evolving query patterns\n",
    "- **Technology evolution compatibility**: Ensuring compatibility with future technology updates\n",
    "\n",
    "### **Expected Performance Analysis Outcomes**\n",
    "\n",
    "#### **Comprehensive Performance Validation:**\n",
    "- ‚úÖ **Dramatic performance improvement**: Confirmed 60-90% query response time improvement\n",
    "- ‚úÖ **Significant resource optimization**: Validated 40-70% resource consumption reduction\n",
    "- ‚úÖ **Enhanced user experience**: Proven improvement in user query performance and satisfaction\n",
    "- ‚úÖ **Quantified business value**: Measured business impact and return on investment\n",
    "\n",
    "#### **Strategic Achievement Recognition:**\n",
    "- **Performance excellence**: Achievement of industry-leading query performance\n",
    "- **Resource efficiency**: Optimal utilization of infrastructure resources\n",
    "- **User satisfaction**: Enhanced user experience and productivity\n",
    "- **Competitive advantage**: Performance levels providing business differentiation\n",
    "\n",
    "**Next step**: With comprehensive performance analysis complete, we'll implement advanced query optimization techniques to further enhance partitioned model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5780deb-4ae1-45a3-a8ea-b67bd6e57a86",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "lakehouses=labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "if LakehouseName in lakehouses.values:\n",
    "    lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]\n",
    "else:\n",
    "    print(\"You need to complete Lab 2 to create the required lakehouse for this lab\")\n",
    "\n",
    "workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "workspaceName = sempy.fabric.resolve_workspace_name(workspaceId)\n",
    "print(f\"WorkspaceId = {workspaceId}, LakehouseID = {lakehouseId}, Workspace Name = {workspaceName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f76ea7-bd3e-44c7-adce-4b5660da8b57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 5. Advanced Query Optimization and Validation Testing\n",
    "\n",
    "### Maximizing Partitioning Benefits Through Intelligent Query Optimization\n",
    "\n",
    "This section focuses on **advanced query optimization techniques** that maximize the benefits of column partitioning, implementing **query-specific optimizations** and conducting **comprehensive validation** to ensure **optimal performance** across diverse query patterns.\n",
    "\n",
    "#### **Query Optimization Framework Objectives:**\n",
    "- **Query-specific optimization**: Tailoring queries to leverage partitioning benefits maximally\n",
    "- **Performance validation**: Comprehensive testing of optimized queries across scenarios\n",
    "- **Pattern-based optimization**: Implementing optimization strategies for common query patterns\n",
    "- **Enterprise query performance**: Ensuring optimal performance for production query workloads\n",
    "\n",
    "### **Intelligent Query Optimization for Partitioned Models**\n",
    "\n",
    "#### **Query Optimization Strategy Matrix:**\n",
    "\n",
    "| Query Pattern | Optimization Technique | Performance Benefit | Implementation Focus |\n",
    "|---------------|------------------------|-------------------|---------------------|\n",
    "| **Filter-Heavy Queries** | Partition pruning optimization | 70-90% improvement | WHERE clause optimization |\n",
    "| **Aggregation Queries** | Partition-parallel processing | 50-80% improvement | GROUP BY optimization |\n",
    "| **Join Operations** | Partition-aware joins | 40-70% improvement | JOIN strategy optimization |\n",
    "| **Complex Analytics** | Multi-level optimization | 60-85% improvement | Combined technique application |\n",
    "\n",
    "#### **Advanced Query Optimization Implementation:**\n",
    "```python\n",
    "# Intelligent query optimization framework\n",
    "query_optimization = {\n",
    "    'partition_aware_queries': True,\n",
    "    'filter_optimization': True,\n",
    "    'aggregation_enhancement': True,\n",
    "    'join_optimization': True,\n",
    "    'performance_validation': True\n",
    "}\n",
    "```\n",
    "\n",
    "### **Partition-Aware Query Design and Optimization**\n",
    "\n",
    "#### **Strategic Query Optimization Techniques:**\n",
    "\n",
    "##### **1. Partition Pruning Maximization:**\n",
    "- **Filter clause optimization**: Strategic WHERE clause design for maximum partition elimination\n",
    "- **Predicate pushdown**: Ensuring filters are applied at the partition level\n",
    "- **Multi-level filtering**: Combining multiple filters for enhanced partition pruning\n",
    "- **Dynamic filter optimization**: Adapting filters based on partition characteristics\n",
    "\n",
    "##### **2. Parallel Processing Optimization:**\n",
    "- **Partition-parallel aggregations**: Leveraging parallel processing across partitions\n",
    "- **Concurrent partition access**: Optimizing concurrent access to multiple partitions\n",
    "- **Load balancing**: Ensuring even workload distribution across partitions\n",
    "- **Resource allocation optimization**: Optimal resource allocation for parallel processing\n",
    "\n",
    "#### **Query Optimization Benefits:**\n",
    "- ‚úÖ **Maximum partition pruning**: Optimal elimination of irrelevant partitions\n",
    "- ‚úÖ **Enhanced parallel processing**: Improved utilization of parallel processing capabilities\n",
    "- ‚úÖ **Resource efficiency**: Optimized resource utilization for query execution\n",
    "- ‚úÖ **Consistent performance**: Predictable performance across different query patterns\n",
    "\n",
    "### **Comprehensive Query Performance Validation**\n",
    "\n",
    "#### **Multi-Scenario Query Testing Framework:**\n",
    "\n",
    "##### **1. Query Pattern Performance Testing:**\n",
    "- **Simple filter queries**: Testing basic partition pruning effectiveness\n",
    "- **Complex analytical queries**: Validating performance for sophisticated business analytics\n",
    "- **Join-heavy queries**: Testing partition-aware join optimization\n",
    "- **Aggregation-intensive queries**: Validating parallel aggregation performance\n",
    "\n",
    "##### **2. Real-World Scenario Validation:**\n",
    "- **Business dashboard queries**: Testing performance for actual dashboard requirements\n",
    "- **Report generation queries**: Validating performance for scheduled report generation\n",
    "- **Ad-hoc analytical queries**: Testing performance for exploratory data analysis\n",
    "- **Concurrent user scenarios**: Validating performance under multi-user loads\n",
    "\n",
    "#### **Performance Testing Implementation:**\n",
    "```python\n",
    "# Comprehensive query validation framework\n",
    "query_validation = {\n",
    "    'test_scenarios': ['simple_filters', 'complex_analytics', 'joins', 'aggregations'],\n",
    "    'performance_metrics': ['response_time', 'resource_usage', 'throughput'],\n",
    "    'validation_depth': 'comprehensive',\n",
    "    'real_world_testing': True\n",
    "}\n",
    "```\n",
    "\n",
    "### **Advanced Query Pattern Optimization**\n",
    "\n",
    "#### **Specialized Optimization Techniques:**\n",
    "\n",
    "##### **1. Time-Based Query Optimization:**\n",
    "- **Temporal filter optimization**: Leveraging time-based partitioning for date range queries\n",
    "- **Rolling window queries**: Optimizing queries over sliding time windows\n",
    "- **Historical analysis queries**: Efficient processing of long-term historical analysis\n",
    "- **Real-time data queries**: Optimizing queries for current/recent data access\n",
    "\n",
    "##### **2. Business Intelligence Query Optimization:**\n",
    "- **Dashboard query optimization**: Specific optimization for business dashboard requirements\n",
    "- **Report query enhancement**: Optimizing scheduled report generation queries\n",
    "- **Interactive analytics optimization**: Enhancing real-time user interaction performance\n",
    "- **Drill-down query optimization**: Optimizing hierarchical data exploration queries\n",
    "\n",
    "#### **Specialized Optimization Results:**\n",
    "- **üéØ Business-aligned performance**: Optimization specifically tailored to business requirements\n",
    "- **‚ö° Interactive analytics**: Enhanced performance for real-time user interactions\n",
    "- **üìä Dashboard responsiveness**: Improved performance for business dashboards\n",
    "- **üîÑ Report efficiency**: Optimized performance for scheduled report generation\n",
    "\n",
    "### **Query Performance Monitoring and Continuous Optimization**\n",
    "\n",
    "#### **Advanced Performance Monitoring Framework:**\n",
    "\n",
    "##### **1. Real-Time Query Performance Tracking:**\n",
    "- **Query execution monitoring**: Continuous monitoring of query performance metrics\n",
    "- **Partition utilization tracking**: Understanding partition access patterns and efficiency\n",
    "- **Resource consumption analysis**: Monitoring resource usage for different query types\n",
    "- **Performance trend analysis**: Understanding query performance patterns over time\n",
    "\n",
    "##### **2. Intelligent Performance Optimization:**\n",
    "- **Adaptive query optimization**: Dynamic optimization based on performance patterns\n",
    "- **Machine learning-driven optimization**: AI-based query performance enhancement\n",
    "- **Predictive performance analysis**: Forecasting query performance based on characteristics\n",
    "- **Continuous improvement automation**: Automated identification and implementation of optimizations\n",
    "\n",
    "#### **Monitoring and Optimization Benefits:**\n",
    "- **üìà Continuous improvement**: Ongoing enhancement of query performance\n",
    "- **üéØ Proactive optimization**: Early identification and resolution of performance issues\n",
    "- **‚ö° Adaptive performance**: Dynamic optimization based on changing requirements\n",
    "- **üöÄ Innovation integration**: Integration of new optimization techniques and technologies\n",
    "\n",
    "### **Enterprise Query Performance Validation**\n",
    "\n",
    "#### **Production-Ready Performance Validation:**\n",
    "\n",
    "##### **1. Comprehensive Performance Testing:**\n",
    "- **Load testing**: Validation of performance under enterprise-scale loads\n",
    "- **Stress testing**: Performance validation under extreme conditions\n",
    "- **Concurrent user testing**: Multi-user performance validation\n",
    "- **Integration testing**: Performance validation with enterprise systems\n",
    "\n",
    "##### **2. Business Impact Validation:**\n",
    "- **User acceptance testing**: Validation of user experience improvements\n",
    "- **Business process validation**: Confirming improvement in business process efficiency\n",
    "- **SLA compliance validation**: Ensuring performance meets service level agreements\n",
    "- **ROI validation**: Confirming return on investment from query optimization\n",
    "\n",
    "### **Expected Query Optimization Outcomes**\n",
    "\n",
    "#### **Advanced Query Performance Achievement:**\n",
    "- ‚úÖ **Optimized query performance**: Maximum performance benefit from partitioning implementation\n",
    "- ‚úÖ **Pattern-specific optimization**: Tailored optimization for different query patterns\n",
    "- ‚úÖ **Enterprise performance validation**: Comprehensive validation of production-ready performance\n",
    "- ‚úÖ **Continuous optimization framework**: Automated ongoing performance enhancement\n",
    "\n",
    "#### **Strategic Performance Excellence:**\n",
    "- **Query performance leadership**: Industry-leading query performance through optimization\n",
    "- **Business process enhancement**: Improved efficiency of data-driven business processes\n",
    "- **User experience excellence**: Superior user experience through optimized query performance\n",
    "- **Competitive advantage**: Performance levels providing significant competitive differentiation\n",
    "\n",
    "**Next step**: With query optimization complete, we'll explore advanced large-scale partitioning techniques for massive enterprise datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34aab3-49e8-40fd-8a57-5a5a36cace2d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)\n",
    "\n",
    "def filter_func(e):\n",
    "    retVal:bool=True\n",
    "    if e.EventSubclass.ToString() == \"VertiPaqScanInternal\":\n",
    "        retVal=False      \n",
    "    #     #if e.EventSubClass.ToString() == \"VertiPaqScanInternal\":\n",
    "    #     retVal=False\n",
    "    return retVal\n",
    "\n",
    "# define events to trace and their corresponding columns\n",
    "def runQueryWithTrace (expr:str,workspaceName:str,SemanticModelName:str,Result:Optional[bool]=True,Trace:Optional[bool]=True,DMV:Optional[bool]=True,ClearCache:Optional[bool]=True) -> pandas.DataFrame :\n",
    "    event_schema = fabric.Trace.get_default_query_trace_schema()\n",
    "    event_schema.update({\"ExecutionMetrics\":[\"EventClass\",\"TextData\"]})\n",
    "    del event_schema['VertiPaqSEQueryBegin']\n",
    "    del event_schema['VertiPaqSEQueryCacheMatch']\n",
    "    del event_schema['DirectQueryBegin']\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    if ClearCache:\n",
    "        labs.clear_cache(SemanticModelName)\n",
    "\n",
    "    WorkspaceName:str = workspaceName\n",
    "    SemanticModelName:str = SemanticModelName\n",
    "\n",
    "    with fabric.create_trace_connection(SemanticModelName,WorkspaceName) as trace_connection:\n",
    "        # create trace on server with specified events\n",
    "        with trace_connection.create_trace(\n",
    "            event_schema=event_schema, \n",
    "            name=\"Simple Query Trace\",\n",
    "            filter_predicate=filter_func,\n",
    "            stop_event=\"QueryEnd\"\n",
    "            ) as trace:\n",
    "\n",
    "            trace.start()\n",
    "\n",
    "            df:FabricDataFrame=sempy.fabric.evaluate_dax(\n",
    "                dataset=SemanticModelName, \n",
    "                dax_string=expr)\n",
    "\n",
    "            if Result:\n",
    "                displayHTML(f\"<H2>####### DAX QUERY RESULT #######</H2>\")\n",
    "                display(df)\n",
    "\n",
    "            # Wait 5 seconds for trace data to arrive\n",
    "            time.sleep(5)\n",
    "\n",
    "            # stop Trace and collect logs\n",
    "            final_trace_logs:pandas.DataFrame = trace.stop()\n",
    "\n",
    "    if Trace:\n",
    "        displayHTML(f\"<H2>####### SERVER TIMINGS #######</H2>\")\n",
    "        display(final_trace_logs)\n",
    "    \n",
    "    if DMV:\n",
    "        displayHTML(f\"<H2>####### SHOW DMV RESULTS #######</H2>\")\n",
    "        runDMV()\n",
    "\n",
    "    return final_trace_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc7ef61-d34e-4d0c-856c-7023719e0086",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "##https://medium.com/@sqltidy/delays-in-the-automatically-generated-schema-in-the-sql-analytics-endpoint-of-the-lakehouse-b01c7633035d\n",
    "\n",
    "def triggerMetadataRefresh():\n",
    "    client = fabric.FabricRestClient()\n",
    "    response = client.get(f\"/v1/workspaces/{workspaceId}/lakehouses/{lakehouseId}\")\n",
    "    sqlendpoint = response.json()['properties']['sqlEndpointProperties']['id']\n",
    "\n",
    "    # trigger sync\n",
    "    uri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}\"\n",
    "    payload = {\"commands\":[{\"$type\":\"MetadataRefreshExternalCommand\"}]}\n",
    "    response = client.post(uri,json= payload)\n",
    "    batchId = response.json()['batchId']\n",
    "\n",
    "    # Monitor Progress\n",
    "    statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "    statusresponsedata = client.get(statusuri).json()\n",
    "    progressState = statusresponsedata['progressState']\n",
    "    print(f\"Metadata refresh : {progressState}\")\n",
    "    while progressState != \"success\":\n",
    "        statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "        statusresponsedata = client.get(statusuri).json()\n",
    "        progressState = statusresponsedata['progressState']\n",
    "        print(f\"Metadata refresh : {progressState}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    print('Metadata refresh complete')\n",
    "\n",
    "triggerMetadataRefresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35770fb9-3a0f-44b7-b5d6-5982e6c722d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 6. Large-Scale Enterprise Partitioning for Massive Datasets\n",
    "\n",
    "### Advanced Partitioning Strategies for Billion-Row Tables and Enterprise Scale\n",
    "\n",
    "This section explores **advanced partitioning techniques** specifically designed for **massive enterprise datasets**, implementing **sophisticated strategies** that maintain optimal performance at **billion-row scale** while ensuring **enterprise-grade reliability** and **management efficiency**.\n",
    "\n",
    "#### **Large-Scale Partitioning Objectives:**\n",
    "- **Massive dataset optimization**: Partitioning strategies for billion-row tables and beyond\n",
    "- **Enterprise scalability**: Partitioning approaches that scale with organizational growth\n",
    "- **Performance consistency**: Maintaining optimal performance regardless of data volume\n",
    "- **Management efficiency**: Streamlined management of large-scale partitioned systems\n",
    "\n",
    "### **Enterprise-Scale Partitioning Architecture**\n",
    "\n",
    "#### **Massive Dataset Partitioning Framework:**\n",
    "\n",
    "| Scale Category | Data Volume | Partitioning Strategy | Performance Target |\n",
    "|----------------|-------------|----------------------|-------------------|\n",
    "| **Large Scale** | 100M - 1B rows | Advanced single-level partitioning | 60-80% improvement |\n",
    "| **Enterprise Scale** | 1B - 10B rows | Multi-level partitioning | 70-90% improvement |\n",
    "| **Massive Scale** | 10B+ rows | Hierarchical partitioning | 80-95% improvement |\n",
    "| **Global Scale** | Distributed datasets | Cross-region partitioning | 85-98% improvement |\n",
    "\n",
    "#### **Advanced Large-Scale Implementation:**\n",
    "```python\n",
    "# Enterprise-scale partitioning configuration\n",
    "large_scale_partitioning = {\n",
    "    'scale_tier': 'enterprise_massive',\n",
    "    'partitioning_strategy': 'multi_level_hierarchical',\n",
    "    'performance_optimization': 'maximum',\n",
    "    'management_automation': True,\n",
    "    'scalability_framework': 'unlimited'\n",
    "}\n",
    "```\n",
    "\n",
    "### **Multi-Level Hierarchical Partitioning**\n",
    "\n",
    "#### **Sophisticated Partitioning Hierarchy Design:**\n",
    "\n",
    "##### **1. Hierarchical Partitioning Architecture:**\n",
    "- **Primary partitioning**: High-level partitioning by major business dimensions\n",
    "- **Secondary partitioning**: Sub-partitioning within primary partitions for granular optimization\n",
    "- **Tertiary partitioning**: Fine-grained partitioning for specialized performance requirements\n",
    "- **Dynamic partitioning**: Adaptive partitioning that evolves with data characteristics\n",
    "\n",
    "##### **2. Business-Aligned Hierarchical Design:**\n",
    "- **Temporal hierarchy**: Year ‚Üí Quarter ‚Üí Month ‚Üí Day partitioning for time-series data\n",
    "- **Geographic hierarchy**: Continent ‚Üí Country ‚Üí Region ‚Üí City for location-based data\n",
    "- **Organizational hierarchy**: Division ‚Üí Department ‚Üí Team for enterprise organizational data\n",
    "- **Product hierarchy**: Category ‚Üí Subcategory ‚Üí Product for retail and e-commerce data\n",
    "\n",
    "#### **Hierarchical Partitioning Benefits:**\n",
    "- ‚úÖ **Maximum query pruning**: Multi-level partition elimination for optimal performance\n",
    "- ‚úÖ **Balanced partition sizes**: Hierarchical structure maintains optimal partition sizing\n",
    "- ‚úÖ **Management efficiency**: Structured hierarchy simplifies large-scale partition management\n",
    "- ‚úÖ **Business alignment**: Partitioning structure aligned with business processes and reporting\n",
    "\n",
    "### **Advanced Partition Management and Automation**\n",
    "\n",
    "#### **Enterprise Partition Management Framework:**\n",
    "\n",
    "##### **1. Automated Partition Lifecycle Management:**\n",
    "- **Automatic partition creation**: Dynamic creation of new partitions based on data growth\n",
    "- **Partition maintenance automation**: Automated maintenance tasks for partition health\n",
    "- **Partition optimization**: Continuous optimization of partition structure and performance\n",
    "- **Partition archival**: Intelligent archival of historical partitions based on access patterns\n",
    "\n",
    "##### **2. Intelligent Partition Balancing:**\n",
    "- **Load balancing**: Automatic balancing of data across partitions\n",
    "- **Skew detection and correction**: Identification and correction of partition imbalances\n",
    "- **Performance optimization**: Continuous rebalancing for optimal performance\n",
    "- **Resource allocation**: Dynamic resource allocation based on partition characteristics\n",
    "\n",
    "#### **Management Automation Benefits:**\n",
    "- **üîÑ Reduced operational overhead**: Automated management reducing manual intervention\n",
    "- **‚ö° Consistent performance**: Automated optimization maintaining consistent performance\n",
    "- **üéØ Proactive maintenance**: Predictive maintenance preventing performance issues\n",
    "- **üìà Scalability automation**: Automated scaling accommodating data and user growth\n",
    "\n",
    "### **Performance Optimization for Massive Scale**\n",
    "\n",
    "#### **Advanced Performance Strategies:**\n",
    "\n",
    "##### **1. Memory Optimization for Large-Scale Partitioning:**\n",
    "- **Intelligent memory allocation**: Optimal memory distribution across massive partition sets\n",
    "- **Partition caching strategies**: Advanced caching for frequently accessed partitions\n",
    "- **Memory pool management**: Efficient memory pool allocation for large-scale operations\n",
    "- **Garbage collection optimization**: Enhanced garbage collection for large partition systems\n",
    "\n",
    "##### **2. Parallel Processing Maximization:**\n",
    "- **Partition-parallel query execution**: Maximum parallelization across partition boundaries\n",
    "- **Resource pool optimization**: Optimal allocation of compute resources across partitions\n",
    "- **Load distribution**: Intelligent load distribution for maximum throughput\n",
    "- **Concurrent operation coordination**: Efficient coordination of concurrent large-scale operations\n",
    "\n",
    "#### **Large-Scale Performance Results:**\n",
    "- **üìä Massive dataset performance**: Consistent high performance regardless of data volume\n",
    "- **‚ö° Linear scalability**: Performance that scales linearly with infrastructure investment\n",
    "- **üíæ Resource efficiency**: Optimal resource utilization at massive scale\n",
    "- **üöÄ Enterprise capability**: Performance levels suitable for largest enterprise deployments\n",
    "\n",
    "### **Advanced Monitoring and Management for Large-Scale Systems**\n",
    "\n",
    "#### **Enterprise-Grade Monitoring Framework:**\n",
    "\n",
    "##### **1. Comprehensive Large-Scale Monitoring:**\n",
    "- **Partition-level performance monitoring**: Detailed monitoring of individual partition performance\n",
    "- **System-wide performance tracking**: Holistic monitoring of entire partitioned system\n",
    "- **Resource utilization monitoring**: Comprehensive tracking of resource usage across partitions\n",
    "- **Predictive performance analysis**: AI-driven prediction of performance trends and issues\n",
    "\n",
    "##### **2. Intelligent Alerting and Response:**\n",
    "- **Performance threshold monitoring**: Automated alerting for performance degradation\n",
    "- **Capacity planning alerts**: Proactive alerts for resource and capacity requirements\n",
    "- **Partition health monitoring**: Continuous monitoring of partition health and integrity\n",
    "- **Automated response systems**: Intelligent automated responses to common issues\n",
    "\n",
    "#### **Monitoring and Management Results:**\n",
    "```python\n",
    "# Large-scale monitoring framework\n",
    "monitoring_framework = {\n",
    "    'partition_level_monitoring': True,\n",
    "    'system_wide_tracking': True,\n",
    "    'predictive_analysis': True,\n",
    "    'automated_response': True,\n",
    "    'enterprise_integration': True\n",
    "}\n",
    "```\n",
    "\n",
    "### **Enterprise Integration and Deployment**\n",
    "\n",
    "#### **Production-Ready Large-Scale Deployment:**\n",
    "\n",
    "##### **1. Enterprise System Integration:**\n",
    "- **Monitoring system integration**: Integration with enterprise monitoring and alerting systems\n",
    "- **Backup and recovery**: Enterprise-grade backup and recovery for partitioned systems\n",
    "- **Security and compliance**: Advanced security controls for large-scale partitioned data\n",
    "- **Disaster recovery**: Comprehensive disaster recovery planning for partitioned systems\n",
    "\n",
    "##### **2. Change Management and Governance:**\n",
    "- **Partition governance**: Enterprise governance frameworks for partition management\n",
    "- **Change control**: Formal change control processes for partition modifications\n",
    "- **Documentation and training**: Comprehensive documentation and training for partition management\n",
    "- **Compliance validation**: Ensuring compliance with enterprise governance requirements\n",
    "\n",
    "### **Expected Large-Scale Partitioning Outcomes**\n",
    "\n",
    "#### **Enterprise-Scale Achievement:**\n",
    "- ‚úÖ **Massive dataset performance**: Optimal performance for billion-row and larger datasets\n",
    "- ‚úÖ **Linear scalability**: Performance that scales efficiently with data and infrastructure growth\n",
    "- ‚úÖ **Enterprise management**: Production-ready management and monitoring capabilities\n",
    "- ‚úÖ **Future-proof architecture**: Partitioning architecture designed for unlimited scale\n",
    "\n",
    "#### **Strategic Business Capability:**\n",
    "- **Competitive advantage**: Capability to handle larger datasets than competitors\n",
    "- **Innovation enablement**: Technology foundation for advanced analytics and AI at scale\n",
    "- **Cost optimization**: Efficient handling of massive datasets reducing infrastructure costs\n",
    "- **Business agility**: Rapid access to insights from massive datasets enabling quick decisions\n",
    "\n",
    "**Next step**: With large-scale partitioning mastery achieved, we'll explore advanced partition maintenance and optimization strategies for ongoing performance excellence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e354f-b155-4cc0-891c-3dcf8a00211f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "reframeOK:bool=False\n",
    "while not reframeOK:\n",
    "    try:\n",
    "        result:pandas.DataFrame = labs.refresh_semantic_model(dataset=SemanticModelName)\n",
    "        reframeOK=True\n",
    "    except:\n",
    "        print('Error with reframe... trying again.')\n",
    "        triggerMetadataRefresh()\n",
    "        time.sleep(3)\n",
    "\n",
    "print('Custom Semantic Model reframe OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91296d1-13d3-48f9-9b0c-8ab63b628edd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Warm the cache and check the DMV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05270b62-d502-4e7b-9f44-fcaa4693d05b",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "trace1 = runQueryWithTrace(\"\"\"\n",
    "\n",
    "    EVALUATE\n",
    "        {\n",
    "            max(fact_myevents_1bln[DateKey]),\n",
    "            max(fact_myevents_1bln[Quantity_ThisYear]),\n",
    "            max(fact_myevents_1bln_partitioned_datekey[DateKey]),\n",
    "            max(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "        }\n",
    "\n",
    "\"\"\",workspaceName,SemanticModelName,Result=False,DMV=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6209c-0426-4b87-b0e3-bdf8225d70df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 6. Run Vertipaq Analyzer on Custom Semantic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2b265-d3e7-44f8-b223-724735945849",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "analyzer:dict[str,pandas.DataFrame] = labs.vertipaq_analyzer(dataset=SemanticModelName)\n",
    "\n",
    "for key, value in analyzer.items():\n",
    "    print(key)\n",
    "    display(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d8925e-961b-4a24-b698-c06a3cf3a3aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 7. Focus on VPAX result for **DateKey** and **Quantity_ThisYear** columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5acf8-920e-4bed-93cb-a24f95417ddc",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(analyzer[\"Columns\"].query(\"`Column Name`=='DateKey' & `Is Resident`==True\"))\n",
    "display(analyzer[\"Columns\"].query(\"`Column Name`=='Quantity_ThisYear' & `Is Resident`==True\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de6a700-c2f8-457d-bc3d-e298cd5cf0ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 8 Run some DAX Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee06b59-cebe-4389-b8f6-b691b6865f50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 8.1 Period Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11844eb9-195b-44a8-8714-f66325272f5f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Period Comparison against **base** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582e3cb-b971-4c96-90fc-6b38e7c74096",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Sum of Quantity PM] =\n",
    "            CALCULATE([Sum of Quantity],PREVIOUSMONTH(dim_Date[DateKey]))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM Delta] =\n",
    "            [Sum of Quantity] - [Sum of Quantity PM]\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity PM %] =\n",
    "            [Sum of Quantity PM Delta] / [Sum of Quantity]\n",
    "        \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            -- GROUP BY --\n",
    "            dim_Date[FirstDateofMonth] ,\n",
    "            --  FILTER  --\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofYear] ) ,\n",
    "             -- MEASURES --\n",
    "            \"Quantity\" \t\t\t\t, [Sum of Quantity],\n",
    "            \"Quantity PM\" \t\t\t, [Sum of Quantity PM],\n",
    "            \"Quantity PM Delta\"\t\t, [Sum of Quantity PM Delta] ,\n",
    "            \"Quantity PM % \" \t\t, [Sum of Quantity PM %]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "trace1 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False,Trace=False)\n",
    "trace1 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False,Trace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557720c-3d34-49bb-9a2d-01ed98d5bb92",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Period Comparison against **Partitioned** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b839f489-5cc7-42f7-96de-7bcf4c2e382e",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Sum of Quantity PM] =\n",
    "            CALCULATE([Sum of Quantity],PREVIOUSMONTH(dim_Date[DateKey]))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM Delta] =\n",
    "            [Sum of Quantity] - [Sum of Quantity PM]\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity PM %] =\n",
    "            [Sum of Quantity PM Delta] / [Sum of Quantity]\n",
    "        \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            -- GROUP BY --\n",
    "            dim_Date[FirstDateofMonth] ,\n",
    "            --  FILTER  --\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofYear] ) ,\n",
    "             -- MEASURES --\n",
    "            \"Quantity\" \t\t\t\t, [Sum of Quantity],\n",
    "            \"Quantity PM\" \t\t\t, [Sum of Quantity PM],\n",
    "            \"Quantity PM Delta\"\t\t, [Sum of Quantity PM Delta] ,\n",
    "            \"Quantity PM % \" \t\t, [Sum of Quantity PM %]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "trace2 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False,Trace=False)\n",
    "trace2 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False,Trace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9fd2c-1602-4bcf-94d8-e4dbd2984025",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(trace1)\n",
    "display(trace2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3392f85-83ee-420e-b404-509e106150ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 8.2 Running Total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611df2ab-2004-46bc-ba00-83965637d539",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Running Total against **Base** Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf42cc-5368-4f9e-bd52-de134df94d28",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "\t    MEASURE dim_Date[Sum of Quantity YTD] =\n",
    "\t\t    TOTALYTD([Sum of Quantity],dim_Date[DateKey])\n",
    "\t\n",
    "\t    MEASURE fact_myevents_1bln[Sum of Quantity QTD] =\n",
    "\t\t    TOTALQTD([Sum of Quantity],dim_Date[DateKey])\t\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            -- GROUP BY --\n",
    "            dim_Date[FirstDateofMonth] ,\n",
    "            --  FILTER  --\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofYear] ) ,\n",
    "             -- MEASURES --\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"Quantity YTD\" \t, [Sum of Quantity YTD] ,\n",
    "            \"Quantity QTD\" \t, [Sum of Quantity QTD]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace3 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a164621-4808-400d-a2f2-967738b972f2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Running Total against **Partitioned** Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b249175-5118-4828-995f-c6dec0592dda",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str=\"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "\t    MEASURE dim_Date[Sum of Quantity YTD] =\n",
    "\t\t    TOTALYTD([Sum of Quantity],dim_Date[DateKey])\n",
    "\t\n",
    "\t    MEASURE fact_myevents_1bln[Sum of Quantity QTD] =\n",
    "\t\t    TOTALQTD([Sum of Quantity],dim_Date[DateKey])\t\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            -- GROUP BY --\n",
    "            dim_Date[FirstDateofMonth] ,\n",
    "            --  FILTER  --\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofYear] ) ,\n",
    "             -- MEASURES --\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"Quantity YTD\" \t, [Sum of Quantity YTD] ,\n",
    "            \"Quantity QTD\" \t, [Sum of Quantity QTD]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace4 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trace3)\n",
    "display(trace4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33899584-56e9-4cef-a16a-427b57c3d00f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 8.3 RANK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab8de0-f4d8-48bf-811a-82556f8dcd67",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run RANK over **Base** Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db7eec-35e7-46a1-b40b-bb87379a220a",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Sum of Quantity Rank] =\n",
    "            RANKX(ALL(dim_Geography[COUNTRY]) , [Sum of Quantity] )\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"Rank\" \t\t\t, [Sum of Quantity Rank]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace5 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7c9d2-cff0-4332-9b34-d06acfaa3fe5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run RANK over **Partitioned** Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcdf8ea-4991-443f-89f8-44e1f47773c3",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Sum of Quantity Rank] =\n",
    "            RANKX(ALL(dim_Geography[COUNTRY]) , [Sum of Quantity] )\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"Rank\" \t\t\t, [Sum of Quantity Rank]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace6 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efb33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trace5)\n",
    "display(trace6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf39bc48-bca0-4ca3-928e-b69ed899fe81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 8.4 Percent of Parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c749bb-170b-4d04-871f-3bc4861aa2a8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Percent of Parent over **Base** Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcec580-b64d-4858-b8a4-58db8dda5acc",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "\t    MEASURE dim_Date[Percentage of Parent] =\n",
    "\t\t    [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"% of Parent\"\t, [Percentage of Parent]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace7 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a8ead-88b6-423d-9735-f534d8cb2704",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Percent of Parent over **Partitioned** Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecd3ce-5ae1-4046-b75d-677bab283f27",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "\t    MEASURE dim_Date[Percentage of Parent] =\n",
    "\t\t    [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"% of Parent\"\t, [Percentage of Parent]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace8 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51825ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trace7)\n",
    "display(trace8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce2c6a5-871a-4e58-acbf-9639ecc34a7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 8.5 All measures combined in one query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94737720-ebb1-438a-b0c0-90eb96e3ef87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run all measures on **Base** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b39d3-ddfb-4c67-887a-a1b1957367bf",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Percentage of Parent] =\n",
    "            [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity Rank] =\n",
    "            RANKX(ALL(dim_Geography[COUNTRY]) , [Sum of Quantity] )\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity YTD] =\n",
    "            TOTALYTD([Sum of Quantity],dim_Date[DateKey])\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity QTD] =\n",
    "            TOTALQTD([Sum of Quantity],dim_Date[DateKey])\t\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM] =\n",
    "            CALCULATE([Sum of Quantity],PREVIOUSMONTH(dim_Date[DateKey]))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM Delta] =\n",
    "            [Sum of Quantity] - [Sum of Quantity PM]\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity PM %] =\n",
    "            [Sum of Quantity PM Delta] / [Sum of Quantity]\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "            \"Quantity\" \t\t\t\t, [Sum of Quantity],\n",
    "            \"% of Parent\"\t\t\t, [Percentage of Parent],\n",
    "            \"Rank\" \t\t\t\t\t, [Sum of Quantity Rank],\n",
    "            \"Quantity YTD\" \t\t\t, [Sum of Quantity YTD] ,\n",
    "            \"Quantity QTD\" \t\t\t, [Sum of Quantity QTD]\t,\t\n",
    "            \"Quantity PM\" \t\t\t, [Sum of Quantity PM],\n",
    "            \"Quantity PM Delta\"\t\t, [Sum of Quantity PM Delta] ,\n",
    "            \"Quantity PM %\" \t\t, [Sum of Quantity PM %]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace9 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea9574-19e5-4249-aaa3-ca55cb72ecf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run all measures on **Partitioned** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807265ad-5fb0-4fd5-8acc-64c52ab07c3c",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Percentage of Parent] =\n",
    "            [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity Rank] =\n",
    "            RANKX(ALL(dim_Geography[COUNTRY]) , [Sum of Quantity] )\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity YTD] =\n",
    "            TOTALYTD([Sum of Quantity],dim_Date[DateKey])\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity QTD] =\n",
    "            TOTALQTD([Sum of Quantity],dim_Date[DateKey])\t\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM] =\n",
    "            CALCULATE([Sum of Quantity],PREVIOUSMONTH(dim_Date[DateKey]))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM Delta] =\n",
    "            [Sum of Quantity] - [Sum of Quantity PM]\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity PM %] =\n",
    "            [Sum of Quantity PM Delta] / [Sum of Quantity]\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "            \"Quantity\" \t\t\t\t, [Sum of Quantity],\n",
    "            \"% of Parent\"\t\t\t, [Percentage of Parent],\n",
    "            \"Rank\" \t\t\t\t\t, [Sum of Quantity Rank],\n",
    "            \"Quantity YTD\" \t\t\t, [Sum of Quantity YTD] ,\n",
    "            \"Quantity QTD\" \t\t\t, [Sum of Quantity QTD]\t,\t\n",
    "            \"Quantity PM\" \t\t\t, [Sum of Quantity PM],\n",
    "            \"Quantity PM Delta\"\t\t, [Sum of Quantity PM Delta] ,\n",
    "            \"Quantity PM %\" \t\t, [Sum of Quantity PM %]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace10 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trace9)\n",
    "display(trace10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa6f6e",
   "metadata": {},
   "source": [
    "## 12. Workshop Summary: Column Partitioning Mastery Achievement\n",
    "\n",
    "### Comprehensive Partitioning Excellence and Enterprise Performance Leadership\n",
    "\n",
    "Congratulations! üéâ You have successfully completed the **Advanced Column Partitioning Workshop**, achieving **expert-level mastery** of enterprise-grade partitioning strategies that deliver **dramatic performance improvements** and position you as a **Direct Lake partitioning specialist**.\n",
    "\n",
    "### üèÜ Advanced Partitioning Competencies Mastered\n",
    "\n",
    "#### **Expert-Level Technical Skills Developed:**\n",
    "\n",
    "##### **üèóÔ∏è 1. Strategic Partitioning Design Mastery**\n",
    "- **Data-driven partitioning**: Expert analysis of data patterns for optimal partitioning strategies\n",
    "- **Multi-level partitioning**: Advanced hierarchical partitioning for massive enterprise datasets\n",
    "- **Performance optimization**: Achieving 60-95% query performance improvements through strategic partitioning\n",
    "- **Enterprise scalability**: Partitioning architectures that scale with organizational growth\n",
    "\n",
    "##### **‚ö° 2. Performance Optimization Excellence**\n",
    "- **Query optimization**: Advanced query optimization techniques leveraging partitioning benefits\n",
    "- **Resource efficiency**: Achieving 40-70% reduction in memory and compute resource consumption\n",
    "- **Parallel processing**: Maximizing parallel processing capabilities through intelligent partitioning\n",
    "- **Large-scale performance**: Maintaining optimal performance for billion-row and larger datasets\n",
    "\n",
    "##### **üéØ 3. Enterprise Deployment Leadership**\n",
    "- **Production implementation**: Enterprise-ready partitioning deployment and management\n",
    "- **Monitoring and maintenance**: Comprehensive monitoring and automated maintenance frameworks\n",
    "- **Business alignment**: Partitioning strategies aligned with business processes and requirements\n",
    "- **Future-proof architecture**: Partitioning designs prepared for unlimited scale and growth\n",
    "\n",
    "### üìà Quantified Performance Achievement Summary\n",
    "\n",
    "#### **Measurable Business Value Delivered:**\n",
    "\n",
    "| Performance Area | Achievement Level | Business Impact | Strategic Value |\n",
    "|------------------|------------------|-----------------|-----------------|\n",
    "| **Query Performance** | 60-95% improvement | Enhanced user productivity | Competitive advantage |\n",
    "| **Resource Efficiency** | 40-70% reduction | Reduced infrastructure costs | Cost optimization |\n",
    "| **System Scalability** | Linear scale capability | Unlimited growth potential | Future-proof investment |\n",
    "| **User Experience** | Dramatic improvement | Enhanced user satisfaction | Business enablement |\n",
    "\n",
    "#### **Enterprise-Grade Capabilities Achieved:**\n",
    "- ‚úÖ **Massive dataset handling**: Proven capability for billion-row dataset optimization\n",
    "- ‚úÖ **Production deployment**: Enterprise-ready partitioning implementation expertise\n",
    "- ‚úÖ **Performance leadership**: Industry-leading query performance through advanced partitioning\n",
    "- ‚úÖ **Strategic business enablement**: Technology foundation for advanced analytics and competitive advantage\n",
    "\n",
    "### üåü Advanced Technical Expertise Gained\n",
    "\n",
    "#### **Sophisticated Partitioning Techniques Mastered:**\n",
    "\n",
    "##### **1. Strategic Partitioning Design:**\n",
    "- **Intelligent partition key selection**: Data-driven selection of optimal partitioning columns\n",
    "- **Hierarchical partitioning architecture**: Multi-level partitioning for complex enterprise requirements\n",
    "- **Business-aligned partitioning**: Partitioning strategies aligned with organizational structure and processes\n",
    "- **Dynamic partitioning**: Adaptive partitioning that evolves with changing data characteristics\n",
    "\n",
    "##### **2. Performance Optimization Mastery:**\n",
    "- **Query-specific optimization**: Tailored optimization techniques for different query patterns\n",
    "- **Resource allocation optimization**: Optimal memory and compute resource allocation strategies\n",
    "- **Parallel processing maximization**: Advanced techniques for leveraging parallel processing capabilities\n",
    "- **Large-scale performance management**: Strategies for maintaining performance at massive scale\n",
    "\n",
    "##### **3. Enterprise Integration Excellence:**\n",
    "- **Production deployment**: Risk-managed deployment strategies for enterprise environments\n",
    "- **Monitoring and alerting**: Comprehensive monitoring frameworks for partitioned systems\n",
    "- **Automated management**: Intelligent automation for partition lifecycle management\n",
    "- **Business process integration**: Seamless integration with enterprise business processes\n",
    "\n",
    "### üöÄ Strategic Career and Business Impact\n",
    "\n",
    "#### **Professional Development Achievement:**\n",
    "- **üéì Expert specialist certification**: Advanced Direct Lake partitioning specialist\n",
    "- **üíº Enterprise leadership**: Qualified to lead large-scale partitioning initiatives\n",
    "- **üåü Performance optimization leader**: Recognized expertise in query performance optimization\n",
    "- **üìà Technology innovation**: Capability to drive technology innovation and competitive advantage\n",
    "\n",
    "#### **Business Value Creation Capability:**\n",
    "- **Competitive differentiation**: Technology leadership providing competitive business advantage\n",
    "- **Cost optimization expertise**: Proven ability to reduce infrastructure and operational costs\n",
    "- **Innovation enablement**: Technology foundation for advanced analytics and AI initiatives\n",
    "- **Strategic business acceleration**: Capability to accelerate business processes through performance optimization\n",
    "\n",
    "### üéØ Practical Application and Implementation Readiness\n",
    "\n",
    "#### **Immediate Implementation Opportunities:**\n",
    "1. **Enterprise deployment**: Apply partitioning strategies to production Direct Lake environments\n",
    "2. **Performance optimization**: Lead partitioning optimization initiatives for existing models\n",
    "3. **Team development**: Train and develop organizational partitioning expertise\n",
    "4. **Innovation projects**: Initiate advanced partitioning and performance optimization projects\n",
    "\n",
    "#### **Advanced Learning and Development Path:**\n",
    "- **Lab 7 - High Cardinality Optimization**: Specialized techniques for complex data scenarios\n",
    "- **Lab 8 - Hybrid Scenarios**: Advanced integration strategies combining multiple optimization approaches\n",
    "- **Advanced workshops**: Machine learning integration, AI-driven optimization, emerging technologies\n",
    "- **Thought leadership**: Establish expertise through knowledge sharing and industry leadership\n",
    "\n",
    "### üèÖ Workshop Completion Certification\n",
    "\n",
    "#### **Advanced Column Partitioning Mastery Certification:**\n",
    "You have successfully demonstrated:\n",
    "- ‚úÖ **Expert-level partitioning skills** for enterprise-scale Direct Lake optimization\n",
    "- ‚úÖ **Production deployment capabilities** for large-scale partitioning implementation\n",
    "- ‚úÖ **Performance optimization mastery** achieving industry-leading query performance\n",
    "- ‚úÖ **Business value delivery expertise** translating technical optimization to business advantage\n",
    "\n",
    "#### **Professional Recognition and Advancement:**\n",
    "- **üèÜ Advanced practitioner**: Certified expert in Direct Lake column partitioning\n",
    "- **üí° Innovation leader**: Qualified to lead performance optimization and innovation initiatives\n",
    "- **üéØ Enterprise consultant**: Capable of providing enterprise-level partitioning consulting\n",
    "- **üöÄ Technology strategist**: Qualified to develop and implement enterprise technology strategies\n",
    "\n",
    "### üìã Strategic Next Steps and Continuous Excellence\n",
    "\n",
    "#### **Immediate Action Plan:**\n",
    "1. **Document expertise**: Create comprehensive documentation of partitioning insights and strategies\n",
    "2. **Strategic planning**: Develop partitioning roadmap for organizational implementation\n",
    "3. **Stakeholder engagement**: Present business case and value proposition to enterprise stakeholders\n",
    "4. **Team preparation**: Prepare organizational teams for advanced partitioning implementation\n",
    "\n",
    "#### **Long-term Strategic Initiatives:**\n",
    "1. **Enterprise rollout**: Implement advanced partitioning across all enterprise Direct Lake models\n",
    "2. **Center of excellence**: Establish organizational center of excellence for partitioning optimization\n",
    "3. **Innovation leadership**: Lead next-generation partitioning and performance optimization initiatives\n",
    "4. **Industry leadership**: Establish thought leadership in Direct Lake performance optimization\n",
    "\n",
    "### üåü Final Achievement Recognition\n",
    "\n",
    "**Congratulations on achieving Advanced Column Partitioning Mastery!** \n",
    "\n",
    "You have successfully:\n",
    "- üéØ **Mastered enterprise-grade partitioning strategies** for optimal Direct Lake performance\n",
    "- üöÄ **Developed large-scale deployment expertise** for massive dataset optimization\n",
    "- üí° **Gained performance optimization leadership** for competitive advantage\n",
    "- üèÜ **Achieved industry-leading partitioning expertise** for strategic business enablement\n",
    "\n",
    "You are now equipped with **advanced partitioning expertise** to lead enterprise performance optimization initiatives, drive significant business value through technology excellence, and establish competitive advantage through superior Direct Lake performance.\n",
    "\n",
    "**Welcome to the elite community of Advanced Direct Lake Partitioning Specialists!** üåü\n",
    "\n",
    "### üîÑ Preparation for Advanced Workshops\n",
    "\n",
    "#### **Next Learning Journey:**\n",
    "- **Lab 7 - High Cardinality Optimization**: Ready to tackle specialized optimization for complex data scenarios\n",
    "- **Lab 8 - Hybrid Scenarios**: Prepared for advanced integration of multiple optimization strategies\n",
    "- **Advanced specialization**: Foundation established for specialized optimization techniques and emerging technologies\n",
    "\n",
    "Your partitioning mastery provides the **essential foundation** for advanced Direct Lake optimization techniques and positions you for **continued excellence** in enterprise data performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6270d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {
    "0408074c-1014-4c64-982e-bf06ad39bd42": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "count",
        "binsNumber": 10,
        "categoryFieldKeys": [
         "0"
        ],
        "chartType": "bar",
        "evaluatesOverAllRecords": false,
        "isStacked": false,
        "seriesFieldKeys": [
         "0"
        ],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "python",
      "table": {
       "rows": [
        {
         "0": "QueryBegin",
         "1": "DAXQuery",
         "2": "2025-03-16 22:45:45.600",
         "3": "DEFINE\n\n        MEASURE dim_Date[Sum of Quantity] = \n            SUM(fact_myevents_1bln[Quantity_ThisYear])\n            \n\t    MEASURE dim_Date[Percentage of Parent] =\n\t\t    [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n\n    EVALUATE\n        SUMMARIZECOLUMNS(\n            dim_Geography[COUNTRY] ,\n            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n            \"Quantity\" \t\t, [Sum of Quantity],\n            \"% of Parent\"\t, [Percentage of Parent]\n            )",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:45.586",
         "6": "NaT",
         "7": null,
         "8": null,
         "9": null
        },
        {
         "0": "DirectQueryEnd",
         "1": null,
         "2": "2025-03-16 22:45:47.053",
         "3": "\nSELECT sch.name AS schemaname,\ntbl.name AS tablename,\nc.name AS columnname\nFROM sys.masked_columns AS c\nJOIN sys.tables AS tbl ON c.object_id = tbl.object_id\nJOIN sys.schemas AS sch ON tbl.schema_id = sch.schema_id\nWHERE c.is_masked = 1;\n",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:47.053",
         "6": "2025-03-16 22:45:47.053",
         "7": "0",
         "8": "31",
         "9": "Success"
        },
        {
         "0": "DirectQueryEnd",
         "1": null,
         "2": "2025-03-16 22:45:47.633",
         "3": "\nSELECT 1 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[DateKey]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 2 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[UserID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 3 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[OperatingSystemID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 4 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[OperatingSystemVendorID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 5 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[GeographyID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 6 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[Quantity_ThisYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 7 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[Quantity_LastYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 8 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[HLL_HashBucket]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 9 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[HLL_FirstZero]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 11 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[DateKey]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 12 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[UserID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 13 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[OperatingSystemID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 14 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[OperatingSystemVendorID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 15 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[GeographyID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 16 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[Quantity_ThisYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 17 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[Quantity_LastYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 18 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[HLL_HashBucket]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 19 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[HLL_FirstZero]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 21 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[DateKey]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 22 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[UserID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 23 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[OperatingSystemID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 24 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[OperatingSystemVendorID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 25 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[GeographyID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 26 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[Quantity_ThisYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 27 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[Quantity_LastYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 28 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[HLL_HashBucket]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 29 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[HLL_FirstZero]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 41 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[DateKey]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 42 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Day]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 43 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[DaySuffix]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 44 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Weekday]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 45 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekDayName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 46 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekDayName_Short]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 47 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekDayName_FirstLetter]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 48 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[DOWInMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 49 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[DayOfYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 50 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekOfMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 51 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekOfYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 52 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Month]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 53 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MonthName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 54 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MonthName_Short]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 55 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MonthName_FirstLetter]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 56 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Quarter]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 57 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[QuarterName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 58 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Year]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 59 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MMYYYY]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 60 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MonthYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 61 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[IsWeekend]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 62 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[IsHoliday]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 63 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[HolidayName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 64 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[SpecialDays]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 65 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FinancialYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 66 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FinancialQuater]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 67 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FinancialMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 68 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FirstDateofYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 69 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[LastDateofYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 70 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FirstDateofQuater]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 71 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[LastDateofQuater]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 72 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FirstDateofMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 73 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[LastDateofMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 74 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FirstDateofWeek]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 75 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[LastDateofWeek]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 76 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 77 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentQuater]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 78 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 79 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentWeek]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 80 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentDay]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 82 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[GeographyID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 83 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[GeographyName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 84 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[COUNTRY]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 85 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[ISO]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 86 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[UN]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 87 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[Num]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 88 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[DialingCode]', 'COLUMN') as PermCheckResult\n",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:47.616",
         "6": "2025-03-16 22:45:47.633",
         "7": "16",
         "8": "47",
         "9": "Success"
        },
        {
         "0": "DirectQueryEnd",
         "1": null,
         "2": "2025-03-16 22:45:49.133",
         "3": "\nSELECT 0 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[fact_myevents_1bln]')\n\nUNION ALL\n\nSELECT 1 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[fact_myevents_1bln_no_vorder]')\n\nUNION ALL\n\nSELECT 2 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[fact_myevents_1bln_partitioned_datekey]')\n\nUNION ALL\n\nSELECT 4 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[dim_Date]')\n\nUNION ALL\n\nSELECT 5 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[dim_Geography]')\n",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:49.116",
         "6": "2025-03-16 22:45:49.133",
         "7": "16",
         "8": "63",
         "9": "Success"
        },
        {
         "0": "VertiPaqSEQueryEnd",
         "1": "VertiPaqScan",
         "2": "2025-03-16 22:45:49.396",
         "3": "SET DC_KIND=\"AUTO\";\r\nSELECT\r\n[dim Geography (30)].[COUNTRY (112)] AS [dim Geography (30)$COUNTRY (112)],\r\nSUM([fact myevents 1bln (15)].[Quantity ThisYear (39)]) AS [$Measure0]\r\nFROM [fact myevents 1bln (15)]\r\n\tLEFT OUTER JOIN [dim Date (27)] ON [fact myevents 1bln (15)].[DateKey (34)]=[dim Date (27)].[DateKey (70)]\r\n\tLEFT OUTER JOIN [dim Geography (30)] ON [fact myevents 1bln (15)].[GeographyID (38)]=[dim Geography (30)].[GeographyID (110)]\r\nWHERE\r\n\t[dim Date (27)].[FirstDateofMonth (101)] = 43466.000000;\r\n\r\n\r\n[Estimated size (volume, marshalling bytes): 247, 3952]\r\n",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:49.133",
         "6": "2025-03-16 22:45:49.396",
         "7": "266",
         "8": "3656",
         "9": "Success"
        },
        {
         "0": "QueryEnd",
         "1": "DAXQuery",
         "2": "2025-03-16 22:45:49.413",
         "3": "DEFINE\n\n        MEASURE dim_Date[Sum of Quantity] = \n            SUM(fact_myevents_1bln[Quantity_ThisYear])\n            \n\t    MEASURE dim_Date[Percentage of Parent] =\n\t\t    [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n\n    EVALUATE\n        SUMMARIZECOLUMNS(\n            dim_Geography[COUNTRY] ,\n            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n            \"Quantity\" \t\t, [Sum of Quantity],\n            \"% of Parent\"\t, [Percentage of Parent]\n            )\r\n\r\n[WaitTime: 0 ms]",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:45.586",
         "6": "2025-03-16 22:45:49.413",
         "7": "3828",
         "8": "3734",
         "9": "Success"
        },
        {
         "0": "ExecutionMetrics",
         "1": null,
         "2": "NaT",
         "3": "{\n\t\"timeStart\": \"2025-03-16T22:45:45.586Z\",\n\t\"timeEnd\": \"2025-03-16T22:45:49.414Z\",\n\n\t\"durationMs\": 3828,\n\t\"datasourceConnectionThrottleTimeMs\": 0,\n\t\"externalQueryExecutionTimeMs\": 21,\n\t\"vertipaqJobCpuTimeMs\": 3656,\n\t\"queryProcessingCpuTimeMs\": 78,\n\t\"totalCpuTimeMs\": 3734,\n\t\"executionDelayMs\": 0,\n\n\t\"approximatePeakMemConsumptionKB\": 5244,\n\n\t\"directQueryTimeoutMs\": 3597000,\n\t\"tabularConnectionTimeoutMs\": 3600000,\n\n\t\"commandType\": \"Statement\",\n\t\"queryDialect\": 3,\n\t\"queryResultRows\": 239\n}",
         "4": null,
         "5": "NaT",
         "6": "NaT",
         "7": null,
         "8": null,
         "9": null
        }
       ],
       "schema": [
        {
         "key": "0",
         "name": "Event Class",
         "type": "string"
        },
        {
         "key": "1",
         "name": "Event Subclass",
         "type": "string"
        },
        {
         "key": "2",
         "name": "Current Time",
         "type": "timestamp"
        },
        {
         "key": "3",
         "name": "Text Data",
         "type": "string"
        },
        {
         "key": "4",
         "name": "Session ID",
         "type": "string"
        },
        {
         "key": "5",
         "name": "Start Time",
         "type": "timestamp"
        },
        {
         "key": "6",
         "name": "End Time",
         "type": "timestamp"
        },
        {
         "key": "7",
         "name": "Duration",
         "type": "bigint"
        },
        {
         "key": "8",
         "name": "Cpu Time",
         "type": "bigint"
        },
        {
         "key": "9",
         "name": "Success",
         "type": "string"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "dataframeType": "pandas"
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
