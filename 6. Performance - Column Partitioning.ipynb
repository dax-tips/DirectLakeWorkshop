{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10b5d50-1798-4f0d-acd4-5728791368f7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 6: Performance — Column Partitioning\n",
    "\n",
    "This lab compares query performance between a standard (non-partitioned) table and a column-partitioned table inside a Direct Lake semantic model. You will run the same set of DAX query patterns against both tables, capturing server timing metrics each time, so you can measure the impact of column partitioning on cold-cache and warm-cache execution.\n",
    "\n",
    "**Prerequisites:** Lab 2 must be completed (Big Data lakehouse and semantic model).\n",
    "\n",
    "---\n",
    "\n",
    "*Dieses Lab vergleicht die Abfrageleistung zwischen einer Standard-Tabelle (nicht partitioniert) und einer spaltenpartitionierten Tabelle in einem Direct Lake Semantic Model. Sie fuehren dieselben DAX-Abfragemuster gegen beide Tabellen aus, erfassen jeweils Server-Timing-Metriken und koennen so die Auswirkungen der Spaltenpartitionierung auf die Cold-Cache- und Warm-Cache-Ausfuehrung messen.*\n",
    "\n",
    "*Voraussetzung: Lab 2 muss abgeschlossen sein (Big Data Lakehouse und Semantic Model).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0d833-91cf-4c94-acc4-17a71e453a13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Install Semantic Link Labs\n",
    "\n",
    "Install the Semantic Link Labs Python library used throughout this lab.\n",
    "\n",
    "---\n",
    "\n",
    "*Installieren Sie die Semantic Link Labs Python-Bibliothek, die in diesem Lab verwendet wird.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ba3207-5db8-4a88-a3ec-849a28f4c8f1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04495fa-42ae-41c9-a701-8f9c635fb5ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Load Python Libraries\n",
    "\n",
    "Import the required libraries and attach to the BigData lakehouse.\n",
    "\n",
    "---\n",
    "\n",
    "*Importieren Sie die erforderlichen Bibliotheken und verbinden Sie sich mit dem BigData-Lakehouse.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76cd66-7fc7-4daf-a648-e0d7b1bb6e60",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries for performance analysis and column partitioning optimization\n",
    "import sempy_labs as labs\n",
    "from sempy import fabric\n",
    "import sempy\n",
    "import pandas\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Configure BigData lakehouse for column partitioning experiments\n",
    "LakehouseName = \"BigData\"\n",
    "lakehouses = labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "for l in lakehouses:\n",
    "    if l.startswith(\"Big\"):\n",
    "        LakehouseName = l\n",
    "\n",
    "# Set up semantic model for performance testing\n",
    "SemanticModelName = f\"{LakehouseName}_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6978e8d5-5d86-4da6-9567-8a1e5c49bfb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Setup Parameters\n",
    "\n",
    "Define workspace and lakehouse identifiers used by subsequent steps.\n",
    "\n",
    "---\n",
    "\n",
    "*Definieren Sie die Workspace- und Lakehouse-Bezeichner, die in den folgenden Schritten verwendet werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5780deb-4ae1-45a3-a8ea-b67bd6e57a86",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Validate BigData lakehouse exists (required from Lab 2)\n",
    "lakehouses=labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "if LakehouseName in lakehouses.values:\n",
    "    lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]\n",
    "else:\n",
    "    print(\"You need to complete Lab 2 to create the required lakehouse for this lab\")\n",
    "\n",
    "# Configure workspace parameters for performance testing\n",
    "workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "workspaceName = sempy.fabric.resolve_workspace_name(workspaceId)\n",
    "print(f\"WorkspaceId = {workspaceId}, LakehouseID = {lakehouseId}, Workspace Name = {workspaceName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f76ea7-bd3e-44c7-adce-4b5660da8b57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 4: Create DAX Query Function with Server Timings\n",
    "\n",
    "Build a helper function that executes a DAX query while capturing server timing traces. This function is used in later steps to measure cold-cache and warm-cache performance.\n",
    "\n",
    "---\n",
    "\n",
    "*Erstellen Sie eine Hilfsfunktion, die eine DAX-Abfrage ausfuehrt und dabei Server-Timing-Traces erfasst. Diese Funktion wird in spaeteren Schritten zur Messung der Cold-Cache- und Warm-Cache-Leistung verwendet.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34aab3-49e8-40fd-8a57-5a5a36cace2d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "\n",
    "#### Generate Unique Trace Name - Start ####\n",
    "import json, base64, re\n",
    "token = notebookutils.credentials.getToken(\"pbi\")\n",
    "payload = token.split(\".\")[1]\n",
    "payload += \"=\" * (4 - len(payload) % 4)\n",
    "upn = json.loads(base64.b64decode(payload)).get(\"upn\")\n",
    "\n",
    "# Extract just the user part (e.g. \"SQLKDL.user39\")\n",
    "user_id = upn.split(\"@\")[0]\n",
    "lab_number = 6  # set per lab\n",
    "\n",
    "# Remove characters not allowed in trace names: . , ; ' ` : / \\ * | ? \" & % $ ! + = ( ) [ ] { } < >\n",
    "user_id_clean = re.sub(r\"[.,;'`:/\\\\*|?\\\"&%$!+=(){}\\[\\]<>]\", \"_\", user_id)\n",
    "trace_name = f\"Lab{lab_number}_{user_id_clean}\"\n",
    "#### Generate Unique Trace Name - End ####\n",
    "\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)\n",
    "\n",
    "def filter_func(e):\n",
    "    retVal:bool=True\n",
    "    if e.EventSubclass.ToString() == \"VertiPaqScanInternal\":\n",
    "        retVal=False      \n",
    "    #     #if e.EventSubClass.ToString() == \"VertiPaqScanInternal\":\n",
    "    #     retVal=False\n",
    "    return retVal\n",
    "\n",
    "# define events to trace and their corresponding columns\n",
    "def runQueryWithTrace (expr:str,workspaceName:str,SemanticModelName:str,Result:Optional[bool]=True,Trace:Optional[bool]=True,DMV:Optional[bool]=True,ClearCache:Optional[bool]=True) -> pandas.DataFrame :\n",
    "    event_schema = fabric.Trace.get_default_query_trace_schema()\n",
    "    event_schema.update({\"ExecutionMetrics\":[\"EventClass\",\"TextData\"]})\n",
    "    del event_schema['VertiPaqSEQueryBegin']\n",
    "    del event_schema['VertiPaqSEQueryCacheMatch']\n",
    "    del event_schema['DirectQueryBegin']\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    if ClearCache:\n",
    "        labs.clear_cache(SemanticModelName)\n",
    "\n",
    "    WorkspaceName:str = workspaceName\n",
    "    SemanticModelName:str = SemanticModelName\n",
    "\n",
    "    with fabric.create_trace_connection(SemanticModelName,WorkspaceName) as trace_connection:\n",
    "        # create trace on server with specified events\n",
    "        with trace_connection.create_trace(\n",
    "            event_schema=event_schema, \n",
    "            name=trace_name,\n",
    "            filter_predicate=filter_func,\n",
    "            stop_event=\"QueryEnd\"\n",
    "            ) as trace:\n",
    "\n",
    "            trace.start()\n",
    "\n",
    "            df:FabricDataFrame=sempy.fabric.evaluate_dax(\n",
    "                dataset=SemanticModelName, \n",
    "                dax_string=expr)\n",
    "\n",
    "            if Result:\n",
    "                displayHTML(f\"<H2>####### DAX QUERY RESULT #######</H2>\")\n",
    "                display(df)\n",
    "\n",
    "            # Wait 5 seconds for trace data to arrive\n",
    "            time.sleep(5)\n",
    "\n",
    "            # stop Trace and collect logs\n",
    "            final_trace_logs:pandas.DataFrame = trace.stop()\n",
    "\n",
    "    if Trace:\n",
    "        displayHTML(f\"<H2>####### SERVER TIMINGS #######</H2>\")\n",
    "        display(final_trace_logs)\n",
    "    \n",
    "    if DMV:\n",
    "        displayHTML(f\"<H2>####### SHOW DMV RESULTS #######</H2>\")\n",
    "        runDMV()\n",
    "\n",
    "    return final_trace_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35770fb9-3a0f-44b7-b5d6-5982e6c722d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 5: Reframe the Semantic Model\n",
    "\n",
    "Reframe the model so it picks up the latest data from the lakehouse before running performance tests.\n",
    "\n",
    "---\n",
    "\n",
    "*Reframen Sie das Modell, damit es die neuesten Daten aus dem Lakehouse uebernimmt, bevor die Leistungstests durchgefuehrt werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e354f-b155-4cc0-891c-3dcf8a00211f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Refresh the semantic model to ensure all data changes are synchronized\n",
    "labs.refresh_semantic_model(SemanticModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91296d1-13d3-48f9-9b0c-8ab63b628edd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 6: Run Vertipaq Analyzer\n",
    "\n",
    "Generate column-level storage statistics using Vertipaq Analyzer. Note that this analyses data held in the semantic model, not the Delta table directly.\n",
    "\n",
    "---\n",
    "\n",
    "*Erzeugen Sie spaltenbasierte Speicherstatistiken mit dem Vertipaq Analyzer. Beachten Sie, dass dies die Daten im Semantic Model analysiert, nicht direkt die Delta-Tabelle.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2b265-d3e7-44f8-b223-724735945849",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "analyzer:dict[str,pandas.DataFrame] = labs.vertipaq_analyzer(dataset=SemanticModelName)\n",
    "\n",
    "for key, value in analyzer.items():\n",
    "    print(key)\n",
    "    display(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5acf8-920e-4bed-93cb-a24f95417ddc",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(analyzer[\"Columns\"].query(\"`Column Name`=='DateKey' & `Is Resident`==True\"))\n",
    "display(analyzer[\"Columns\"].query(\"`Column Name`=='Quantity_ThisYear' & `Is Resident`==True\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de6a700-c2f8-457d-bc3d-e298cd5cf0ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 7: Run DAX Queries — Base vs. Partitioned\n",
    "\n",
    "The following sub-steps run five DAX query patterns. Each pattern is executed first against the base (non-partitioned) table, then against the column-partitioned table, so you can compare server timing results side by side.\n",
    "\n",
    "---\n",
    "\n",
    "*Die folgenden Teilschritte fuehren fuenf DAX-Abfragemuster aus. Jedes Muster wird zuerst gegen die Basis-Tabelle (nicht partitioniert) und dann gegen die spaltenpartitionierte Tabelle ausgefuehrt, sodass Sie die Server-Timing-Ergebnisse direkt vergleichen koennen.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee06b59-cebe-4389-b8f6-b691b6865f50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 7.1 Period Comparison\n",
    "\n",
    "---\n",
    "\n",
    "*Periodenvergleich*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11844eb9-195b-44a8-8714-f66325272f5f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Period Comparison against the base table\n",
    "\n",
    "---\n",
    "\n",
    "*Periodenvergleich gegen die Basis-Tabelle ausfuehren*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582e3cb-b971-4c96-90fc-6b38e7c74096",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Sum of Quantity PM] =\n",
    "            CALCULATE([Sum of Quantity],PREVIOUSMONTH(dim_Date[DateKey]))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM Delta] =\n",
    "            [Sum of Quantity] - [Sum of Quantity PM]\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity PM %] =\n",
    "            [Sum of Quantity PM Delta] / [Sum of Quantity]\n",
    "        \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            -- GROUP BY --\n",
    "            dim_Date[FirstDateofMonth] ,\n",
    "            --  FILTER  --\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofYear] ) ,\n",
    "             -- MEASURES --\n",
    "            \"Quantity\" \t\t\t\t, [Sum of Quantity],\n",
    "            \"Quantity PM\" \t\t\t, [Sum of Quantity PM],\n",
    "            \"Quantity PM Delta\"\t\t, [Sum of Quantity PM Delta] ,\n",
    "            \"Quantity PM % \" \t\t, [Sum of Quantity PM %]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "trace1 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False,Trace=False)\n",
    "trace1 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False,Trace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557720c-3d34-49bb-9a2d-01ed98d5bb92",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Period Comparison against the partitioned table\n",
    "\n",
    "---\n",
    "\n",
    "*Periodenvergleich gegen die partitionierte Tabelle ausfuehren*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b839f489-5cc7-42f7-96de-7bcf4c2e382e",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Sum of Quantity PM] =\n",
    "            CALCULATE([Sum of Quantity],PREVIOUSMONTH(dim_Date[DateKey]))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM Delta] =\n",
    "            [Sum of Quantity] - [Sum of Quantity PM]\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity PM %] =\n",
    "            [Sum of Quantity PM Delta] / [Sum of Quantity]\n",
    "        \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            -- GROUP BY --\n",
    "            dim_Date[FirstDateofMonth] ,\n",
    "            --  FILTER  --\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofYear] ) ,\n",
    "             -- MEASURES --\n",
    "            \"Quantity\" \t\t\t\t, [Sum of Quantity],\n",
    "            \"Quantity PM\" \t\t\t, [Sum of Quantity PM],\n",
    "            \"Quantity PM Delta\"\t\t, [Sum of Quantity PM Delta] ,\n",
    "            \"Quantity PM % \" \t\t, [Sum of Quantity PM %]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    trace1 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False,Trace=False)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    trace1 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False,Trace=False)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9fd2c-1602-4bcf-94d8-e4dbd2984025",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(trace1)\n",
    "display(trace2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3392f85-83ee-420e-b404-509e106150ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 7.2 Running Total\n",
    "\n",
    "---\n",
    "\n",
    "*Laufende Summe*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611df2ab-2004-46bc-ba00-83965637d539",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Running Total against the base table\n",
    "\n",
    "---\n",
    "\n",
    "*Laufende Summe gegen die Basis-Tabelle ausfuehren*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf42cc-5368-4f9e-bd52-de134df94d28",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "\t    MEASURE dim_Date[Sum of Quantity YTD] =\n",
    "\t\t    TOTALYTD([Sum of Quantity],dim_Date[DateKey])\n",
    "\t\n",
    "\t    MEASURE fact_myevents_1bln[Sum of Quantity QTD] =\n",
    "\t\t    TOTALQTD([Sum of Quantity],dim_Date[DateKey])\t\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            -- GROUP BY --\n",
    "            dim_Date[FirstDateofMonth] ,\n",
    "            --  FILTER  --\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofYear] ) ,\n",
    "             -- MEASURES --\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"Quantity YTD\" \t, [Sum of Quantity YTD] ,\n",
    "            \"Quantity QTD\" \t, [Sum of Quantity QTD]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace3 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a164621-4808-400d-a2f2-967738b972f2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Running Total against the partitioned table\n",
    "\n",
    "---\n",
    "\n",
    "*Laufende Summe gegen die partitionierte Tabelle ausfuehren*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b249175-5118-4828-995f-c6dec0592dda",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str=\"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "\t    MEASURE dim_Date[Sum of Quantity YTD] =\n",
    "\t\t    TOTALYTD([Sum of Quantity],dim_Date[DateKey])\n",
    "\t\n",
    "\t    MEASURE fact_myevents_1bln[Sum of Quantity QTD] =\n",
    "\t\t    TOTALQTD([Sum of Quantity],dim_Date[DateKey])\t\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            -- GROUP BY --\n",
    "            dim_Date[FirstDateofMonth] ,\n",
    "            --  FILTER  --\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofYear] ) ,\n",
    "             -- MEASURES --\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"Quantity YTD\" \t, [Sum of Quantity YTD] ,\n",
    "            \"Quantity QTD\" \t, [Sum of Quantity QTD]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace4 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trace3)\n",
    "display(trace4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33899584-56e9-4cef-a16a-427b57c3d00f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 7.3 RANK\n",
    "\n",
    "---\n",
    "\n",
    "*Rangfolge*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab8de0-f4d8-48bf-811a-82556f8dcd67",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run RANK against the base table\n",
    "\n",
    "---\n",
    "\n",
    "*RANK gegen die Basis-Tabelle ausfuehren*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db7eec-35e7-46a1-b40b-bb87379a220a",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Sum of Quantity Rank] =\n",
    "            RANKX(ALL(dim_Geography[COUNTRY]) , [Sum of Quantity] )\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"Rank\" \t\t\t, [Sum of Quantity Rank]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace5 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7c9d2-cff0-4332-9b34-d06acfaa3fe5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run RANK against the partitioned table\n",
    "\n",
    "---\n",
    "\n",
    "*RANK gegen die partitionierte Tabelle ausfuehren*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcdf8ea-4991-443f-89f8-44e1f47773c3",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Sum of Quantity Rank] =\n",
    "            RANKX(ALL(dim_Geography[COUNTRY]) , [Sum of Quantity] )\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"Rank\" \t\t\t, [Sum of Quantity Rank]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace6 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efb33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trace5)\n",
    "display(trace6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf39bc48-bca0-4ca3-928e-b69ed899fe81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 7.4 Percent of Parent\n",
    "\n",
    "---\n",
    "\n",
    "*Prozent des uebergeordneten Elements*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c749bb-170b-4d04-871f-3bc4861aa2a8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Percent of Parent against the base table\n",
    "\n",
    "---\n",
    "\n",
    "*Prozent des uebergeordneten Elements gegen die Basis-Tabelle ausfuehren*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcec580-b64d-4858-b8a4-58db8dda5acc",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "\t    MEASURE dim_Date[Percentage of Parent] =\n",
    "\t\t    [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"% of Parent\"\t, [Percentage of Parent]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace7 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a8ead-88b6-423d-9735-f534d8cb2704",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Percent of Parent against the partitioned table\n",
    "\n",
    "---\n",
    "\n",
    "*Prozent des uebergeordneten Elements gegen die partitionierte Tabelle ausfuehren*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecd3ce-5ae1-4046-b75d-677bab283f27",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "\t    MEASURE dim_Date[Percentage of Parent] =\n",
    "\t\t    [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"% of Parent\"\t, [Percentage of Parent]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace8 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51825ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trace7)\n",
    "display(trace8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce2c6a5-871a-4e58-acbf-9639ecc34a7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 7.5 All Measures Combined\n",
    "\n",
    "Run all five query patterns in a single DAX query to compare total execution time.\n",
    "\n",
    "---\n",
    "\n",
    "*Fuehren Sie alle fuenf Abfragemuster in einer einzigen DAX-Abfrage aus, um die Gesamtausfuehrungszeit zu vergleichen.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94737720-ebb1-438a-b0c0-90eb96e3ef87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run all measures against the base table\n",
    "\n",
    "---\n",
    "\n",
    "*Alle Measures gegen die Basis-Tabelle ausfuehren*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b39d3-ddfb-4c67-887a-a1b1957367bf",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Percentage of Parent] =\n",
    "            [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity Rank] =\n",
    "            RANKX(ALL(dim_Geography[COUNTRY]) , [Sum of Quantity] )\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity YTD] =\n",
    "            TOTALYTD([Sum of Quantity],dim_Date[DateKey])\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity QTD] =\n",
    "            TOTALQTD([Sum of Quantity],dim_Date[DateKey])\t\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM] =\n",
    "            CALCULATE([Sum of Quantity],PREVIOUSMONTH(dim_Date[DateKey]))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM Delta] =\n",
    "            [Sum of Quantity] - [Sum of Quantity PM]\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity PM %] =\n",
    "            [Sum of Quantity PM Delta] / [Sum of Quantity]\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "            \"Quantity\" \t\t\t\t, [Sum of Quantity],\n",
    "            \"% of Parent\"\t\t\t, [Percentage of Parent],\n",
    "            \"Rank\" \t\t\t\t\t, [Sum of Quantity Rank],\n",
    "            \"Quantity YTD\" \t\t\t, [Sum of Quantity YTD] ,\n",
    "            \"Quantity QTD\" \t\t\t, [Sum of Quantity QTD]\t,\t\n",
    "            \"Quantity PM\" \t\t\t, [Sum of Quantity PM],\n",
    "            \"Quantity PM Delta\"\t\t, [Sum of Quantity PM Delta] ,\n",
    "            \"Quantity PM %\" \t\t, [Sum of Quantity PM %]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace9 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea9574-19e5-4249-aaa3-ca55cb72ecf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run all measures against the partitioned table\n",
    "\n",
    "---\n",
    "\n",
    "*Alle Measures gegen die partitionierte Tabelle ausfuehren*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807265ad-5fb0-4fd5-8acc-64c52ab07c3c",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Percentage of Parent] =\n",
    "            [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity Rank] =\n",
    "            RANKX(ALL(dim_Geography[COUNTRY]) , [Sum of Quantity] )\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity YTD] =\n",
    "            TOTALYTD([Sum of Quantity],dim_Date[DateKey])\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity QTD] =\n",
    "            TOTALQTD([Sum of Quantity],dim_Date[DateKey])\t\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM] =\n",
    "            CALCULATE([Sum of Quantity],PREVIOUSMONTH(dim_Date[DateKey]))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM Delta] =\n",
    "            [Sum of Quantity] - [Sum of Quantity PM]\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity PM %] =\n",
    "            [Sum of Quantity PM Delta] / [Sum of Quantity]\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "            \"Quantity\" \t\t\t\t, [Sum of Quantity],\n",
    "            \"% of Parent\"\t\t\t, [Percentage of Parent],\n",
    "            \"Rank\" \t\t\t\t\t, [Sum of Quantity Rank],\n",
    "            \"Quantity YTD\" \t\t\t, [Sum of Quantity YTD] ,\n",
    "            \"Quantity QTD\" \t\t\t, [Sum of Quantity QTD]\t,\t\n",
    "            \"Quantity PM\" \t\t\t, [Sum of Quantity PM],\n",
    "            \"Quantity PM Delta\"\t\t, [Sum of Quantity PM Delta] ,\n",
    "            \"Quantity PM %\" \t\t, [Sum of Quantity PM %]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace10 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trace9)\n",
    "display(trace10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa6f6e",
   "metadata": {},
   "source": [
    "## Step 8: Stop the Spark Session\n",
    "\n",
    "---\n",
    "\n",
    "*Spark-Sitzung beenden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6270d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc4009",
   "metadata": {},
   "source": [
    "## Lab Summary\n",
    "\n",
    "In this lab you measured the performance difference between a standard table and a column-partitioned table using five common DAX query patterns: period comparison, running total, RANK, percent of parent, and a combined query. Server timing traces were captured for each execution to quantify the impact.\n",
    "\n",
    "**Key Takeaways**\n",
    "- Column partitioning reorganises how data is stored on disk, which can significantly reduce the amount of data scanned during query execution.\n",
    "- Cold-cache performance (first query after a reframe) is where partitioning tends to show the largest benefit, because less data needs to be paged into memory.\n",
    "- Warm-cache queries benefit less from partitioning since data is already resident in memory, but I/O savings can still be observed.\n",
    "- Vertipaq Analyzer provides the column-level storage statistics needed to identify partitioning candidates.\n",
    "- Server timing traces are essential for objective performance comparison — avoid relying on perceived speed alone.\n",
    "\n",
    "**Next Lab:** Lab 7 explores splitting high cardinality columns to reduce model size and improve query performance.\n",
    "\n",
    "---\n",
    "\n",
    "*In diesem Lab haben Sie den Leistungsunterschied zwischen einer Standard-Tabelle und einer spaltenpartitionierten Tabelle anhand von fuenf gaengigen DAX-Abfragemustern gemessen: Periodenvergleich, laufende Summe, RANK, Prozent des uebergeordneten Elements und eine kombinierte Abfrage. Fuer jede Ausfuehrung wurden Server-Timing-Traces erfasst, um die Auswirkungen zu quantifizieren.*\n",
    "\n",
    "**Wichtige Erkenntnisse**\n",
    "- Spaltenpartitionierung organisiert die Datenspeicherung auf der Festplatte neu, was die Menge der bei der Abfrageausfuehrung gescannten Daten erheblich reduzieren kann.\n",
    "- Die Cold-Cache-Leistung (erste Abfrage nach einem Reframe) profitiert am meisten von der Partitionierung, da weniger Daten in den Arbeitsspeicher geladen werden muessen.\n",
    "- Warm-Cache-Abfragen profitieren weniger, da die Daten bereits im Speicher liegen, aber E/A-Einsparungen koennen dennoch beobachtet werden.\n",
    "- Der Vertipaq Analyzer liefert die spaltenbasierten Speicherstatistiken, die zur Identifizierung von Partitionierungskandidaten erforderlich sind.\n",
    "- Server-Timing-Traces sind fuer einen objektiven Leistungsvergleich unerlaesslich — verlassen Sie sich nicht nur auf die gefuehlte Geschwindigkeit.\n",
    "\n",
    "**Naechstes Lab:** Lab 7 untersucht das Aufteilen von Spalten mit hoher Kardinalitaet, um die Modellgroesse zu reduzieren und die Abfrageleistung zu verbessern."
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {
    "0408074c-1014-4c64-982e-bf06ad39bd42": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "count",
        "binsNumber": 10,
        "categoryFieldKeys": [
         "0"
        ],
        "chartType": "bar",
        "evaluatesOverAllRecords": false,
        "isStacked": false,
        "seriesFieldKeys": [
         "0"
        ],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "python",
      "table": {
       "rows": [
        {
         "0": "QueryBegin",
         "1": "DAXQuery",
         "2": "2025-03-16 22:45:45.600",
         "3": "DEFINE\n\n        MEASURE dim_Date[Sum of Quantity] = \n            SUM(fact_myevents_1bln[Quantity_ThisYear])\n            \n\t    MEASURE dim_Date[Percentage of Parent] =\n\t\t    [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n\n    EVALUATE\n        SUMMARIZECOLUMNS(\n            dim_Geography[COUNTRY] ,\n            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n            \"Quantity\" \t\t, [Sum of Quantity],\n            \"% of Parent\"\t, [Percentage of Parent]\n            )",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:45.586",
         "6": "NaT",
         "7": null,
         "8": null,
         "9": null
        },
        {
         "0": "DirectQueryEnd",
         "1": null,
         "2": "2025-03-16 22:45:47.053",
         "3": "\nSELECT sch.name AS schemaname,\ntbl.name AS tablename,\nc.name AS columnname\nFROM sys.masked_columns AS c\nJOIN sys.tables AS tbl ON c.object_id = tbl.object_id\nJOIN sys.schemas AS sch ON tbl.schema_id = sch.schema_id\nWHERE c.is_masked = 1;\n",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:47.053",
         "6": "2025-03-16 22:45:47.053",
         "7": "0",
         "8": "31",
         "9": "Success"
        },
        {
         "0": "DirectQueryEnd",
         "1": null,
         "2": "2025-03-16 22:45:47.633",
         "3": "\nSELECT 1 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[DateKey]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 2 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[UserID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 3 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[OperatingSystemID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 4 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[OperatingSystemVendorID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 5 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[GeographyID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 6 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[Quantity_ThisYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 7 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[Quantity_LastYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 8 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[HLL_HashBucket]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 9 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[HLL_FirstZero]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 11 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[DateKey]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 12 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[UserID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 13 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[OperatingSystemID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 14 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[OperatingSystemVendorID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 15 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[GeographyID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 16 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[Quantity_ThisYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 17 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[Quantity_LastYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 18 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[HLL_HashBucket]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 19 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[HLL_FirstZero]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 21 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[DateKey]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 22 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[UserID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 23 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[OperatingSystemID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 24 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[OperatingSystemVendorID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 25 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[GeographyID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 26 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[Quantity_ThisYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 27 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[Quantity_LastYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 28 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[HLL_HashBucket]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 29 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[HLL_FirstZero]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 41 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[DateKey]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 42 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Day]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 43 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[DaySuffix]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 44 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Weekday]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 45 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekDayName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 46 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekDayName_Short]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 47 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekDayName_FirstLetter]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 48 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[DOWInMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 49 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[DayOfYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 50 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekOfMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 51 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekOfYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 52 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Month]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 53 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MonthName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 54 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MonthName_Short]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 55 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MonthName_FirstLetter]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 56 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Quarter]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 57 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[QuarterName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 58 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Year]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 59 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MMYYYY]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 60 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MonthYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 61 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[IsWeekend]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 62 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[IsHoliday]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 63 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[HolidayName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 64 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[SpecialDays]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 65 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FinancialYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 66 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FinancialQuater]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 67 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FinancialMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 68 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FirstDateofYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 69 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[LastDateofYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 70 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FirstDateofQuater]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 71 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[LastDateofQuater]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 72 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FirstDateofMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 73 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[LastDateofMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 74 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FirstDateofWeek]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 75 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[LastDateofWeek]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 76 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 77 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentQuater]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 78 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 79 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentWeek]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 80 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentDay]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 82 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[GeographyID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 83 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[GeographyName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 84 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[COUNTRY]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 85 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[ISO]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 86 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[UN]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 87 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[Num]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 88 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[DialingCode]', 'COLUMN') as PermCheckResult\n",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:47.616",
         "6": "2025-03-16 22:45:47.633",
         "7": "16",
         "8": "47",
         "9": "Success"
        },
        {
         "0": "DirectQueryEnd",
         "1": null,
         "2": "2025-03-16 22:45:49.133",
         "3": "\nSELECT 0 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[fact_myevents_1bln]')\n\nUNION ALL\n\nSELECT 1 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[fact_myevents_1bln_no_vorder]')\n\nUNION ALL\n\nSELECT 2 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[fact_myevents_1bln_partitioned_datekey]')\n\nUNION ALL\n\nSELECT 4 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[dim_Date]')\n\nUNION ALL\n\nSELECT 5 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[dim_Geography]')\n",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:49.116",
         "6": "2025-03-16 22:45:49.133",
         "7": "16",
         "8": "63",
         "9": "Success"
        },
        {
         "0": "VertiPaqSEQueryEnd",
         "1": "VertiPaqScan",
         "2": "2025-03-16 22:45:49.396",
         "3": "SET DC_KIND=\"AUTO\";\r\nSELECT\r\n[dim Geography (30)].[COUNTRY (112)] AS [dim Geography (30)$COUNTRY (112)],\r\nSUM([fact myevents 1bln (15)].[Quantity ThisYear (39)]) AS [$Measure0]\r\nFROM [fact myevents 1bln (15)]\r\n\tLEFT OUTER JOIN [dim Date (27)] ON [fact myevents 1bln (15)].[DateKey (34)]=[dim Date (27)].[DateKey (70)]\r\n\tLEFT OUTER JOIN [dim Geography (30)] ON [fact myevents 1bln (15)].[GeographyID (38)]=[dim Geography (30)].[GeographyID (110)]\r\nWHERE\r\n\t[dim Date (27)].[FirstDateofMonth (101)] = 43466.000000;\r\n\r\n\r\n[Estimated size (volume, marshalling bytes): 247, 3952]\r\n",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:49.133",
         "6": "2025-03-16 22:45:49.396",
         "7": "266",
         "8": "3656",
         "9": "Success"
        },
        {
         "0": "QueryEnd",
         "1": "DAXQuery",
         "2": "2025-03-16 22:45:49.413",
         "3": "DEFINE\n\n        MEASURE dim_Date[Sum of Quantity] = \n            SUM(fact_myevents_1bln[Quantity_ThisYear])\n            \n\t    MEASURE dim_Date[Percentage of Parent] =\n\t\t    [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n\n    EVALUATE\n        SUMMARIZECOLUMNS(\n            dim_Geography[COUNTRY] ,\n            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n            \"Quantity\" \t\t, [Sum of Quantity],\n            \"% of Parent\"\t, [Percentage of Parent]\n            )\r\n\r\n[WaitTime: 0 ms]",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:45.586",
         "6": "2025-03-16 22:45:49.413",
         "7": "3828",
         "8": "3734",
         "9": "Success"
        },
        {
         "0": "ExecutionMetrics",
         "1": null,
         "2": "NaT",
         "3": "{\n\t\"timeStart\": \"2025-03-16T22:45:45.586Z\",\n\t\"timeEnd\": \"2025-03-16T22:45:49.414Z\",\n\n\t\"durationMs\": 3828,\n\t\"datasourceConnectionThrottleTimeMs\": 0,\n\t\"externalQueryExecutionTimeMs\": 21,\n\t\"vertipaqJobCpuTimeMs\": 3656,\n\t\"queryProcessingCpuTimeMs\": 78,\n\t\"totalCpuTimeMs\": 3734,\n\t\"executionDelayMs\": 0,\n\n\t\"approximatePeakMemConsumptionKB\": 5244,\n\n\t\"directQueryTimeoutMs\": 3597000,\n\t\"tabularConnectionTimeoutMs\": 3600000,\n\n\t\"commandType\": \"Statement\",\n\t\"queryDialect\": 3,\n\t\"queryResultRows\": 239\n}",
         "4": null,
         "5": "NaT",
         "6": "NaT",
         "7": null,
         "8": null,
         "9": null
        }
       ],
       "schema": [
        {
         "key": "0",
         "name": "Event Class",
         "type": "string"
        },
        {
         "key": "1",
         "name": "Event Subclass",
         "type": "string"
        },
        {
         "key": "2",
         "name": "Current Time",
         "type": "timestamp"
        },
        {
         "key": "3",
         "name": "Text Data",
         "type": "string"
        },
        {
         "key": "4",
         "name": "Session ID",
         "type": "string"
        },
        {
         "key": "5",
         "name": "Start Time",
         "type": "timestamp"
        },
        {
         "key": "6",
         "name": "End Time",
         "type": "timestamp"
        },
        {
         "key": "7",
         "name": "Duration",
         "type": "bigint"
        },
        {
         "key": "8",
         "name": "Cpu Time",
         "type": "bigint"
        },
        {
         "key": "9",
         "name": "Success",
         "type": "string"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "dataframeType": "pandas"
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
