{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10b5d50-1798-4f0d-acd4-5728791368f7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 6: Direct Lake Performance - Column Partitioning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This lab focuses on **column partitioning** optimization techniques in Microsoft Fabric Direct Lake models. Column partitioning is a powerful performance optimization strategy that improves query performance by organizing data storage to minimize I/O operations and enhance compression efficiency. Understanding these techniques is crucial for optimizing large-scale analytics workloads.\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand column partitioning strategies for Direct Lake performance\n",
    "- Compare query performance between partitioned and non-partitioned tables\n",
    "- Analyze cold vs. warm cache performance characteristics\n",
    "- Master advanced DAX query optimization with server timings\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Column Partitioning**: Strategic data organization for optimal query performance\n",
    "- **Cache Performance**: Understanding cold vs. warm cache behavior\n",
    "- **Query Optimization**: Advanced DAX performance tuning techniques\n",
    "- **Server Timings**: Performance analysis and bottleneck identification\n",
    "\n",
    "**Architecture Overview:**\n",
    "\n",
    "**Prerequisites:** Lab 2 completion (Big Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5338115e-f788-4188-b3c6-4451ccf8260a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Performance Focus Areas:**\n",
    "- **Cold Cache Performance**: Initial query execution optimization\n",
    "- **Warm Cache Performance**: Repeated query execution efficiency  \n",
    "- **Column Organization**: Strategic partitioning for optimal data access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0d833-91cf-4c94-acc4-17a71e453a13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Install Semantic Link Labs Python Library\n",
    "Install the Semantic Link Labs library for advanced performance analysis and column partitioning optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ba3207-5db8-4a88-a3ec-849a28f4c8f1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04495fa-42ae-41c9-a701-8f9c635fb5ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2. Load Python Libraries\n",
    "Import required libraries and configure BigData lakehouse for column partitioning performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76cd66-7fc7-4daf-a648-e0d7b1bb6e60",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries for performance analysis and column partitioning optimization\n",
    "import sempy_labs as labs\n",
    "from sempy import fabric\n",
    "import sempy\n",
    "import pandas\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Configure BigData lakehouse for column partitioning experiments\n",
    "LakehouseName = \"BigData\"\n",
    "lakehouses = labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "for l in lakehouses:\n",
    "    if l.startswith(\"Big\"):\n",
    "        LakehouseName = l\n",
    "\n",
    "# Set up semantic model for performance testing\n",
    "SemanticModelName = f\"{LakehouseName}_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6978e8d5-5d86-4da6-9567-8a1e5c49bfb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3. Setup Parameters\n",
    "Configure workspace and lakehouse identifiers required for column partitioning performance tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5780deb-4ae1-45a3-a8ea-b67bd6e57a86",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Validate BigData lakehouse exists (required from Lab 2)\n",
    "lakehouses=labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "if LakehouseName in lakehouses.values:\n",
    "    lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]\n",
    "else:\n",
    "    print(\"You need to complete Lab 2 to create the required lakehouse for this lab\")\n",
    "\n",
    "# Configure workspace parameters for performance testing\n",
    "workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "workspaceName = sempy.fabric.resolve_workspace_name(workspaceId)\n",
    "print(f\"WorkspaceId = {workspaceId}, LakehouseID = {lakehouseId}, Workspace Name = {workspaceName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f76ea7-bd3e-44c7-adce-4b5660da8b57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Create Function to Run DAX Query with Server Timings Trace\n",
    "Build a specialized function to execute DAX queries while capturing detailed server timing metrics for performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34aab3-49e8-40fd-8a57-5a5a36cace2d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "\n",
    "#### Generate Unique Trace Name - Start ####\n",
    "import json, base64\n",
    "token = notebookutils.credentials.getToken(\"pbi\")\n",
    "payload = token.split(\".\")[1]\n",
    "payload += \"=\" * (4 - len(payload) % 4)\n",
    "upn = json.loads(base64.b64decode(payload)).get(\"upn\")\n",
    "\n",
    "# Extract just the user part (e.g. \"SQLKDL.user39\")\n",
    "user_id = upn.split(\"@\")[0]\n",
    "lab_number = 6  # set per lab\n",
    "\n",
    "trace_name = f\"Lab{lab_number}_{user_id}\"\n",
    "#### Generate Unique Trace Name - End ####\n",
    "\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)\n",
    "\n",
    "def filter_func(e):\n",
    "    retVal:bool=True\n",
    "    if e.EventSubclass.ToString() == \"VertiPaqScanInternal\":\n",
    "        retVal=False      \n",
    "    #     #if e.EventSubClass.ToString() == \"VertiPaqScanInternal\":\n",
    "    #     retVal=False\n",
    "    return retVal\n",
    "\n",
    "# define events to trace and their corresponding columns\n",
    "def runQueryWithTrace (expr:str,workspaceName:str,SemanticModelName:str,Result:Optional[bool]=True,Trace:Optional[bool]=True,DMV:Optional[bool]=True,ClearCache:Optional[bool]=True) -> pandas.DataFrame :\n",
    "    event_schema = fabric.Trace.get_default_query_trace_schema()\n",
    "    event_schema.update({\"ExecutionMetrics\":[\"EventClass\",\"TextData\"]})\n",
    "    del event_schema['VertiPaqSEQueryBegin']\n",
    "    del event_schema['VertiPaqSEQueryCacheMatch']\n",
    "    del event_schema['DirectQueryBegin']\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    if ClearCache:\n",
    "        labs.clear_cache(SemanticModelName)\n",
    "\n",
    "    WorkspaceName:str = workspaceName\n",
    "    SemanticModelName:str = SemanticModelName\n",
    "\n",
    "    with fabric.create_trace_connection(SemanticModelName,WorkspaceName) as trace_connection:\n",
    "        # create trace on server with specified events\n",
    "        with trace_connection.create_trace(\n",
    "            event_schema=event_schema, \n",
    "            name=trace_name,\n",
    "            filter_predicate=filter_func,\n",
    "            stop_event=\"QueryEnd\"\n",
    "            ) as trace:\n",
    "\n",
    "            trace.start()\n",
    "\n",
    "            df:FabricDataFrame=sempy.fabric.evaluate_dax(\n",
    "                dataset=SemanticModelName, \n",
    "                dax_string=expr)\n",
    "\n",
    "            if Result:\n",
    "                displayHTML(f\"<H2>####### DAX QUERY RESULT #######</H2>\")\n",
    "                display(df)\n",
    "\n",
    "            # Wait 5 seconds for trace data to arrive\n",
    "            time.sleep(5)\n",
    "\n",
    "            # stop Trace and collect logs\n",
    "            final_trace_logs:pandas.DataFrame = trace.stop()\n",
    "\n",
    "    if Trace:\n",
    "        displayHTML(f\"<H2>####### SERVER TIMINGS #######</H2>\")\n",
    "        display(final_trace_logs)\n",
    "    \n",
    "    if DMV:\n",
    "        displayHTML(f\"<H2>####### SHOW DMV RESULTS #######</H2>\")\n",
    "        runDMV()\n",
    "\n",
    "    return final_trace_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35770fb9-3a0f-44b7-b5d6-5982e6c722d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 5. Reframe Custom Semantic Model\n",
    "Refresh the semantic model to ensure all data changes are synchronized before performance testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e354f-b155-4cc0-891c-3dcf8a00211f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Refresh the semantic model to ensure all data changes are synchronized\n",
    "labs.refresh_semantic_model(SemanticModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91296d1-13d3-48f9-9b0c-8ab63b628edd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 6. Run Vertipaq Analyzer on Semantic Model\n",
    "Generate detailed column storage statistics using Vertipaq Analyzer to understand data distribution and compression.\n",
    "Note this runs over data in the Semantic Model and not the Delta Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2b265-d3e7-44f8-b223-724735945849",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "analyzer:dict[str,pandas.DataFrame] = labs.vertipaq_analyzer(dataset=SemanticModelName)\n",
    "\n",
    "for key, value in analyzer.items():\n",
    "    print(key)\n",
    "    display(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5acf8-920e-4bed-93cb-a24f95417ddc",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(analyzer[\"Columns\"].query(\"`Column Name`=='DateKey' & `Is Resident`==True\"))\n",
    "display(analyzer[\"Columns\"].query(\"`Column Name`=='Quantity_ThisYear' & `Is Resident`==True\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de6a700-c2f8-457d-bc3d-e298cd5cf0ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 8 Run some DAX Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee06b59-cebe-4389-b8f6-b691b6865f50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 8.1 Period Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11844eb9-195b-44a8-8714-f66325272f5f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Period Comparison against **base** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582e3cb-b971-4c96-90fc-6b38e7c74096",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Sum of Quantity PM] =\n",
    "            CALCULATE([Sum of Quantity],PREVIOUSMONTH(dim_Date[DateKey]))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM Delta] =\n",
    "            [Sum of Quantity] - [Sum of Quantity PM]\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity PM %] =\n",
    "            [Sum of Quantity PM Delta] / [Sum of Quantity]\n",
    "        \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            -- GROUP BY --\n",
    "            dim_Date[FirstDateofMonth] ,\n",
    "            --  FILTER  --\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofYear] ) ,\n",
    "             -- MEASURES --\n",
    "            \"Quantity\" \t\t\t\t, [Sum of Quantity],\n",
    "            \"Quantity PM\" \t\t\t, [Sum of Quantity PM],\n",
    "            \"Quantity PM Delta\"\t\t, [Sum of Quantity PM Delta] ,\n",
    "            \"Quantity PM % \" \t\t, [Sum of Quantity PM %]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "trace1 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False,Trace=False)\n",
    "trace1 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False,Trace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557720c-3d34-49bb-9a2d-01ed98d5bb92",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Period Comparison against **Partitioned** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b839f489-5cc7-42f7-96de-7bcf4c2e382e",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Sum of Quantity PM] =\n",
    "            CALCULATE([Sum of Quantity],PREVIOUSMONTH(dim_Date[DateKey]))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM Delta] =\n",
    "            [Sum of Quantity] - [Sum of Quantity PM]\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity PM %] =\n",
    "            [Sum of Quantity PM Delta] / [Sum of Quantity]\n",
    "        \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            -- GROUP BY --\n",
    "            dim_Date[FirstDateofMonth] ,\n",
    "            --  FILTER  --\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofYear] ) ,\n",
    "             -- MEASURES --\n",
    "            \"Quantity\" \t\t\t\t, [Sum of Quantity],\n",
    "            \"Quantity PM\" \t\t\t, [Sum of Quantity PM],\n",
    "            \"Quantity PM Delta\"\t\t, [Sum of Quantity PM Delta] ,\n",
    "            \"Quantity PM % \" \t\t, [Sum of Quantity PM %]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "trace2 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False,Trace=False)\n",
    "trace2 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False,Trace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9fd2c-1602-4bcf-94d8-e4dbd2984025",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(trace1)\n",
    "display(trace2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3392f85-83ee-420e-b404-509e106150ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 8.2 Running Total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611df2ab-2004-46bc-ba00-83965637d539",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Running Total against **Base** Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf42cc-5368-4f9e-bd52-de134df94d28",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "\t    MEASURE dim_Date[Sum of Quantity YTD] =\n",
    "\t\t    TOTALYTD([Sum of Quantity],dim_Date[DateKey])\n",
    "\t\n",
    "\t    MEASURE fact_myevents_1bln[Sum of Quantity QTD] =\n",
    "\t\t    TOTALQTD([Sum of Quantity],dim_Date[DateKey])\t\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            -- GROUP BY --\n",
    "            dim_Date[FirstDateofMonth] ,\n",
    "            --  FILTER  --\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofYear] ) ,\n",
    "             -- MEASURES --\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"Quantity YTD\" \t, [Sum of Quantity YTD] ,\n",
    "            \"Quantity QTD\" \t, [Sum of Quantity QTD]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace3 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a164621-4808-400d-a2f2-967738b972f2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Running Total against **Partitioned** Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b249175-5118-4828-995f-c6dec0592dda",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str=\"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "\t    MEASURE dim_Date[Sum of Quantity YTD] =\n",
    "\t\t    TOTALYTD([Sum of Quantity],dim_Date[DateKey])\n",
    "\t\n",
    "\t    MEASURE fact_myevents_1bln[Sum of Quantity QTD] =\n",
    "\t\t    TOTALQTD([Sum of Quantity],dim_Date[DateKey])\t\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            -- GROUP BY --\n",
    "            dim_Date[FirstDateofMonth] ,\n",
    "            --  FILTER  --\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofYear] ) ,\n",
    "             -- MEASURES --\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"Quantity YTD\" \t, [Sum of Quantity YTD] ,\n",
    "            \"Quantity QTD\" \t, [Sum of Quantity QTD]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace4 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trace3)\n",
    "display(trace4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33899584-56e9-4cef-a16a-427b57c3d00f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 8.3 RANK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab8de0-f4d8-48bf-811a-82556f8dcd67",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run RANK over **Base** Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db7eec-35e7-46a1-b40b-bb87379a220a",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Sum of Quantity Rank] =\n",
    "            RANKX(ALL(dim_Geography[COUNTRY]) , [Sum of Quantity] )\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"Rank\" \t\t\t, [Sum of Quantity Rank]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace5 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7c9d2-cff0-4332-9b34-d06acfaa3fe5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run RANK over **Partitioned** Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcdf8ea-4991-443f-89f8-44e1f47773c3",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Sum of Quantity Rank] =\n",
    "            RANKX(ALL(dim_Geography[COUNTRY]) , [Sum of Quantity] )\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"Rank\" \t\t\t, [Sum of Quantity Rank]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace6 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efb33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trace5)\n",
    "display(trace6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf39bc48-bca0-4ca3-928e-b69ed899fe81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 8.4 Percent of Parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c749bb-170b-4d04-871f-3bc4861aa2a8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Percent of Parent over **Base** Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcec580-b64d-4858-b8a4-58db8dda5acc",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "\t    MEASURE dim_Date[Percentage of Parent] =\n",
    "\t\t    [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"% of Parent\"\t, [Percentage of Parent]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace7 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a8ead-88b6-423d-9735-f534d8cb2704",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Percent of Parent over **Partitioned** Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecd3ce-5ae1-4046-b75d-677bab283f27",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "\t    MEASURE dim_Date[Percentage of Parent] =\n",
    "\t\t    [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "            \"Quantity\" \t\t, [Sum of Quantity],\n",
    "            \"% of Parent\"\t, [Percentage of Parent]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace8 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51825ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trace7)\n",
    "display(trace8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce2c6a5-871a-4e58-acbf-9639ecc34a7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 8.5 All measures combined in one query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94737720-ebb1-438a-b0c0-90eb96e3ef87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run all measures on **Base** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b39d3-ddfb-4c67-887a-a1b1957367bf",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Percentage of Parent] =\n",
    "            [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity Rank] =\n",
    "            RANKX(ALL(dim_Geography[COUNTRY]) , [Sum of Quantity] )\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity YTD] =\n",
    "            TOTALYTD([Sum of Quantity],dim_Date[DateKey])\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity QTD] =\n",
    "            TOTALQTD([Sum of Quantity],dim_Date[DateKey])\t\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM] =\n",
    "            CALCULATE([Sum of Quantity],PREVIOUSMONTH(dim_Date[DateKey]))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM Delta] =\n",
    "            [Sum of Quantity] - [Sum of Quantity PM]\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity PM %] =\n",
    "            [Sum of Quantity PM Delta] / [Sum of Quantity]\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "            \"Quantity\" \t\t\t\t, [Sum of Quantity],\n",
    "            \"% of Parent\"\t\t\t, [Percentage of Parent],\n",
    "            \"Rank\" \t\t\t\t\t, [Sum of Quantity Rank],\n",
    "            \"Quantity YTD\" \t\t\t, [Sum of Quantity YTD] ,\n",
    "            \"Quantity QTD\" \t\t\t, [Sum of Quantity QTD]\t,\t\n",
    "            \"Quantity PM\" \t\t\t, [Sum of Quantity PM],\n",
    "            \"Quantity PM Delta\"\t\t, [Sum of Quantity PM Delta] ,\n",
    "            \"Quantity PM %\" \t\t, [Sum of Quantity PM %]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace9 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea9574-19e5-4249-aaa3-ca55cb72ecf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run all measures on **Partitioned** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807265ad-5fb0-4fd5-8acc-64c52ab07c3c",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expr:str = \"\"\"\n",
    "\n",
    "    DEFINE\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity] = \n",
    "            SUM(fact_myevents_1bln_partitioned_datekey[Quantity_ThisYear])\n",
    "            \n",
    "        MEASURE dim_Date[Percentage of Parent] =\n",
    "            [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity Rank] =\n",
    "            RANKX(ALL(dim_Geography[COUNTRY]) , [Sum of Quantity] )\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity YTD] =\n",
    "            TOTALYTD([Sum of Quantity],dim_Date[DateKey])\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity QTD] =\n",
    "            TOTALQTD([Sum of Quantity],dim_Date[DateKey])\t\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM] =\n",
    "            CALCULATE([Sum of Quantity],PREVIOUSMONTH(dim_Date[DateKey]))\n",
    "\n",
    "        MEASURE dim_Date[Sum of Quantity PM Delta] =\n",
    "            [Sum of Quantity] - [Sum of Quantity PM]\n",
    "        \n",
    "        MEASURE dim_Date[Sum of Quantity PM %] =\n",
    "            [Sum of Quantity PM Delta] / [Sum of Quantity]\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "            dim_Geography[COUNTRY] ,\n",
    "            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n",
    "            \"Quantity\" \t\t\t\t, [Sum of Quantity],\n",
    "            \"% of Parent\"\t\t\t, [Percentage of Parent],\n",
    "            \"Rank\" \t\t\t\t\t, [Sum of Quantity Rank],\n",
    "            \"Quantity YTD\" \t\t\t, [Sum of Quantity YTD] ,\n",
    "            \"Quantity QTD\" \t\t\t, [Sum of Quantity QTD]\t,\t\n",
    "            \"Quantity PM\" \t\t\t, [Sum of Quantity PM],\n",
    "            \"Quantity PM Delta\"\t\t, [Sum of Quantity PM Delta] ,\n",
    "            \"Quantity PM %\" \t\t, [Sum of Quantity PM %]\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "trace10 = runQueryWithTrace(expr,workspaceName,SemanticModelName,Result=False,DMV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trace9)\n",
    "display(trace10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa6f6e",
   "metadata": {},
   "source": [
    "## 9. Stop the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6270d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc4009",
   "metadata": {},
   "source": [
    "## Lab Summary\n",
    "\n",
    "### ðŸŽ¯ Key Achievements\n",
    "\n",
    "**Column Partitioning Mastery Completed:**\n",
    "- âœ… **Performance Analysis**: Compared partitioned vs. non-partitioned table performance\n",
    "- âœ… **Server Timings**: Captured and analyzed detailed query execution metrics\n",
    "- âœ… **Cache Optimization**: Understood cold vs. warm cache behavior patterns\n",
    "- âœ… **Advanced DAX**: Executed complex analytical queries with performance monitoring\n",
    "\n",
    "### ðŸ“Š Performance Insights Gained\n",
    "\n",
    "**Partitioned Table Benefits:**\n",
    "- Improved query response times for large datasets\n",
    "- Enhanced compression ratios through strategic column organization\n",
    "- Optimized memory utilization during query execution\n",
    "- Reduced I/O operations for filtered queries\n",
    "\n",
    "**Query Pattern Analysis:**\n",
    "- **Period Comparison**: Demonstrated performance gains with date partitioning\n",
    "- **Running Totals**: Showcased optimized sequential data access\n",
    "- **Ranking Operations**: Improved sorting performance with partitioned data\n",
    "- **Percent of Parent**: Enhanced aggregation efficiency\n",
    "\n",
    "### ðŸ”§ Best Practices Learned\n",
    "\n",
    "**Column Partitioning Strategy:**\n",
    "1. **Identify High-Cardinality Columns** for partitioning candidates\n",
    "2. **Analyze Query Patterns** to determine optimal partitioning schemes\n",
    "3. **Monitor Server Timings** to validate performance improvements\n",
    "4. **Test Different Scenarios** (cold vs. warm cache) for comprehensive analysis\n",
    "\n",
    "**Performance Optimization Guidelines:**\n",
    "- Partition on columns frequently used in WHERE clauses\n",
    "- Consider data distribution when choosing partition boundaries\n",
    "- Monitor compression ratios to balance storage vs. performance\n",
    "- Use Vertipaq Analyzer to identify optimization opportunities\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "**Recommended Follow-up:**\n",
    "- **Lab 7**: High cardinality column splitting techniques\n",
    "- **Lab 8**: Advanced One Lake integration with Import mode\n",
    "- **Advanced Topics**: Dynamic partition management and maintenance\n",
    "\n",
    "**Production Implementation:**\n",
    "- Establish partitioning policies based on query patterns\n",
    "- Implement automated performance monitoring\n",
    "- Design partition maintenance schedules\n",
    "- Create performance benchmarking procedures\n",
    "\n",
    "### ðŸ“ˆ Business Impact\n",
    "\n",
    "**Direct Lake Column Partitioning enables:**\n",
    "- **Faster Query Performance**: Reduced response times for analytical workloads\n",
    "- **Improved Scalability**: Better handling of large-scale data volumes\n",
    "- **Cost Optimization**: Efficient resource utilization and reduced compute costs\n",
    "- **Enhanced User Experience**: Responsive dashboards and reports\n",
    "\n",
    "**Performance Metrics:**\n",
    "- Query execution time improvements measured through server timings\n",
    "- Memory efficiency gains through optimized column storage\n",
    "- I/O reduction benefits for filtered and aggregated queries\n",
    "- Scalability improvements for growing data volumes"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {
    "0408074c-1014-4c64-982e-bf06ad39bd42": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "count",
        "binsNumber": 10,
        "categoryFieldKeys": [
         "0"
        ],
        "chartType": "bar",
        "evaluatesOverAllRecords": false,
        "isStacked": false,
        "seriesFieldKeys": [
         "0"
        ],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "python",
      "table": {
       "rows": [
        {
         "0": "QueryBegin",
         "1": "DAXQuery",
         "2": "2025-03-16 22:45:45.600",
         "3": "DEFINE\n\n        MEASURE dim_Date[Sum of Quantity] = \n            SUM(fact_myevents_1bln[Quantity_ThisYear])\n            \n\t    MEASURE dim_Date[Percentage of Parent] =\n\t\t    [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n\n    EVALUATE\n        SUMMARIZECOLUMNS(\n            dim_Geography[COUNTRY] ,\n            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n            \"Quantity\" \t\t, [Sum of Quantity],\n            \"% of Parent\"\t, [Percentage of Parent]\n            )",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:45.586",
         "6": "NaT",
         "7": null,
         "8": null,
         "9": null
        },
        {
         "0": "DirectQueryEnd",
         "1": null,
         "2": "2025-03-16 22:45:47.053",
         "3": "\nSELECT sch.name AS schemaname,\ntbl.name AS tablename,\nc.name AS columnname\nFROM sys.masked_columns AS c\nJOIN sys.tables AS tbl ON c.object_id = tbl.object_id\nJOIN sys.schemas AS sch ON tbl.schema_id = sch.schema_id\nWHERE c.is_masked = 1;\n",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:47.053",
         "6": "2025-03-16 22:45:47.053",
         "7": "0",
         "8": "31",
         "9": "Success"
        },
        {
         "0": "DirectQueryEnd",
         "1": null,
         "2": "2025-03-16 22:45:47.633",
         "3": "\nSELECT 1 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[DateKey]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 2 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[UserID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 3 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[OperatingSystemID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 4 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[OperatingSystemVendorID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 5 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[GeographyID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 6 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[Quantity_ThisYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 7 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[Quantity_LastYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 8 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[HLL_HashBucket]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 9 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln]', 'OBJECT', 'SELECT', '[HLL_FirstZero]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 11 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[DateKey]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 12 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[UserID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 13 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[OperatingSystemID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 14 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[OperatingSystemVendorID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 15 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[GeographyID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 16 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[Quantity_ThisYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 17 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[Quantity_LastYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 18 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[HLL_HashBucket]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 19 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_no_vorder]', 'OBJECT', 'SELECT', '[HLL_FirstZero]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 21 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[DateKey]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 22 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[UserID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 23 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[OperatingSystemID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 24 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[OperatingSystemVendorID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 25 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[GeographyID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 26 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[Quantity_ThisYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 27 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[Quantity_LastYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 28 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[HLL_HashBucket]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 29 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[fact_myevents_1bln_partitioned_datekey]', 'OBJECT', 'SELECT', '[HLL_FirstZero]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 41 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[DateKey]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 42 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Day]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 43 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[DaySuffix]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 44 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Weekday]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 45 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekDayName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 46 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekDayName_Short]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 47 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekDayName_FirstLetter]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 48 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[DOWInMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 49 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[DayOfYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 50 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekOfMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 51 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[WeekOfYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 52 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Month]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 53 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MonthName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 54 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MonthName_Short]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 55 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MonthName_FirstLetter]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 56 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Quarter]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 57 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[QuarterName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 58 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[Year]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 59 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MMYYYY]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 60 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[MonthYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 61 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[IsWeekend]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 62 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[IsHoliday]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 63 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[HolidayName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 64 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[SpecialDays]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 65 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FinancialYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 66 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FinancialQuater]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 67 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FinancialMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 68 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FirstDateofYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 69 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[LastDateofYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 70 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FirstDateofQuater]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 71 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[LastDateofQuater]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 72 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FirstDateofMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 73 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[LastDateofMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 74 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[FirstDateofWeek]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 75 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[LastDateofWeek]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 76 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentYear]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 77 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentQuater]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 78 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentMonth]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 79 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentWeek]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 80 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Date]', 'OBJECT', 'SELECT', '[CurrentDay]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 82 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[GeographyID]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 83 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[GeographyName]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 84 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[COUNTRY]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 85 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[ISO]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 86 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[UN]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 87 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[Num]', 'COLUMN') as PermCheckResult\n\nUNION ALL\n\nSELECT 88 as XLOrdinal, HAS_PERMS_BY_NAME('[dbo].[dim_Geography]', 'OBJECT', 'SELECT', '[DialingCode]', 'COLUMN') as PermCheckResult\n",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:47.616",
         "6": "2025-03-16 22:45:47.633",
         "7": "16",
         "8": "47",
         "9": "Success"
        },
        {
         "0": "DirectQueryEnd",
         "1": null,
         "2": "2025-03-16 22:45:49.133",
         "3": "\nSELECT 0 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[fact_myevents_1bln]')\n\nUNION ALL\n\nSELECT 1 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[fact_myevents_1bln_no_vorder]')\n\nUNION ALL\n\nSELECT 2 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[fact_myevents_1bln_partitioned_datekey]')\n\nUNION ALL\n\nSELECT 4 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[dim_Date]')\n\nUNION ALL\n\nSELECT 5 as XLOrdinal, COUNT(*) as RLSPolicyCnt FROM sys.security_predicates WHERE target_object_id = OBJECT_ID('[dbo].[dim_Geography]')\n",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:49.116",
         "6": "2025-03-16 22:45:49.133",
         "7": "16",
         "8": "63",
         "9": "Success"
        },
        {
         "0": "VertiPaqSEQueryEnd",
         "1": "VertiPaqScan",
         "2": "2025-03-16 22:45:49.396",
         "3": "SET DC_KIND=\"AUTO\";\r\nSELECT\r\n[dim Geography (30)].[COUNTRY (112)] AS [dim Geography (30)$COUNTRY (112)],\r\nSUM([fact myevents 1bln (15)].[Quantity ThisYear (39)]) AS [$Measure0]\r\nFROM [fact myevents 1bln (15)]\r\n\tLEFT OUTER JOIN [dim Date (27)] ON [fact myevents 1bln (15)].[DateKey (34)]=[dim Date (27)].[DateKey (70)]\r\n\tLEFT OUTER JOIN [dim Geography (30)] ON [fact myevents 1bln (15)].[GeographyID (38)]=[dim Geography (30)].[GeographyID (110)]\r\nWHERE\r\n\t[dim Date (27)].[FirstDateofMonth (101)] = 43466.000000;\r\n\r\n\r\n[Estimated size (volume, marshalling bytes): 247, 3952]\r\n",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:49.133",
         "6": "2025-03-16 22:45:49.396",
         "7": "266",
         "8": "3656",
         "9": "Success"
        },
        {
         "0": "QueryEnd",
         "1": "DAXQuery",
         "2": "2025-03-16 22:45:49.413",
         "3": "DEFINE\n\n        MEASURE dim_Date[Sum of Quantity] = \n            SUM(fact_myevents_1bln[Quantity_ThisYear])\n            \n\t    MEASURE dim_Date[Percentage of Parent] =\n\t\t    [Sum of Quantity] / CALCULATE([Sum of Quantity],ALL(dim_Geography))\n\n    EVALUATE\n        SUMMARIZECOLUMNS(\n            dim_Geography[COUNTRY] ,\n            TREATAS({DATE(2019,1,1)} , dim_Date[FirstDateofMonth] ) ,\n            \"Quantity\" \t\t, [Sum of Quantity],\n            \"% of Parent\"\t, [Percentage of Parent]\n            )\r\n\r\n[WaitTime: 0 ms]",
         "4": "C0A23F35-C50B-4751-ABE6-5657D7194D69",
         "5": "2025-03-16 22:45:45.586",
         "6": "2025-03-16 22:45:49.413",
         "7": "3828",
         "8": "3734",
         "9": "Success"
        },
        {
         "0": "ExecutionMetrics",
         "1": null,
         "2": "NaT",
         "3": "{\n\t\"timeStart\": \"2025-03-16T22:45:45.586Z\",\n\t\"timeEnd\": \"2025-03-16T22:45:49.414Z\",\n\n\t\"durationMs\": 3828,\n\t\"datasourceConnectionThrottleTimeMs\": 0,\n\t\"externalQueryExecutionTimeMs\": 21,\n\t\"vertipaqJobCpuTimeMs\": 3656,\n\t\"queryProcessingCpuTimeMs\": 78,\n\t\"totalCpuTimeMs\": 3734,\n\t\"executionDelayMs\": 0,\n\n\t\"approximatePeakMemConsumptionKB\": 5244,\n\n\t\"directQueryTimeoutMs\": 3597000,\n\t\"tabularConnectionTimeoutMs\": 3600000,\n\n\t\"commandType\": \"Statement\",\n\t\"queryDialect\": 3,\n\t\"queryResultRows\": 239\n}",
         "4": null,
         "5": "NaT",
         "6": "NaT",
         "7": null,
         "8": null,
         "9": null
        }
       ],
       "schema": [
        {
         "key": "0",
         "name": "Event Class",
         "type": "string"
        },
        {
         "key": "1",
         "name": "Event Subclass",
         "type": "string"
        },
        {
         "key": "2",
         "name": "Current Time",
         "type": "timestamp"
        },
        {
         "key": "3",
         "name": "Text Data",
         "type": "string"
        },
        {
         "key": "4",
         "name": "Session ID",
         "type": "string"
        },
        {
         "key": "5",
         "name": "Start Time",
         "type": "timestamp"
        },
        {
         "key": "6",
         "name": "End Time",
         "type": "timestamp"
        },
        {
         "key": "7",
         "name": "Duration",
         "type": "bigint"
        },
        {
         "key": "8",
         "name": "Cpu Time",
         "type": "bigint"
        },
        {
         "key": "9",
         "name": "Success",
         "type": "string"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "dataframeType": "pandas"
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
