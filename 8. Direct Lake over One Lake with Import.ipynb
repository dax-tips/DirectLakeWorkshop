{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d63d4a-a0ee-4357-a020-7103ba035f01",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 8: Direct Lake over One Lake with Import Mode Integration\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This advanced lab demonstrates **composite storage mode optimization** by combining Direct Lake and Import mode tables within a single Microsoft Fabric semantic model. This powerful technique allows you to leverage the best of both storage modes: Direct Lake for large-scale data access and Import mode for specific optimizations and performance tuning of critical tables.\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand Direct Lake over One Lake vs. Direct Lake over SQL differences\n",
    "- Learn to convert Direct Lake tables to Import mode within the same model\n",
    "- Master hybrid storage mode configuration for optimal performance\n",
    "- Analyze performance characteristics of mixed storage mode scenarios\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Direct Lake over One Lake**: Advanced DirectLake implementation with OneLake integration\n",
    "- **Composite Storage Modes**: Strategic combination of Direct Lake and Import modes\n",
    "- **Model Cloning**: Creating optimized model variants for testing\n",
    "- **Storage Mode Conversion**: Dynamic switching between Direct Lake and Import modes\n",
    "\n",
    "**Prerequisites:** Lab 7 completion (high cardinality column optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af395dd-b0d0-46a3-995f-d859eed2f903",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Install Semantic Link Labs Python Library\n",
    "Install the Semantic Link Labs library for advanced Direct Lake and hybrid storage mode operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3356d3-7f40-4071-9a25-c42dfbdc27e1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Install semantic-link-labs library for hybrid storage mode operations\n",
    "%pip install -q --upgrade pip\n",
    "%pip install -q semantic-link-labs azure-core==1.31.0 PyJWT==2.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07e358-e8a6-4e44-8e9c-baf45bed2d18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2. Install Python Libraries and Setup Parameters\n",
    "Import required libraries and configure parameters for hybrid Direct Lake and Import mode operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfd109-9068-486b-a42d-101366ce7554",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries for hybrid storage mode operations and model management\n",
    "import sempy_labs as labs\n",
    "from sempy import fabric\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from sempy_labs.tom._model import TOMWrapper, connect_semantic_model\n",
    "\n",
    "# Import specialized helper functions for advanced operations\n",
    "from sempy_labs._helper_functions import (\n",
    "    format_dax_object_name,\n",
    "    generate_guid,\n",
    "    _make_list_unique,\n",
    "    resolve_dataset_name_and_id,\n",
    "    resolve_workspace_name_and_id,\n",
    "    _base_api,\n",
    "    resolve_workspace_id,\n",
    "    resolve_item_id,\n",
    "    resolve_lakehouse_id,\n",
    "    resolve_lakehouse_name_and_id\n",
    ")\n",
    "\n",
    "# Initialize Analysis Services for advanced model operations\n",
    "fabric._client._utils._init_analysis_services()\n",
    "import Microsoft.AnalysisServices.Tabular as TOM\n",
    "import Microsoft.AnalysisServices\n",
    "\n",
    "# Configure model names for hybrid storage mode testing\n",
    "LakehouseName = \"BigData\"\n",
    "SemanticModelName = f\"{LakehouseName}_model\"\n",
    "ClonedModelName = SemanticModelName + \"_clone\"\n",
    "workspace = None\n",
    "\n",
    "(workspace_name, workspace_id) = resolve_workspace_name_and_id(workspace)\n",
    "(lakehouse_name, lakehouse_id) = resolve_lakehouse_name_and_id(lakehouse=LakehouseName, workspace=workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9aadf-8559-4d03-b64b-00f0fb6f02e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3. Clone BigData Semantic Model\n",
    "Create a copy of the existing BigData semantic model for hybrid storage mode experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a99238-7a97-4adc-8e86-d83f6b03569d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "#Clear any existing cloned model if re-running\n",
    "df = fabric.list_items()\n",
    "if ClonedModelName in df.values:\n",
    "    model_id = df.at[df[df['Display Name'] == ClonedModelName].index[0], 'Id']\n",
    "    fabric.delete_item(model_id)\n",
    "    print(\"Cloned model deleted\")\n",
    "\n",
    "with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "    newDB = tom._tom_server.Databases.GetByName(SemanticModelName).Clone()\n",
    "    newModel = tom._tom_server.Databases.GetByName(SemanticModelName).Model.Clone()\n",
    "    newDB.Name = ClonedModelName\n",
    "    newDB.ID = str(uuid.uuid4())\n",
    "    #newDB.Model = newModel\n",
    "    newModel.CopyTo(newDB.Model)\n",
    "    tom._tom_server.Databases.Add(newDB)\n",
    "\n",
    "    newDB.Update(Microsoft.AnalysisServices.UpdateOptions.ExpandFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc048908-0e39-4d19-8ab0-76e7b7119a23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Frame the Cloned Model\n",
    "Refresh the cloned semantic model to ensure all data connections and relationships are properly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385bc2ca-99d8-4ab7-8905-690fd9d82f83",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Refresh the cloned model to initialize data connections\n",
    "labs.refresh_semantic_model(dataset=ClonedModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748b06b-ddab-49ab-adbd-b11d5487881d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 5. Check Direct Lake Version\n",
    "Identify whether the model uses Direct Lake over SQL (Sql.Database) or Direct Lake over One Lake (Azure.Lakehouse).\n",
    "\n",
    "eg:  \n",
    "let database = SqlDatabase(....)     = **DL/SQL**  \n",
    "let database = Azure.Lakehouse(....) = **DL/OL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633e142-19e3-4595-b4c2-45e15d03f82a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for e in tom.model.Expressions:\n",
    "        print(e.Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4a9ee-1d8b-4f08-beac-68f66565b5d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 6. Show Storage Mode for Each Table in Cloned Model\n",
    "Display the current storage mode configuration for all tables in the cloned semantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea073c-45c6-4f3d-8a1f-059e6dd12501",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "objects = {}\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for t in tom.model.Tables:\n",
    "        #print(t.Name)\n",
    "        for p in t.Partitions:\n",
    "            #print(p.Mode)\n",
    "            objects[t.Name] = str(p.Mode)\n",
    " \n",
    "df=pd.DataFrame([objects])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14e8e7-7a27-44b6-9bdb-7f6c5c7df19d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 7. Try to Convert Direct Lake Table to Import (First Attempt)\n",
    "Attempt to convert a Direct Lake table to Import mode - this will fail if using Direct Lake over SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7e6c2-6e56-40d9-b570-90976bc5eb60",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Attempt to convert Direct Lake table to Import mode (will fail if Direct Lake over SQL)\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    tom.convert_direct_lake_to_import(\n",
    "        table_name=\"dim_Date\" ,\n",
    "        entity_name=\"dim_Date\" ,\n",
    "        source=\"BigData\",\n",
    "        source_type = \"Lakehouse\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021862b-6754-4ff9-827b-047a0dec4674",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 8. Convert Cloned Model to Direct Lake over One Lake\n",
    "Modify the cloned model to use Direct Lake over One Lake instead of Direct Lake over SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ac75c-ae9c-4c06-8017-8341c71d50ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "\n",
    "    #Convert import tables to Direct Lake\n",
    "    for t in tom.model.Tables:\n",
    "        for p in t.Partitions:\n",
    "            if(p.Mode==TOM.ModeType.Import):\n",
    "                t.Partitions.Remove(p)\n",
    "                tom.add_entity_partition(table_name=t.Name,entity_name=t.Name)\n",
    "                print(f\"Table {t.Name} converted\")\n",
    "            p.Source.SchemaName=None\n",
    "\n",
    "    for e in tom.model.Expressions:\n",
    "        e.Expression = f\"\"\"\n",
    "        let\n",
    "            Source = AzureStorage.DataLake(\"https://onelake.dfs.fabric.microsoft.com/{workspace_id}/{lakehouse_id}\", [HierarchicalNavigation=true])\n",
    "        in\n",
    "            Source\"\"\"\n",
    "        \n",
    "print(\"Converted semantic model to use DirectLake over One Lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d226b6-0dfc-422d-a656-bc35a2ed12c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 9. Convert Direct Lake Table to Import (Second Attempt)\n",
    "Successfully convert a Direct Lake table to Import mode now that the model uses Direct Lake over One Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396054e2-5eb2-4a0e-bdea-54b9ad278c1e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert Direct Lake table to Import mode (should work with Direct Lake over One Lake)\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    tom.convert_direct_lake_to_import(\n",
    "        table_name=\"dim_Date\" ,\n",
    "        entity_name=\"dim_Date\" ,\n",
    "        source=\"BigData\",\n",
    "        source_type = \"Lakehouse\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9c1ba-ec93-48c4-a7c8-721ac6b98500",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 10. Show Storage Mode for Each Table After Conversion\n",
    "Display the updated storage mode configuration showing the hybrid model with both Direct Lake and Import tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a8f7e-c0b2-4b01-ab63-67ab0cb8cc3a",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "objects = {}\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for t in tom.model.Tables:\n",
    "        #print(t.Name)\n",
    "        for p in t.Partitions:\n",
    "            #print(p.Mode)\n",
    "            objects[t.Name] = str(p.Mode)\n",
    " \n",
    "df=pd.DataFrame([objects])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b5870",
   "metadata": {},
   "source": [
    "## Clear Schema Name\n",
    "This is a temporary step to resolve issue.\n",
    "\n",
    "Use Tabular Editor to remove/save/replace/save tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33badbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for t in tom.model.Tables:\n",
    "        for p in t.Partitions:\n",
    "            if isinstance(p.Source,Microsoft.AnalysisServices.Tabular.EntityPartitionSource):\n",
    "                p.Source.SchemaName=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868820b-7b38-4fb9-b438-a3aac82b24ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## <mark>SET CREDENTIALS AND LARGE MODEL IN SERVICE</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3da9d-c267-4dcf-8fbd-36f80c26505a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Refresh import table model so import table gets hydrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447cca4-0e31-4492-af98-0ae0d96ace79",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "labs.refresh_semantic_model(dataset=ClonedModelName,tables=[\"dim_Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd5186",
   "metadata": {},
   "source": [
    "## Put Relationships back after removing/adding Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing relationships\n",
    "            for r in tom.model.Relationships:\n",
    "                tom.model.Relationships.Remove(r)\n",
    "\n",
    "            #2. Creates correct relationships\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln\"                    , from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln\"                    , from_column=\"GeographyID\" , to_table=\"dim_Geography\"  , to_column=\"GeographyID\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "\n",
    "            tom.add_relationship(from_table=\"fact_myevents_2bln\"                    , from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"fact_myevents_2bln\"                    , from_column=\"GeographyID\" , to_table=\"dim_Geography\"  , to_column=\"GeographyID\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln_partitioned_datekey\", from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln_partitioned_datekey\", from_column=\"GeographyID\" , to_table=\"dim_Geography\"  , to_column=\"GeographyID\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding relationships... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd6b65d-4119-457a-b1d4-98a1d935002b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Recalculate relationship indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649a743-9043-4a31-ba52-7ab1eeb9b4e7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "labs.refresh_semantic_model(dataset=ClonedModelName,refresh_type=\"calculate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcef16-502c-4517-873b-90bafe383dbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Show what version of Direct Lake is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a5e7c6-6b05-4a7f-87a2-e05ed34df9ad",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for e in tom.model.Expressions:\n",
    "        print(e.Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3b889",
   "metadata": {},
   "source": [
    "## Update relationship to Many to Many (Temporarily needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2314c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    #1. Remove any existing relationships\n",
    "    for r in tom.model.Relationships:\n",
    "        if r.FromTable.Name == \"fact_myevents_1bln\" and r.ToTable.Name == \"dim_Date\":\n",
    "            tom.model.Relationships.Remove(r)\n",
    "\n",
    "    #2. Creates correct relationships\n",
    "    tom.add_relationship(from_table=\"fact_myevents_1bln\"                    , from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"Many\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504bf7f1-ef01-40e9-b67f-0bc1efb82f07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run query on 1Bln Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b0ade2-ebf5-41d0-9c02-1016a8b1440a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "df=fabric.evaluate_dax(\n",
    "    dataset=ClonedModelName,\n",
    "    dax_string=\"\"\"\n",
    "        EVALUATE\n",
    "\t        SUMMARIZECOLUMNS(\n",
    "\t\t        dim_Date[DateKey],\n",
    "\t\t        \"Quantity\" , [Sum of Sales (1bln)]\n",
    "\t\t        )\n",
    "        \"\"\"\n",
    "    )\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18476ac8-6970-4f8b-b41e-299c6e2752dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run query on 2Bln Row\n",
    "#### This will fail due to guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129efc8-5e63-4684-8b40-563ea586782e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "df=fabric.evaluate_dax(\n",
    "    dataset=ClonedModelName,\n",
    "    dax_string=\"\"\"\n",
    "        EVALUATE\n",
    "\t        SUMMARIZECOLUMNS(\n",
    "\t\t        dim_Date[DateKey],\n",
    "\t\t        \"Quantity\" , [Sum of Sales (2bln)]\n",
    "\t\t        )\n",
    "        \"\"\"\n",
    "\n",
    "    )\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e779019-d17c-43e3-b946-40b6f015bbcf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Convert cloned model to Direct Lake over SQL Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d9159-f096-426c-ae42-9828e778887f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(labs.list_lakehouses())\n",
    "endpointid = df[df['Lakehouse Name']==LakehouseName]['SQL Endpoint ID'].iloc[0]\n",
    "server = df[df['Lakehouse Name']==LakehouseName]['SQL Endpoint Connection String'].iloc[0]\n",
    "\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "\n",
    "    #Convert import tables to Direct Lake\n",
    "    for t in tom.model.Tables:\n",
    "        for p in t.Partitions:\n",
    "            if(p.Mode==TOM.ModeType.Import):\n",
    "                t.Partitions.Remove(p)\n",
    "                tom.add_entity_partition(table_name=t.Name,entity_name=t.Name)\n",
    "                print(f\"Table {t.Name} converted\")\n",
    "            p.Source.SchemaName=None\n",
    "\n",
    "    #Switch Model to Direct Lake over SQL\n",
    "    for e in tom.model.Expressions:\n",
    "        e.Expression = f\"\"\"\n",
    "        let\n",
    "            Source = Sql.Database(\"{server}\", \"{endpointid}\")\n",
    "        in\n",
    "            Source\"\"\"\n",
    "\n",
    "print(\"Converted to Direct Lake over SQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1277a1",
   "metadata": {},
   "source": [
    "## Check what version of Direct Lake is being used\n",
    "\n",
    "##### Sql.Database    = DirectLake over SQL\n",
    "\n",
    "##### Azure.Lakehouse = DirectLake over One Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f1201-49d3-4e8a-af4d-b25460c836be",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for e in tom.model.Expressions:\n",
    "        print(e.Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662bccf-96ff-4237-a167-739a9188ede7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Show storage mode for each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884eb022-3cfd-4faa-92e3-79f32525950d",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "objects = {}\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for t in tom.model.Tables:\n",
    "        #print(t.Name)\n",
    "        for p in t.Partitions:\n",
    "            #print(p.Mode)\n",
    "            objects[t.Name] = str(p.Mode)\n",
    " \n",
    "df=pd.DataFrame([objects])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42ead8-fb11-4abc-be7b-bf99d2e170dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run query on 2bln row table\n",
    "This should work, but fall back to SQL Endpoint\n",
    "(Run twice if you get error first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6546b0c5-fd4c-4ab8-8cf4-7a0f9064250d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "df=fabric.evaluate_dax(\n",
    "    dataset=ClonedModelName,\n",
    "    dax_string=\"\"\"\n",
    "        EVALUATE\n",
    "\t        SUMMARIZECOLUMNS(\n",
    "\t\t        dim_Date[DateKey],\n",
    "\t\t        \"Quantity\" , [Sum of Sales (2bln)]\n",
    "\t\t        )\n",
    "        \"\"\"\n",
    "    )\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f1c63-8279-482b-b1d4-11bca298b658",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Show TMSL code for cloned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7266c6-802c-4819-9a84-42cb342d3df6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    x= tom.get_bim()\n",
    "\n",
    "    formatted_json = json.dumps(x, indent=4)\n",
    "    print(formatted_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805cf68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab 8 Summary: Hybrid Storage Mode Mastery\n",
    "\n",
    "**🎯 What You Accomplished:**\n",
    "This lab demonstrated the most advanced Direct Lake techniques, showcasing hybrid storage modes that combine Direct Lake over One Lake with Import mode capabilities for maximum flexibility and performance.\n",
    "\n",
    "**🔧 Key Technical Achievements:**\n",
    "- **Hybrid Architecture Implementation**: Successfully configured Direct Lake over One Lake with Import mode fallback capabilities\n",
    "- **Storage Mode Optimization**: Demonstrated conversion between Direct Lake over One Lake and Direct Lake over SQL Endpoint\n",
    "- **Large-Scale Performance Testing**: Validated query performance on datasets ranging from 1 billion to 2 billion rows\n",
    "- **Guardrail Management**: Explored Direct Lake guardrails and automatic fallback mechanisms to SQL Endpoint\n",
    "- **Advanced Relationship Configuration**: Implemented many-to-many relationships for optimal large table performance\n",
    "\n",
    "**💡 Business Impact:**\n",
    "- **Scalable Architecture**: Hybrid storage modes provide the best of both worlds - Direct Lake performance with Import mode reliability\n",
    "- **Cost Optimization**: Intelligent fallback mechanisms ensure queries succeed while maintaining optimal performance characteristics\n",
    "- **Enterprise Readiness**: Advanced configurations support the largest enterprise datasets with guaranteed query execution\n",
    "- **Flexibility**: Multiple storage mode options allow for precise performance tuning based on specific business requirements\n",
    "\n",
    "**🚀 Advanced Concepts Mastered:**\n",
    "- Direct Lake over One Lake vs. SQL Endpoint implementation differences\n",
    "- Hybrid storage mode architecture patterns\n",
    "- Large-scale data performance optimization strategies\n",
    "- Intelligent query fallback mechanisms\n",
    "\n",
    "**Next Steps**: You now possess comprehensive knowledge of all Direct Lake storage modes and hybrid architectures, enabling you to design and implement enterprise-scale semantic models that deliver optimal performance across any dataset size while maintaining reliability and cost effectiveness."
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1800000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
