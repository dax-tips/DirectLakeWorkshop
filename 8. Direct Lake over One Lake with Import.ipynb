{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d63d4a-a0ee-4357-a020-7103ba035f01",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 8: Direct Lake over One Lake with Import Mode Integration\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This advanced lab demonstrates **composite storage mode optimization** by combining Direct Lake and Import mode tables within a single Microsoft Fabric semantic model. This powerful technique allows you to leverage the best of both storage modes: Direct Lake for large-scale data access and Import mode for specific optimizations and performance tuning of critical tables.\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand Direct Lake over One Lake vs. Direct Lake over SQL differences\n",
    "- Learn to convert Direct Lake tables to Import mode within the same model\n",
    "- Master hybrid storage mode configuration for optimal performance\n",
    "- Analyze performance characteristics of mixed storage mode scenarios\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Direct Lake over One Lake**: Advanced DirectLake implementation with OneLake integration\n",
    "- **Composite Storage Modes**: Strategic combination of Direct Lake and Import modes\n",
    "- **Model Cloning**: Creating optimized model variants for testing\n",
    "- **Storage Mode Conversion**: Dynamic switching between Direct Lake and Import modes\n",
    "\n",
    "**Prerequisites:** Lab 7 completion (high cardinality column optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af395dd-b0d0-46a3-995f-d859eed2f903",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Install Semantic Link Labs Python Library\n",
    "Install the Semantic Link Labs library for advanced Direct Lake and hybrid storage mode operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3356d3-7f40-4071-9a25-c42dfbdc27e1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07e358-e8a6-4e44-8e9c-baf45bed2d18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2. Install Python Libraries and Setup Parameters\n",
    "Import required libraries and configure parameters for hybrid Direct Lake and Import mode operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfd109-9068-486b-a42d-101366ce7554",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries for hybrid storage mode operations and model management\n",
    "import sempy_labs as labs\n",
    "import sempy\n",
    "from sempy import fabric\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from sempy_labs.tom._model import TOMWrapper, connect_semantic_model\n",
    "\n",
    "# Import specialized helper functions for advanced operations\n",
    "from sempy_labs._helper_functions import (\n",
    "    format_dax_object_name,\n",
    "    generate_guid,\n",
    "    _make_list_unique,\n",
    "    resolve_dataset_name_and_id,\n",
    "    resolve_workspace_name_and_id,\n",
    "    _base_api,\n",
    "    resolve_workspace_id,\n",
    "    resolve_item_id,\n",
    "    resolve_lakehouse_id,\n",
    "    resolve_lakehouse_name_and_id\n",
    ")\n",
    "\n",
    "# Initialize Analysis Services for advanced model operations\n",
    "fabric._client._utils._init_analysis_services()\n",
    "import Microsoft.AnalysisServices.Tabular as TOM\n",
    "import Microsoft.AnalysisServices\n",
    "import warnings\n",
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "\n",
    "# Configure model names for hybrid storage mode testing\n",
    "LakehouseName = \"BigData\"\n",
    "SemanticModelName = f\"{LakehouseName}_model\"\n",
    "ClonedModelName = SemanticModelName + \"_clone\"\n",
    "workspace = None\n",
    "\n",
    "(workspace_name, workspace_id) = resolve_workspace_name_and_id(workspace)\n",
    "(lakehouse_name, lakehouse_id) = resolve_lakehouse_name_and_id(lakehouse=LakehouseName, workspace=workspace)\n",
    "\n",
    "#### Generate Unique Trace Name - Start ####\n",
    "import json, base64\n",
    "token = notebookutils.credentials.getToken(\"pbi\")\n",
    "payload = token.split(\".\")[1]\n",
    "payload += \"=\" * (4 - len(payload) % 4)\n",
    "upn = json.loads(base64.b64decode(payload)).get(\"upn\")\n",
    "\n",
    "# Extract just the user part (e.g. \"SQLKDL.user39\")\n",
    "user_id = upn.split(\"@\")[0]\n",
    "lab_number = 8  # set per lab\n",
    "\n",
    "trace_name = f\"Lab{lab_number}_{user_id}\"\n",
    "#### Generate Unique Trace Name - End ####\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)\n",
    "\n",
    "def filter_func(e):\n",
    "    retVal:bool=True\n",
    "    if e.EventSubclass.ToString() == \"VertiPaqScanInternal\":\n",
    "        retVal=False      \n",
    "    #     #if e.EventSubClass.ToString() == \"VertiPaqScanInternal\":\n",
    "    #     retVal=False\n",
    "    return retVal\n",
    "\n",
    "# define events to trace and their corresponding columns\n",
    "def runQueryWithTrace (expr:str,workspaceName:str,SemanticModelName:str,Result:Optional[bool]=True,Trace:Optional[bool]=True,DMV:Optional[bool]=True,ClearCache:Optional[bool]=True) -> pd.DataFrame :\n",
    "    event_schema = fabric.Trace.get_default_query_trace_schema()\n",
    "    event_schema.update({\"ExecutionMetrics\":[\"EventClass\",\"TextData\"]})\n",
    "    del event_schema['VertiPaqSEQueryBegin']\n",
    "    del event_schema['VertiPaqSEQueryCacheMatch']\n",
    "    del event_schema['DirectQueryBegin']\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    WorkspaceName = workspaceName\n",
    "    SemanticModelName = SemanticModelName\n",
    "\n",
    "    if ClearCache:\n",
    "        labs.clear_cache(SemanticModelName)\n",
    "\n",
    "    with fabric.create_trace_connection(SemanticModelName,WorkspaceName) as trace_connection:\n",
    "        # create trace on server with specified events\n",
    "        with trace_connection.create_trace(\n",
    "            event_schema=event_schema, \n",
    "            name=trace_name,\n",
    "            filter_predicate=filter_func,\n",
    "            stop_event=\"QueryEnd\"\n",
    "            ) as trace:\n",
    "\n",
    "            trace.start()\n",
    "\n",
    "            df=sempy.fabric.evaluate_dax(\n",
    "                dataset=SemanticModelName, \n",
    "                dax_string=expr)\n",
    "\n",
    "            if Result:\n",
    "                displayHTML(f\"<H2>####### DAX QUERY RESULT #######</H2>\")\n",
    "                display(df)\n",
    "\n",
    "            # Wait 5 seconds for trace data to arrive\n",
    "            time.sleep(5)\n",
    "\n",
    "            # stop Trace and collect logs\n",
    "            final_trace_logs = trace.stop()\n",
    "\n",
    "    if Trace:\n",
    "        displayHTML(f\"<H2>####### SERVER TIMINGS #######</H2>\")\n",
    "        display(final_trace_logs)\n",
    "    \n",
    "    if DMV:\n",
    "        displayHTML(f\"<H2>####### SHOW DMV RESULTS #######</H2>\")\n",
    "        runDMV()\n",
    "    \n",
    "    return final_trace_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9aadf-8559-4d03-b64b-00f0fb6f02e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3. Clone BigData Semantic Model\n",
    "Create a copy of the existing BigData semantic model for hybrid storage mode experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a99238-7a97-4adc-8e86-d83f6b03569d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "#Clear any existing cloned model if re-running\n",
    "df = fabric.list_items()\n",
    "if ClonedModelName in df.values:\n",
    "    model_id = df.at[df[df['Display Name'] == ClonedModelName].index[0], 'Id']\n",
    "    fabric.delete_item(model_id)\n",
    "    print(\"Cloned model deleted\")\n",
    "\n",
    "with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "    newDB = tom._tom_server.Databases.GetByName(SemanticModelName).Clone()\n",
    "    newModel = tom._tom_server.Databases.GetByName(SemanticModelName).Model.Clone()\n",
    "    newDB.Name = ClonedModelName\n",
    "    newDB.ID = str(uuid.uuid4())\n",
    "    #newDB.Model = newModel\n",
    "    newModel.CopyTo(newDB.Model)\n",
    "    tom._tom_server.Databases.Add(newDB)\n",
    "\n",
    "    newDB.Update(Microsoft.AnalysisServices.UpdateOptions.ExpandFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc048908-0e39-4d19-8ab0-76e7b7119a23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Frame the Cloned Model\n",
    "Refresh the cloned semantic model to ensure all data connections and relationships are properly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385bc2ca-99d8-4ab7-8905-690fd9d82f83",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Refresh the cloned model to initialize data connections\n",
    "labs.refresh_semantic_model(dataset=ClonedModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67971155",
   "metadata": {},
   "source": [
    "## 5. Clear Schema Name\n",
    "This is a temporary step to resolve issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for t in tom.model.Tables:\n",
    "        for p in t.Partitions:\n",
    "            if isinstance(p.Source,Microsoft.AnalysisServices.Tabular.EntityPartitionSource):\n",
    "                p.Source.SchemaName=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a76b88",
   "metadata": {},
   "source": [
    "## 6. Frame the Cloned Model (after schema change)\n",
    "Refresh the cloned semantic model to ensure all data connections and relationships are properly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the cloned model to initialize data connections\n",
    "labs.refresh_semantic_model(dataset=ClonedModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748b06b-ddab-49ab-adbd-b11d5487881d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 7. Check Direct Lake Version\n",
    "Identify whether the model uses Direct Lake over SQL (Sql.Database) or Direct Lake over One Lake (Azure.Lakehouse).\n",
    "\n",
    "eg:  \n",
    "let database = SqlDatabase(....)     = **DL/SQL**  \n",
    "let database = Azure.Lakehouse(....) = **DL/OL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633e142-19e3-4595-b4c2-45e15d03f82a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for e in tom.model.Expressions:\n",
    "        print(e.Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4a9ee-1d8b-4f08-beac-68f66565b5d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 8. Show Storage Mode for Each Table in Cloned Model\n",
    "Display the current storage mode configuration for all tables in the cloned semantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea073c-45c6-4f3d-8a1f-059e6dd12501",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "objects = {}\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for t in tom.model.Tables:\n",
    "        #print(t.Name)\n",
    "        for p in t.Partitions:\n",
    "            #print(p.Mode)\n",
    "            objects[t.Name] = str(p.Mode)\n",
    " \n",
    "df=pd.DataFrame([objects])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14e8e7-7a27-44b6-9bdb-7f6c5c7df19d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 9. Try to Convert Direct Lake Table to Import (First Attempt)\n",
    "Attempt to convert a Direct Lake table to Import mode - **this will fail** if using Direct Lake over SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7e6c2-6e56-40d9-b570-90976bc5eb60",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Attempt to convert Direct Lake table to Import mode (will fail if Direct Lake over SQL)\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    tom.convert_direct_lake_to_import(\n",
    "        table_name=\"dim_Date\" ,\n",
    "        entity_name=\"dim_Date\" ,\n",
    "        source=\"BigData\",\n",
    "        source_type = \"Lakehouse\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021862b-6754-4ff9-827b-047a0dec4674",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 10. Convert Cloned Model to Direct Lake over One Lake\n",
    "Modify the cloned model to use Direct Lake over One Lake instead of Direct Lake over SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ac75c-ae9c-4c06-8017-8341c71d50ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "\n",
    "    for e in tom.model.Expressions:\n",
    "        e.Expression = f\"\"\"\n",
    "        let\n",
    "            Source = AzureStorage.DataLake(\"https://onelake.dfs.fabric.microsoft.com/{workspace_id}/{lakehouse_id}\", [HierarchicalNavigation=true])\n",
    "        in\n",
    "            Source\"\"\"\n",
    "        \n",
    "print(\"Converted semantic model to use DirectLake over One Lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d226b6-0dfc-422d-a656-bc35a2ed12c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 11. Convert Direct Lake Table to Import (Second Attempt)\n",
    "Successfully convert a Direct Lake table to Import mode now that the model uses Direct Lake over One Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396054e2-5eb2-4a0e-bdea-54b9ad278c1e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert Direct Lake table to Import mode (should work with Direct Lake over One Lake)\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    tom.convert_direct_lake_to_import(\n",
    "        table_name=\"dim_Date\" ,\n",
    "        entity_name=\"dim_Date\" ,\n",
    "        source=\"BigData\",\n",
    "        source_type = \"Lakehouse\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9c1ba-ec93-48c4-a7c8-721ac6b98500",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 12. Show Storage Mode for Each Table After Conversion\n",
    "Display the updated storage mode configuration showing the hybrid model with both Direct Lake and Import tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a8f7e-c0b2-4b01-ab63-67ab0cb8cc3a",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "objects = {}\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for t in tom.model.Tables:\n",
    "        #print(t.Name)\n",
    "        for p in t.Partitions:\n",
    "            #print(p.Mode)\n",
    "            objects[t.Name] = str(p.Mode)\n",
    " \n",
    "df=pd.DataFrame([objects])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868820b-7b38-4fb9-b438-a3aac82b24ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## <mark>13. SET CREDENTIALS AND LARGE MODEL IN SERVICE</mark>\n",
    "This must be done on the Semantic Model and not via a Notebook script.  \n",
    "Create a new Shared Cloud Connection to be used for refreshing the import table.  \n",
    "For the purpose of this lab, use OAuth2 for the Authentication method.  \n",
    "Service Principal is the recommended option here for production scenarios.  \n",
    "Make sure the Privacy Level is Organizational (default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3da9d-c267-4dcf-8fbd-36f80c26505a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 14. Refresh import table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447cca4-0e31-4492-af98-0ae0d96ace79",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "labs.refresh_semantic_model(dataset=ClonedModelName,tables=[\"dim_Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd6b65d-4119-457a-b1d4-98a1d935002b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 15. Recalculate relationship indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649a743-9043-4a31-ba52-7ab1eeb9b4e7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "labs.refresh_semantic_model(dataset=ClonedModelName,refresh_type=\"calculate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcef16-502c-4517-873b-90bafe383dbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 16. Show what version of Direct Lake is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a5e7c6-6b05-4a7f-87a2-e05ed34df9ad",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for e in tom.model.Expressions:\n",
    "        print(e.Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3b889",
   "metadata": {},
   "source": [
    "## 17. Update relationship to Many to Many \n",
    "(Not needed any more, keeping step just in case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2314c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "#     #1. Remove any existing relationships\n",
    "#     for r in tom.model.Relationships:\n",
    "#         if r.FromTable.Name == \"fact_myevents_1bln\" and r.ToTable.Name == \"dim_Date\":\n",
    "#             tom.model.Relationships.Remove(r)\n",
    "\n",
    "#     #2. Creates correct relationships\n",
    "#     tom.add_relationship(from_table=\"fact_myevents_1bln\"                    , from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"Many\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504bf7f1-ef01-40e9-b67f-0bc1efb82f07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 18. Run query on 1Bln Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b0ade2-ebf5-41d0-9c02-1016a8b1440a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "        EVALUATE\n",
    "\t        SUMMARIZECOLUMNS(\n",
    "\t\t        dim_Date[DateKey],\n",
    "\t\t        \"Quantity\" , [Sum of Sales (1bln)]\n",
    "\t\t        )\n",
    "\"\"\",workspace_name,ClonedModelName,DMV=False)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18476ac8-6970-4f8b-b41e-299c6e2752dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 19. Run query on 2Bln Row\n",
    "**This will fail due to guardrail**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129efc8-5e63-4684-8b40-563ea586782e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "        EVALUATE\n",
    "\t        SUMMARIZECOLUMNS(\n",
    "\t\t        dim_Date[DateKey],\n",
    "\t\t        \"Quantity\" , [Sum of Sales (2bln)]\n",
    "\t\t        )\n",
    "\"\"\",workspace_name,ClonedModelName,DMV=False)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e779019-d17c-43e3-b946-40b6f015bbcf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 20. Convert cloned model back to DL/SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d9159-f096-426c-ae42-9828e778887f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(labs.list_lakehouses())\n",
    "endpointid = df[df['Lakehouse Name']==LakehouseName]['SQL Endpoint ID'].iloc[0]\n",
    "server = df[df['Lakehouse Name']==LakehouseName]['SQL Endpoint Connection String'].iloc[0]\n",
    "\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "\n",
    "    #Convert import tables to Direct Lake\n",
    "    for t in tom.model.Tables:\n",
    "        for p in t.Partitions:\n",
    "            if(p.Mode==TOM.ModeType.Import):\n",
    "                t.Partitions.Remove(p)\n",
    "                tom.add_entity_partition(table_name=t.Name,entity_name=t.Name)\n",
    "                print(f\"Table {t.Name} converted\")\n",
    "            p.Source.SchemaName=None\n",
    "\n",
    "    #Switch Model to Direct Lake over SQL\n",
    "    for e in tom.model.Expressions:\n",
    "        e.Expression = f\"\"\"\n",
    "        let\n",
    "            Source = Sql.Database(\"{server}\", \"{endpointid}\")\n",
    "        in\n",
    "            Source\"\"\"\n",
    "\n",
    "print(\"Converted to Direct Lake over SQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1277a1",
   "metadata": {},
   "source": [
    "## 21. Check what version of Direct Lake is being used\n",
    "\n",
    "Sql.Database    = DirectLake over SQL  \n",
    "\n",
    "Azure.Lakehouse = DirectLake over One Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f1201-49d3-4e8a-af4d-b25460c836be",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for e in tom.model.Expressions:\n",
    "        print(e.Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662bccf-96ff-4237-a167-739a9188ede7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 22. Show storage mode for each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884eb022-3cfd-4faa-92e3-79f32525950d",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "objects = {}\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for t in tom.model.Tables:\n",
    "        #print(t.Name)\n",
    "        for p in t.Partitions:\n",
    "            #print(p.Mode)\n",
    "            objects[t.Name] = str(p.Mode)\n",
    " \n",
    "df=pd.DataFrame([objects])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42ead8-fb11-4abc-be7b-bf99d2e170dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 23. Run query on 2bln row table\n",
    "This should work, but fall back to SQL Endpoint\n",
    "(Run twice if you get error first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6546b0c5-fd4c-4ab8-8cf4-7a0f9064250d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "        EVALUATE\n",
    "\t        SUMMARIZECOLUMNS(\n",
    "\t\t        dim_Date[DateKey],\n",
    "\t\t        \"Quantity\" , [Sum of Sales (2bln)]\n",
    "\t\t        )\n",
    "\"\"\",workspace_name,ClonedModelName,DMV=False)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f1c63-8279-482b-b1d4-11bca298b658",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 24. Show TMSL code for cloned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7266c6-802c-4819-9a84-42cb342d3df6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    x= tom.get_bim()\n",
    "\n",
    "    formatted_json = json.dumps(x, indent=4)\n",
    "    print(formatted_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc32aaa7",
   "metadata": {},
   "source": [
    "## 25. Stop the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69423f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805cf68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab 8 Summary: Hybrid Storage Mode Mastery\n",
    "\n",
    "**ðŸŽ¯ What You Accomplished:**\n",
    "This lab demonstrated the most advanced Direct Lake techniques, showcasing hybrid storage modes that combine Direct Lake over One Lake with Import mode capabilities for maximum flexibility and performance.\n",
    "\n",
    "**ðŸ”§ Key Technical Achievements:**\n",
    "- **Hybrid Architecture Implementation**: Successfully configured Direct Lake over One Lake with Import mode fallback capabilities\n",
    "- **Storage Mode Optimization**: Demonstrated conversion between Direct Lake over One Lake and Direct Lake over SQL Endpoint\n",
    "- **Large-Scale Performance Testing**: Validated query performance on datasets ranging from 1 billion to 2 billion rows\n",
    "- **Guardrail Management**: Explored Direct Lake guardrails and automatic fallback mechanisms to SQL Endpoint\n",
    "- **Advanced Relationship Configuration**: Implemented many-to-many relationships for optimal large table performance\n",
    "\n",
    "**ðŸ’¡ Business Impact:**\n",
    "- **Scalable Architecture**: Hybrid storage modes provide the best of both worlds - Direct Lake performance with Import mode reliability\n",
    "- **Cost Optimization**: Intelligent fallback mechanisms ensure queries succeed while maintaining optimal performance characteristics\n",
    "- **Enterprise Readiness**: Advanced configurations support the largest enterprise datasets with guaranteed query execution\n",
    "- **Flexibility**: Multiple storage mode options allow for precise performance tuning based on specific business requirements\n",
    "\n",
    "**ðŸš€ Advanced Concepts Mastered:**\n",
    "- Direct Lake over One Lake vs. SQL Endpoint implementation differences\n",
    "- Hybrid storage mode architecture patterns\n",
    "- Large-scale data performance optimization strategies\n",
    "- Intelligent query fallback mechanisms\n",
    "\n",
    "**Next Steps**: You now possess comprehensive knowledge of all Direct Lake storage modes and hybrid architectures, enabling you to design and implement enterprise-scale semantic models that deliver optimal performance across any dataset size while maintaining reliability and cost effectiveness."
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1800000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
