{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d63d4a-a0ee-4357-a020-7103ba035f01",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 8: Direct Lake over One Lake with Import Mode\n",
    "\n",
    "This lab demonstrates how to combine Direct Lake and Import mode tables within a single semantic model. You will clone an existing model, convert it from Direct Lake over SQL to Direct Lake over One Lake, switch one table to Import mode, refresh it, and observe how guardrails and fallback behaviour differ between the two Direct Lake variants.\n",
    "\n",
    "**Prerequisites:** Lab 2 must be completed (Big Data lakehouse and semantic model).\n",
    "\n",
    "---\n",
    "\n",
    "*Dieses Lab zeigt, wie Sie Direct Lake- und Import-Modus-Tabellen in einem einzigen Semantic Model kombinieren koennen. Sie klonen ein bestehendes Modell, konvertieren es von Direct Lake ueber SQL zu Direct Lake ueber One Lake, stellen eine Tabelle auf den Import-Modus um, aktualisieren sie und beobachten, wie sich Guardrails und Fallback-Verhalten zwischen den beiden Direct Lake-Varianten unterscheiden.*\n",
    "\n",
    "*Voraussetzung: Lab 2 muss abgeschlossen sein (Big Data Lakehouse und Semantic Model).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af395dd-b0d0-46a3-995f-d859eed2f903",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Install Semantic Link Labs\n",
    "\n",
    "Install the Semantic Link Labs Python library used throughout this lab.\n",
    "\n",
    "---\n",
    "\n",
    "*Installieren Sie die Semantic Link Labs Python-Bibliothek, die in diesem Lab verwendet wird.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3356d3-7f40-4071-9a25-c42dfbdc27e1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07e358-e8a6-4e44-8e9c-baf45bed2d18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Load Libraries and Setup Parameters\n",
    "\n",
    "Import required libraries and configure workspace, lakehouse, and model identifiers.\n",
    "\n",
    "---\n",
    "\n",
    "*Importieren Sie die erforderlichen Bibliotheken und konfigurieren Sie Workspace-, Lakehouse- und Modellbezeichner.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfd109-9068-486b-a42d-101366ce7554",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries for hybrid storage mode operations and model management\n",
    "import sempy_labs as labs\n",
    "import sempy\n",
    "from sempy import fabric\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from sempy_labs.tom._model import TOMWrapper, connect_semantic_model\n",
    "\n",
    "# Import specialized helper functions for advanced operations\n",
    "from sempy_labs._helper_functions import (\n",
    "    format_dax_object_name,\n",
    "    generate_guid,\n",
    "    _make_list_unique,\n",
    "    resolve_dataset_name_and_id,\n",
    "    resolve_workspace_name_and_id,\n",
    "    _base_api,\n",
    "    resolve_workspace_id,\n",
    "    resolve_item_id,\n",
    "    resolve_lakehouse_id,\n",
    "    resolve_lakehouse_name_and_id\n",
    ")\n",
    "\n",
    "# Initialize Analysis Services for advanced model operations\n",
    "fabric._client._utils._init_analysis_services()\n",
    "import Microsoft.AnalysisServices.Tabular as TOM\n",
    "import Microsoft.AnalysisServices\n",
    "import warnings\n",
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "\n",
    "# Configure model names for hybrid storage mode testing\n",
    "LakehouseName = \"BigData\"\n",
    "SemanticModelName = f\"{LakehouseName}_model\"\n",
    "ClonedModelName = SemanticModelName + \"_clone\"\n",
    "workspace = None\n",
    "\n",
    "(workspace_name, workspace_id) = resolve_workspace_name_and_id(workspace)\n",
    "(lakehouse_name, lakehouse_id) = resolve_lakehouse_name_and_id(lakehouse=LakehouseName, workspace=workspace)\n",
    "\n",
    "#### Generate Unique Trace Name - Start ####\n",
    "import json, base64, re\n",
    "token = notebookutils.credentials.getToken(\"pbi\")\n",
    "payload = token.split(\".\")[1]\n",
    "payload += \"=\" * (4 - len(payload) % 4)\n",
    "upn = json.loads(base64.b64decode(payload)).get(\"upn\")\n",
    "\n",
    "# Extract just the user part (e.g. \"SQLKDL.user39\")\n",
    "user_id = upn.split(\"@\")[0]\n",
    "lab_number = 8  # set per lab\n",
    "\n",
    "# Remove characters not allowed in trace names: . , ; ' ` : / \\ * | ? \" & % $ ! + = ( ) [ ] { } < >\n",
    "user_id_clean = re.sub(r\"[.,;'`:/\\\\*|?\\\"&%$!+=(){}\\[\\]<>]\", \"_\", user_id)\n",
    "trace_name = f\"Lab{lab_number}_{user_id_clean}\"\n",
    "#### Generate Unique Trace Name - End ####\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)\n",
    "\n",
    "def filter_func(e):\n",
    "    retVal:bool=True\n",
    "    if e.EventSubclass.ToString() == \"VertiPaqScanInternal\":\n",
    "        retVal=False      \n",
    "    #     #if e.EventSubClass.ToString() == \"VertiPaqScanInternal\":\n",
    "    #     retVal=False\n",
    "    return retVal\n",
    "\n",
    "# define events to trace and their corresponding columns\n",
    "def runQueryWithTrace (expr:str,workspaceName:str,SemanticModelName:str,Result:Optional[bool]=True,Trace:Optional[bool]=True,DMV:Optional[bool]=True,ClearCache:Optional[bool]=True) -> pd.DataFrame :\n",
    "    event_schema = fabric.Trace.get_default_query_trace_schema()\n",
    "    event_schema.update({\"ExecutionMetrics\":[\"EventClass\",\"TextData\"]})\n",
    "    del event_schema['VertiPaqSEQueryBegin']\n",
    "    del event_schema['VertiPaqSEQueryCacheMatch']\n",
    "    del event_schema['DirectQueryBegin']\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    WorkspaceName = workspaceName\n",
    "    SemanticModelName = SemanticModelName\n",
    "\n",
    "    if ClearCache:\n",
    "        labs.clear_cache(SemanticModelName)\n",
    "\n",
    "    with fabric.create_trace_connection(SemanticModelName,WorkspaceName) as trace_connection:\n",
    "        # create trace on server with specified events\n",
    "        with trace_connection.create_trace(\n",
    "            event_schema=event_schema, \n",
    "            name=trace_name,\n",
    "            filter_predicate=filter_func,\n",
    "            stop_event=\"QueryEnd\"\n",
    "            ) as trace:\n",
    "\n",
    "            trace.start()\n",
    "\n",
    "            df=sempy.fabric.evaluate_dax(\n",
    "                dataset=SemanticModelName, \n",
    "                dax_string=expr)\n",
    "\n",
    "            if Result:\n",
    "                displayHTML(f\"<H2>####### DAX QUERY RESULT #######</H2>\")\n",
    "                display(df)\n",
    "\n",
    "            # Wait 5 seconds for trace data to arrive\n",
    "            time.sleep(5)\n",
    "\n",
    "            # stop Trace and collect logs\n",
    "            final_trace_logs = trace.stop()\n",
    "\n",
    "    if Trace:\n",
    "        displayHTML(f\"<H2>####### SERVER TIMINGS #######</H2>\")\n",
    "        display(final_trace_logs)\n",
    "    \n",
    "    if DMV:\n",
    "        displayHTML(f\"<H2>####### SHOW DMV RESULTS #######</H2>\")\n",
    "        runDMV()\n",
    "    \n",
    "    return final_trace_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9aadf-8559-4d03-b64b-00f0fb6f02e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Clone the BigData Semantic Model\n",
    "\n",
    "Create a copy of the existing BigData semantic model to use for hybrid storage mode experiments without affecting the original.\n",
    "\n",
    "---\n",
    "\n",
    "*Erstellen Sie eine Kopie des bestehenden BigData Semantic Models fuer hybride Speichermodus-Experimente, ohne das Original zu beeinflussen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a99238-7a97-4adc-8e86-d83f6b03569d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "#Clear any existing cloned model if re-running\n",
    "df = fabric.list_items()\n",
    "if ClonedModelName in df.values:\n",
    "    model_id = df.at[df[df['Display Name'] == ClonedModelName].index[0], 'Id']\n",
    "    fabric.delete_item(model_id)\n",
    "    print(\"Cloned model deleted\")\n",
    "\n",
    "with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "    newDB = tom._tom_server.Databases.GetByName(SemanticModelName).Clone()\n",
    "    newModel = tom._tom_server.Databases.GetByName(SemanticModelName).Model.Clone()\n",
    "    newDB.Name = ClonedModelName\n",
    "    newDB.ID = str(uuid.uuid4())\n",
    "    #newDB.Model = newModel\n",
    "    newModel.CopyTo(newDB.Model)\n",
    "    tom._tom_server.Databases.Add(newDB)\n",
    "\n",
    "    newDB.Update(Microsoft.AnalysisServices.UpdateOptions.ExpandFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc048908-0e39-4d19-8ab0-76e7b7119a23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 4: Frame the Cloned Model\n",
    "\n",
    "Reframe the cloned model to initialise all data connections and relationships.\n",
    "\n",
    "---\n",
    "\n",
    "*Reframen Sie das geklonte Modell, um alle Datenverbindungen und Beziehungen zu initialisieren.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385bc2ca-99d8-4ab7-8905-690fd9d82f83",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Refresh the cloned model to initialize data connections\n",
    "labs.refresh_semantic_model(dataset=ClonedModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67971155",
   "metadata": {},
   "source": [
    "## Step 5: Clear Schema Name\n",
    "\n",
    "Temporary step to clear the schema name property and resolve a known issue.\n",
    "\n",
    "---\n",
    "\n",
    "*Temporaerer Schritt zum Loeschen der Schema-Name-Eigenschaft, um ein bekanntes Problem zu beheben.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for t in tom.model.Tables:\n",
    "        for p in t.Partitions:\n",
    "            if isinstance(p.Source,Microsoft.AnalysisServices.Tabular.EntityPartitionSource):\n",
    "                p.Source.SchemaName=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a76b88",
   "metadata": {},
   "source": [
    "## Step 6: Reframe After Schema Change\n",
    "\n",
    "Reframe the model again so it picks up the schema name change from the previous step.\n",
    "\n",
    "---\n",
    "\n",
    "*Reframen Sie das Modell erneut, damit es die Schemanamenaenderung aus dem vorherigen Schritt uebernimmt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the cloned model to initialize data connections\n",
    "labs.refresh_semantic_model(dataset=ClonedModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748b06b-ddab-49ab-adbd-b11d5487881d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 7: Check Direct Lake Version\n",
    "\n",
    "Identify whether the model currently uses Direct Lake over SQL (Sql.Database) or Direct Lake over One Lake (Azure.Lakehouse).\n",
    "\n",
    "- `let database = Sql.Database(...)` = **DL/SQL**\n",
    "- `let database = Azure.Lakehouse(...)` = **DL/OL**\n",
    "\n",
    "---\n",
    "\n",
    "*Stellen Sie fest, ob das Modell derzeit Direct Lake ueber SQL (Sql.Database) oder Direct Lake ueber One Lake (Azure.Lakehouse) verwendet.*\n",
    "\n",
    "- *Sql.Database(...) = DL/SQL*\n",
    "- *Azure.Lakehouse(...) = DL/OL*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633e142-19e3-4595-b4c2-45e15d03f82a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for e in tom.model.Expressions:\n",
    "        print(e.Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4a9ee-1d8b-4f08-beac-68f66565b5d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 8: Show Storage Mode for Each Table\n",
    "\n",
    "Display the current storage mode of every table in the cloned model.\n",
    "\n",
    "---\n",
    "\n",
    "*Zeigen Sie den aktuellen Speichermodus jeder Tabelle im geklonten Modell an.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea073c-45c6-4f3d-8a1f-059e6dd12501",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "objects = {}\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for t in tom.model.Tables:\n",
    "        #print(t.Name)\n",
    "        for p in t.Partitions:\n",
    "            #print(p.Mode)\n",
    "            objects[t.Name] = str(p.Mode)\n",
    " \n",
    "df=pd.DataFrame([objects])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14e8e7-7a27-44b6-9bdb-7f6c5c7df19d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 9: Attempt Import Conversion (DL/SQL — Will Fail)\n",
    "\n",
    "Try to convert a Direct Lake table to Import mode while the model is still using DL/SQL. This is expected to fail, demonstrating that Import conversion requires DL/OL.\n",
    "\n",
    "---\n",
    "\n",
    "*Versuchen Sie, eine Direct Lake-Tabelle in den Import-Modus zu konvertieren, waehrend das Modell noch DL/SQL verwendet. Dies wird erwartungsgemaess fehlschlagen und zeigt, dass die Import-Konvertierung DL/OL erfordert.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7e6c2-6e56-40d9-b570-90976bc5eb60",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Attempt to convert Direct Lake table to Import mode (will fail if Direct Lake over SQL)\n",
    "try:\n",
    "    with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "        tom.convert_direct_lake_to_import(\n",
    "            table_name=\"dim_Date\" ,\n",
    "            entity_name=\"dim_Date\" ,\n",
    "            source=\"BigData\",\n",
    "            source_type = \"Lakehouse\"\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021862b-6754-4ff9-827b-047a0dec4674",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 10: Convert to Direct Lake over One Lake\n",
    "\n",
    "Switch the cloned model from DL/SQL to DL/OL so that Import mode conversion becomes possible.\n",
    "\n",
    "---\n",
    "\n",
    "*Stellen Sie das geklonte Modell von DL/SQL auf DL/OL um, damit die Konvertierung in den Import-Modus moeglich wird.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ac75c-ae9c-4c06-8017-8341c71d50ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "\n",
    "    for e in tom.model.Expressions:\n",
    "        e.Expression = f\"\"\"\n",
    "        let\n",
    "            Source = AzureStorage.DataLake(\"https://onelake.dfs.fabric.microsoft.com/{workspace_id}/{lakehouse_id}\", [HierarchicalNavigation=true])\n",
    "        in\n",
    "            Source\"\"\"\n",
    "        \n",
    "print(\"Converted semantic model to use DirectLake over One Lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d226b6-0dfc-422d-a656-bc35a2ed12c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 11: Convert Direct Lake Table to Import (DL/OL — Will Succeed)\n",
    "\n",
    "Now that the model uses DL/OL, convert a table to Import mode. This creates a hybrid model with both Direct Lake and Import tables.\n",
    "\n",
    "---\n",
    "\n",
    "*Nachdem das Modell DL/OL verwendet, konvertieren Sie eine Tabelle in den Import-Modus. Dadurch entsteht ein hybrides Modell mit sowohl Direct Lake- als auch Import-Tabellen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396054e2-5eb2-4a0e-bdea-54b9ad278c1e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert Direct Lake table to Import mode (should work with Direct Lake over One Lake)\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    tom.convert_direct_lake_to_import(\n",
    "        table_name=\"dim_Date\" ,\n",
    "        entity_name=\"dim_Date\" ,\n",
    "        source=\"BigData\",\n",
    "        source_type = \"Lakehouse\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9c1ba-ec93-48c4-a7c8-721ac6b98500",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 12: Show Storage Modes After Conversion\n",
    "\n",
    "Display the updated storage modes, confirming the hybrid configuration with both Direct Lake and Import tables.\n",
    "\n",
    "---\n",
    "\n",
    "*Zeigen Sie die aktualisierten Speichermodi an und bestaetigen Sie die hybride Konfiguration mit sowohl Direct Lake- als auch Import-Tabellen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a8f7e-c0b2-4b01-ab63-67ab0cb8cc3a",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "objects = {}\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for t in tom.model.Tables:\n",
    "        #print(t.Name)\n",
    "        for p in t.Partitions:\n",
    "            #print(p.Mode)\n",
    "            objects[t.Name] = str(p.Mode)\n",
    " \n",
    "df=pd.DataFrame([objects])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868820b-7b38-4fb9-b438-a3aac82b24ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 13: Set Credentials and Enable Large Model Format (Manual Step)\n",
    "\n",
    "This must be done in the Power BI service, not via a notebook. Create a new Shared Cloud Connection for refreshing the Import table. Use OAuth2 for authentication in this lab (Service Principal is recommended for production). Ensure the Privacy Level is set to Organizational (default).\n",
    "\n",
    "---\n",
    "\n",
    "*Dieser Schritt muss im Power BI Service durchgefuehrt werden, nicht ueber ein Notebook. Erstellen Sie eine neue Shared Cloud Connection zum Aktualisieren der Import-Tabelle. Verwenden Sie OAuth2 fuer die Authentifizierung in diesem Lab (Service Principal wird fuer die Produktion empfohlen). Stellen Sie sicher, dass die Datenschutzebene auf Organisatorisch (Standard) eingestellt ist.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3da9d-c267-4dcf-8fbd-36f80c26505a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 14: Refresh the Import Table\n",
    "\n",
    "Trigger a refresh of the Import mode table so it loads data from the lakehouse.\n",
    "\n",
    "---\n",
    "\n",
    "*Loesen Sie eine Aktualisierung der Import-Modus-Tabelle aus, damit sie Daten aus dem Lakehouse laedt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447cca4-0e31-4492-af98-0ae0d96ace79",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "labs.refresh_semantic_model(dataset=ClonedModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcef16-502c-4517-873b-90bafe383dbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 16: Verify Direct Lake Version\n",
    "\n",
    "Confirm which Direct Lake variant the model is currently using.\n",
    "\n",
    "---\n",
    "\n",
    "*Bestaetigen Sie, welche Direct Lake-Variante das Modell derzeit verwendet.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a5e7c6-6b05-4a7f-87a2-e05ed34df9ad",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for e in tom.model.Expressions:\n",
    "        print(e.Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504bf7f1-ef01-40e9-b67f-0bc1efb82f07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 18: Query the 1 Billion Row Table\n",
    "\n",
    "Run a DAX query against the table with approximately one billion rows to verify the hybrid model works.\n",
    "\n",
    "---\n",
    "\n",
    "*Fuehren Sie eine DAX-Abfrage gegen die Tabelle mit ungefaehr einer Milliarde Zeilen aus, um zu ueberpruefen, ob das hybride Modell funktioniert.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b0ade2-ebf5-41d0-9c02-1016a8b1440a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "        EVALUATE\n",
    "\t        SUMMARIZECOLUMNS(\n",
    "\t\t        dim_Date[DateKey],\n",
    "\t\t        \"Quantity\" , [Sum of Sales (1bln)]\n",
    "\t\t        )\n",
    "\"\"\",workspace_name,ClonedModelName,DMV=False)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18476ac8-6970-4f8b-b41e-299c6e2752dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 19: Query the 2 Billion Row Table\n",
    "\n",
    "Run a DAX query against the table with approximately two billion rows. This query is expected to fail due to Direct Lake guardrail limits.\n",
    "\n",
    "---\n",
    "\n",
    "*Fuehren Sie eine DAX-Abfrage gegen die Tabelle mit ungefaehr zwei Milliarden Zeilen aus. Diese Abfrage wird voraussichtlich aufgrund von Direct Lake Guardrail-Limits fehlschlagen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129efc8-5e63-4684-8b40-563ea586782e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "\tdf = runQueryWithTrace(\"\"\"\n",
    "\t\t\tEVALUATE\n",
    "\t\t\t\tSUMMARIZECOLUMNS(\n",
    "\t\t\t\t\tdim_Date[DateKey],\n",
    "\t\t\t\t\t\"Quantity\" , [Sum of Sales (2bln)]\n",
    "\t\t\t\t\t)\n",
    "\t\"\"\",workspace_name,ClonedModelName,DMV=False)\n",
    "\n",
    "\tdisplay(df)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e779019-d17c-43e3-b946-40b6f015bbcf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 20: Convert Back to DL/SQL\n",
    "\n",
    "Switch the cloned model back to Direct Lake over SQL to observe the difference in fallback behaviour.\n",
    "\n",
    "---\n",
    "\n",
    "*Stellen Sie das geklonte Modell zurueck auf Direct Lake ueber SQL, um den Unterschied im Fallback-Verhalten zu beobachten.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d9159-f096-426c-ae42-9828e778887f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(labs.list_lakehouses())\n",
    "endpointid = df[df['Lakehouse Name']==LakehouseName]['SQL Endpoint ID'].iloc[0]\n",
    "server = df[df['Lakehouse Name']==LakehouseName]['SQL Endpoint Connection String'].iloc[0]\n",
    "\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "\n",
    "    #Convert import tables to Direct Lake\n",
    "    for t in tom.model.Tables:\n",
    "        for p in t.Partitions:\n",
    "            if(p.Mode==TOM.ModeType.Import):\n",
    "                t.Partitions.Remove(p)\n",
    "                tom.add_entity_partition(table_name=t.Name,entity_name=t.Name)\n",
    "                print(f\"Table {t.Name} converted\")\n",
    "            p.Source.SchemaName=None\n",
    "\n",
    "    #Switch Model to Direct Lake over SQL\n",
    "    for e in tom.model.Expressions:\n",
    "        e.Expression = f\"\"\"\n",
    "        let\n",
    "            Source = Sql.Database(\"{server}\", \"{endpointid}\")\n",
    "        in\n",
    "            Source\"\"\"\n",
    "\n",
    "print(\"Converted to Direct Lake over SQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1277a1",
   "metadata": {},
   "source": [
    "## Step 21: Confirm Direct Lake Version\n",
    "\n",
    "Verify the model is now back on DL/SQL.\n",
    "\n",
    "- `Sql.Database(...)` = DL/SQL\n",
    "- `Azure.Lakehouse(...)` = DL/OL\n",
    "\n",
    "---\n",
    "\n",
    "*Ueberpruefen Sie, dass das Modell jetzt wieder auf DL/SQL laeuft.*\n",
    "\n",
    "- *Sql.Database(...) = DL/SQL*\n",
    "- *Azure.Lakehouse(...) = DL/OL*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f1201-49d3-4e8a-af4d-b25460c836be",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for e in tom.model.Expressions:\n",
    "        print(e.Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662bccf-96ff-4237-a167-739a9188ede7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 22: Show Storage Modes\n",
    "\n",
    "Display the storage mode of each table after converting back to DL/SQL.\n",
    "\n",
    "---\n",
    "\n",
    "*Zeigen Sie den Speichermodus jeder Tabelle an, nachdem Sie zurueck auf DL/SQL konvertiert haben.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884eb022-3cfd-4faa-92e3-79f32525950d",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "objects = {}\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    for t in tom.model.Tables:\n",
    "        #print(t.Name)\n",
    "        for p in t.Partitions:\n",
    "            #print(p.Mode)\n",
    "            objects[t.Name] = str(p.Mode)\n",
    " \n",
    "df=pd.DataFrame([objects])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42ead8-fb11-4abc-be7b-bf99d2e170dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 23: Query the 2 Billion Row Table (DL/SQL with Fallback)\n",
    "\n",
    "Re-run the 2 billion row query now that the model is on DL/SQL. The query should succeed by falling back to the SQL Endpoint. Run twice if the first attempt returns an error.\n",
    "\n",
    "---\n",
    "\n",
    "*Fuehren Sie die 2-Milliarden-Zeilen-Abfrage erneut aus, nachdem das Modell auf DL/SQL laeuft. Die Abfrage sollte durch Fallback auf den SQL-Endpoint erfolgreich sein. Fuehren Sie sie zweimal aus, falls der erste Versuch einen Fehler liefert.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6546b0c5-fd4c-4ab8-8cf4-7a0f9064250d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "        EVALUATE\n",
    "\t        SUMMARIZECOLUMNS(\n",
    "\t\t        dim_Date[DateKey],\n",
    "\t\t        \"Quantity\" , [Sum of Sales (2bln)]\n",
    "\t\t        )\n",
    "\"\"\",workspace_name,ClonedModelName,DMV=False)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f1c63-8279-482b-b1d4-11bca298b658",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 24: Show TMSL for the Cloned Model\n",
    "\n",
    "Display the full TMSL definition of the cloned model for reference.\n",
    "\n",
    "---\n",
    "\n",
    "*Zeigen Sie die vollstaendige TMSL-Definition des geklonten Modells als Referenz an.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7266c6-802c-4819-9a84-42cb342d3df6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with labs.tom.connect_semantic_model(dataset=ClonedModelName, readonly=False) as tom:\n",
    "    x= tom.get_bim()\n",
    "\n",
    "    formatted_json = json.dumps(x, indent=4)\n",
    "    print(formatted_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc32aaa7",
   "metadata": {},
   "source": [
    "## Step 25: Stop the Spark Session\n",
    "\n",
    "---\n",
    "\n",
    "*Spark-Sitzung beenden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69423f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805cf68",
   "metadata": {},
   "source": [
    "## Lab Summary\n",
    "\n",
    "In this lab you explored hybrid storage modes by cloning a Direct Lake model, converting it between DL/SQL and DL/OL, and switching one table to Import mode. You observed that Import conversion is only possible under DL/OL, tested guardrail behaviour on tables with one and two billion rows, and saw how DL/SQL provides automatic SQL Endpoint fallback when guardrails are exceeded.\n",
    "\n",
    "**Key Takeaways**\n",
    "- Direct Lake has two variants: DL/SQL (Sql.Database) and DL/OL (Azure.Lakehouse). The variant affects which features are available.\n",
    "- Converting a table from Direct Lake to Import mode requires the model to be on DL/OL. The attempt fails under DL/SQL.\n",
    "- Hybrid models (Direct Lake + Import tables) allow you to keep most tables on Direct Lake while importing specific tables that benefit from in-memory storage.\n",
    "- After refreshing an Import table, relationship indexes must be recalculated for cross-storage-mode joins to work.\n",
    "- DL/OL does not support automatic fallback to the SQL Endpoint — queries that hit guardrails will fail outright.\n",
    "- DL/SQL supports automatic fallback, so the same two-billion-row query that fails under DL/OL succeeds under DL/SQL by routing through the SQL Endpoint.\n",
    "- Shared Cloud Connections with OAuth2 (or Service Principal for production) are required to refresh Import tables in the service.\n",
    "\n",
    "**Workshop Complete.** You have now covered all core Direct Lake topics: model creation, big data, Delta analysis, fallback behaviour, framing, column partitioning, high cardinality optimisation, and hybrid storage modes.\n",
    "\n",
    "---\n",
    "\n",
    "*In diesem Lab haben Sie hybride Speichermodi erkundet, indem Sie ein Direct Lake-Modell geklont, es zwischen DL/SQL und DL/OL konvertiert und eine Tabelle in den Import-Modus umgestellt haben. Sie haben beobachtet, dass die Import-Konvertierung nur unter DL/OL moeglich ist, das Guardrail-Verhalten bei Tabellen mit einer und zwei Milliarden Zeilen getestet und gesehen, wie DL/SQL automatischen SQL-Endpoint-Fallback bietet, wenn Guardrails ueberschritten werden.*\n",
    "\n",
    "**Wichtige Erkenntnisse**\n",
    "- Direct Lake hat zwei Varianten: DL/SQL (Sql.Database) und DL/OL (Azure.Lakehouse). Die Variante bestimmt, welche Funktionen verfuegbar sind.\n",
    "- Die Konvertierung einer Tabelle von Direct Lake in den Import-Modus erfordert, dass das Modell auf DL/OL laeuft. Der Versuch schlaegt unter DL/SQL fehl.\n",
    "- Hybride Modelle (Direct Lake + Import-Tabellen) ermoeglichen es, die meisten Tabellen auf Direct Lake zu belassen, waehrend bestimmte Tabellen importiert werden, die von der In-Memory-Speicherung profitieren.\n",
    "- Nach der Aktualisierung einer Import-Tabelle muessen die Beziehungsindizes neu berechnet werden, damit speichermodusuebergreifende Joins funktionieren.\n",
    "- DL/OL unterstuetzt keinen automatischen Fallback auf den SQL-Endpoint — Abfragen, die Guardrails erreichen, schlagen direkt fehl.\n",
    "- DL/SQL unterstuetzt automatischen Fallback, sodass dieselbe Zwei-Milliarden-Zeilen-Abfrage, die unter DL/OL fehlschlaegt, unter DL/SQL ueber den SQL-Endpoint erfolgreich ist.\n",
    "- Shared Cloud Connections mit OAuth2 (oder Service Principal fuer die Produktion) sind erforderlich, um Import-Tabellen im Service zu aktualisieren.\n",
    "\n",
    "**Workshop abgeschlossen.** Sie haben jetzt alle zentralen Direct Lake-Themen behandelt: Modellerstellung, Big Data, Delta-Analyse, Fallback-Verhalten, Framing, Spaltenpartitionierung, Hochkardinalitaetsoptimierung und hybride Speichermodi."
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1800000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
