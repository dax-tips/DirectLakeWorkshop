{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09b6c60-d142-42f2-aa77-b714234d69d3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 1: Create Direct Lake Semantic Model\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "This lab teaches you how to create a **Direct Lake semantic model** from scratch using Microsoft Fabric. You'll learn the complete workflow from data loading to model creation and validation.\n",
    "\n",
    "### What You'll Build\n",
    "\n",
    "**Workshop Flow:**\n",
    "```\n",
    "1. Lakehouse Setup\n",
    "   ↓\n",
    "2. Load Adventure Works Data  \n",
    "   ↓\n",
    "3. Create Semantic Model\n",
    "   ↓\n",
    "4. Add Relationships\n",
    "   ↓\n",
    "5. Create Measures\n",
    "   ↓\n",
    "6. Test & Validate\n",
    "```\n",
    "\n",
    "**End Result:** A fully functional Direct Lake semantic model ready for Power BI reporting with real-time data access.\n",
    "\n",
    "### Key Concepts\n",
    "- **Direct Lake**: Query data directly from Delta Lake without imports\n",
    "- **Adventure Works**: Sample business dataset with customers, products, and sales\n",
    "- **Semantic Model**: Business logic layer with relationships and measures\n",
    "\n",
    "### Learning Objectives\n",
    "By completing this lab, you'll be able to:\n",
    "- ✅ Set up a lakehouse and load sample data\n",
    "- ✅ Create a Direct Lake semantic model programmatically  \n",
    "- ✅ Define table relationships and business measures\n",
    "- ✅ Validate model performance and behavior\n",
    "\n",
    "**Estimated Time**: 30-45 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd099e87-89f6-45b2-b286-d7f8a12b6a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "Install Semantic Link Labs to enable Direct Lake model creation and management capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ed5ca-a6ba-478a-b029-17b6db9b6308",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q --disable-pip-version-check semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc44ce-395c-4db3-8979-b06acf9f8ecf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2. Import Libraries and Set Variables\n",
    "\n",
    "Import required libraries and define key variables for the lakehouse and semantic model names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e841d6-a757-4029-aa71-88d4bd286c30",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as labs\n",
    "from sempy import fabric\n",
    "import sempy\n",
    "import pandas\n",
    "import json\n",
    "import time\n",
    "\n",
    "LakehouseName = \"AdventureWorks\"\n",
    "SemanticModelName = f\"{LakehouseName}_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99f94c-a564-4aca-b670-31da354b7b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3. Create or Connect to Lakehouse\n",
    "\n",
    "Check if the AdventureWorks lakehouse exists, create it if needed, and retrieve workspace identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760df625-543c-4289-b6cc-f043290d5879",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "lakehouses=labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "if LakehouseName in lakehouses.values:\n",
    "    lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]\n",
    "else:\n",
    "    lakehouseId = fabric.create_lakehouse(LakehouseName)\n",
    "\n",
    "workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "workspaceName = sempy.fabric.resolve_workspace_name(workspaceId)\n",
    "print(f\"WorkspaceId = {workspaceId}, LakehouseID = {lakehouseId}, Workspace Name = {workspaceName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6e83e-d5fd-4020-9766-c236ef176329",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Load Adventure Works Sample Data\n",
    "\n",
    "Load four Adventure Works tables (Customer, Date, Product, Sales) into the lakehouse using region-aware data sources.\n",
    "\n",
    "**Tables being loaded:**\n",
    "- DimCustomer (~18K customers)  \n",
    "- DimDate (2K+ dates)\n",
    "- DimProduct (~600 products)\n",
    "- FactInternetSales (~60K sales records)\n",
    "```\n",
    "Loaded DimCustomer\n",
    "Loaded DimDate  \n",
    "Loaded DimProduct\n",
    "Loaded FactInternetSales\n",
    "Done\n",
    "```\n",
    "\n",
    "### Behind the Scenes\n",
    "- Data is stored in **Delta format** for ACID compliance\n",
    "- **Overwrite mode** ensures clean data for the workshop\n",
    "- **OneLake integration** provides seamless cross-workspace data access\n",
    "\n",
    "🎯 **Success indicator**: All four \"Loaded\" messages followed by \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28505f17-8bf7-4400-a409-344c4d01b4ef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "capacity_name = labs.get_capacity_name()\n",
    "\n",
    "def loadDataToLakehouse(fromTable: str, toTable: str):\n",
    "    \"\"\"\n",
    "    Optimized data loading function with improved error handling and performance.\n",
    "    \n",
    "    Args:\n",
    "        fromTable: Source table name to read from\n",
    "        toTable: Target table name to write to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get lakehouse properties once and reuse\n",
    "        lakehouse_props = notebookutils.lakehouse.getWithProperties(LakehouseName)\n",
    "        workspaceId = lakehouse_props[\"workspaceId\"]\n",
    "        lakehouseId = lakehouse_props[\"id\"]\n",
    "\n",
    "        # Region-aware connection string selection\n",
    "        if capacity_name == \"FabConUS8-P1\":  # West US 3\n",
    "            conn_str = \"abfss://b1d61bbe-de20-4d3a-8075-b8e2eaacb868@onelake.dfs.fabric.microsoft.com/631e45c0-1243-4f42-920a-56bfe6ecdd6d/Tables\"\n",
    "        else:  # North Central US (default)\n",
    "            conn_str = \"abfss://16cf855f-3bf4-4312-a7a1-ccf5cb6a0121@onelake.dfs.fabric.microsoft.com/99ed86df-13d1-4008-a7f6-5768e53f4f85/Tables\"\n",
    "\n",
    "        # Read source data with format specification for better performance\n",
    "        customer_df = spark.read.format(\"delta\").load(f\"{conn_str}/{fromTable}\")\n",
    "        \n",
    "        # Cache the DataFrame if it will be used multiple times or is computation-heavy\n",
    "        customer_df.cache()\n",
    "        \n",
    "        # Write with optimized settings\n",
    "        (customer_df\n",
    "         .write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .save(f\"abfss://{workspaceId}@onelake.dfs.fabric.microsoft.com/{lakehouseId}/Tables/{toTable}\"))\n",
    "        \n",
    "        # Unpersist cached DataFrame to free memory\n",
    "        customer_df.unpersist()\n",
    "        \n",
    "        print(f\"Loaded {toTable}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {toTable}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load all tables with proper error handling\n",
    "tables_to_load = [\n",
    "    (\"DimCustomer\", \"DimCustomer\"),\n",
    "    (\"DimDate\", \"DimDate\"),\n",
    "    (\"DimProduct\", \"DimProduct\"),\n",
    "    (\"FactInternetSales\", \"FactInternetSales\")\n",
    "]\n",
    "\n",
    "for from_table, to_table in tables_to_load:\n",
    "    loadDataToLakehouse(from_table, to_table)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff2996-0b1c-4792-99cd-8d61315e65da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 5. Trigger Metadata Synchronization\n",
    "\n",
    "Force synchronization between lakehouse storage and SQL Analytics Endpoint to ensure schema accuracy for the semantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54e1aa-a94e-4efd-b40e-f497f54589e8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "##https://medium.com/@sqltidy/delays-in-the-automatically-generated-schema-in-the-sql-analytics-endpoint-of-the-lakehouse-b01c7633035d\n",
    "\n",
    "def triggerMetadataRefresh():\n",
    "    client = fabric.FabricRestClient()\n",
    "    response = client.get(f\"/v1/workspaces/{workspaceId}/lakehouses/{lakehouseId}\")\n",
    "    sqlendpoint = response.json()['properties']['sqlEndpointProperties']['id']\n",
    "\n",
    "    # trigger sync\n",
    "    uri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}\"\n",
    "    payload = {\"commands\":[{\"$type\":\"MetadataRefreshExternalCommand\"}]}\n",
    "    response = client.post(uri,json= payload)\n",
    "    batchId = response.json()['batchId']\n",
    "\n",
    "    # Monitor Progress\n",
    "    statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "    statusresponsedata = client.get(statusuri).json()\n",
    "    progressState = statusresponsedata['progressState']\n",
    "    print(f\"Metadata refresh : {progressState}\")\n",
    "    while progressState != \"success\":\n",
    "        statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "        statusresponsedata = client.get(statusuri).json()\n",
    "        progressState = statusresponsedata['progressState']\n",
    "        print(f\"Metadata refresh : {progressState}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    print('Metadata refresh complete')\n",
    "\n",
    "triggerMetadataRefresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e16b1-9893-46d3-ac7d-93a4d1453c5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 6. Create Direct Lake Semantic Model\n",
    "\n",
    "Generates semantic model from lakehouse tables with automatic discovery and robust error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e9cc7-de53-4c81-8408-9fb18ef4383a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sempy import fabric\n",
    "\n",
    "#1. Generate list of ALL table names from lakehouse to add to Semantic Model\n",
    "lakehouseTables:list = labs.lakehouse.get_lakehouse_tables(lakehouse=LakehouseName)[\"Table Name\"]\n",
    "\n",
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        #2 Create the semantic model\n",
    "        if sempy.fabric.list_items().query(f\"`Display Name`=='{LakehouseName}_model' & Type=='SemanticModel'  \").shape[0] ==0:\n",
    "            labs.directlake.generate_direct_lake_semantic_model(dataset=f\"{LakehouseName}_model\",lakehouse_tables=lakehouseTables,workspace=workspaceName,lakehouse=lakehouseId,refresh=False,overwrite=True)\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error creating model... trying again.')\n",
    "        time.sleep(3)\n",
    "        triggerMetadataRefresh()\n",
    "\n",
    "print('Semantic model created OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df3373-52ca-4e45-83a6-40e1b20c184d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 7. Configure Table Relationships\n",
    "\n",
    "Establishes star schema relationships between fact and dimension tables for accurate cross-table analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616193a-57df-4139-91ef-c73830331555",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing relationships\n",
    "            for r in tom.model.Relationships:\n",
    "                tom.model.Relationships.Remove(r)\n",
    "\n",
    "            #2. Creates correct relationships\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"OrderDateKey\" , to_table=\"DimDate\"    , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"CustomerKey\"  , to_table=\"DimCustomer\", to_column=\"CustomerKey\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"ProductKey\"   , to_table=\"DimProduct\" , to_column=\"ProductKey\"    , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding relationships... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba5536-018c-402b-aa0f-663ee5e8d07f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 8. Add Business Intelligence Measures\n",
    "\n",
    "Creates essential DAX measures with proper formatting for business reporting and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666631c-4221-4638-b8ff-10fee8a4f7df",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing measures\n",
    "            for t in tom.model.Tables:\n",
    "                for m in t.Measures:\n",
    "                    tom.remove_object(m)\n",
    "                    print(f\"[{m.Name}] measure removed\")\n",
    "\n",
    "            tom.add_measure(table_name=\"FactInternetSales\" ,measure_name=\"Sum of Sales\",expression=\"SUM(FactInternetSales[SalesAmount])\",format_string=\"\\$#,0.###############;(\\$#,0.###############);\\$#,0.###############\")\n",
    "            tom.add_measure(table_name=\"FactInternetSales\" ,measure_name=\"Count of Sales\",expression=\"COUNTROWS(FactInternetSales)\",format_string=\"#,0\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding measures... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ff91f-0362-4c3e-bf2e-60c64867cec8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 9. Configure Date Table for Time Intelligence\n",
    "\n",
    "Marks DimDate table as date table to enable time-based analysis functions and calendar features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f48bf-893b-4a8f-92a1-e5e80b968840",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            tom.mark_as_date_table(table_name=\"DimDate\",column_name=\"Date\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error with date table... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb148df-8490-47fc-8e9c-6b8cbcb5fb6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 10. Configure Column Sorting for Improved User Experience\n",
    "\n",
    "Sets logical column sorting on date table columns to ensure proper chronological ordering in visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468514f8-1e73-4732-a22f-b0ce5f3c8eea",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "tom = labs.tom.TOMWrapper(dataset=SemanticModelName, workspace=workspaceName, readonly=False)\n",
    "tom.set_sort_by_column(table_name=\"DimDate\",column_name=\"MonthName\"       ,sort_by_column=\"MonthNumberOfYear\")\n",
    "tom.set_sort_by_column(table_name=\"DimDate\",column_name=\"DayOfWeek\"       ,sort_by_column=\"DayNumberOfWeek\")\n",
    "tom.model.SaveChanges()\n",
    "\n",
    "i:int=0\n",
    "for t in tom.model.Tables:\n",
    "    if t.Name==\"DimDate\":\n",
    "        bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "        print(bim)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7babd-5053-4dc5-97e1-4398e67dba26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 11. Hide Fact Table Columns for Optimal User Experience\n",
    "\n",
    "Hides raw fact table columns to guide users toward proper measures and improve usability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dbe0c-7d08-4c11-963f-07b44c20f401",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "i:int=0\n",
    "for t in tom.model.Tables:\n",
    "    if t.Name in [\"FactInternetSales\"]:\n",
    "        for c in t.Columns:\n",
    "            c.IsHidden=True\n",
    "\n",
    "        bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "        print(bim)\n",
    "    i=i+1\n",
    "    \n",
    "tom.model.SaveChanges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc341d-8682-4a4d-8b00-300d8723bcfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 12. Refresh Model and Apply All Configuration Changes\n",
    "\n",
    "Refreshes semantic model to apply all configuration changes with robust retry logic for production readiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed6756-a8b0-4250-9062-3266ae563054",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "reframeOK:bool=False\n",
    "while not reframeOK:\n",
    "    try:\n",
    "        result:pandas.DataFrame = labs.refresh_semantic_model(dataset=SemanticModelName)\n",
    "        reframeOK=True\n",
    "    except:\n",
    "        print('Error with reframe... trying again.')\n",
    "        triggerMetadataRefresh()\n",
    "        time.sleep(3)\n",
    "\n",
    "print('Custom Semantic Model reframe OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ccce3",
   "metadata": {},
   "source": [
    "## 13. Setup DMV Monitoring Function for Direct Lake Performance\n",
    "\n",
    "Creates monitoring function to track Direct Lake column temperature and memory usage using DMV queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab34d2-784f-4a4a-bf61-afdabb4c2b69",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066ff6f-7914-447d-8c02-52840649358c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 14. Explore Direct Lake Capabilities with DAX Functions\n",
    "\n",
    "Uses TABLETRAITS() and guardrails functions to validate Direct Lake configuration and performance limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf138f4-df36-41c4-bb58-a6af71edd179",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=sempy.fabric.evaluate_dax(\n",
    "    dataset=SemanticModelName, \n",
    "    dax_string=\"\"\"\n",
    "    \n",
    "    evaluate tabletraits()\n",
    "    \n",
    "    \"\"\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3394566-f8a8-4ee3-885b-766db5e8a615",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=labs.directlake.get_direct_lake_guardrails()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae67a8-b41c-4c18-8898-aaf4e478e599",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 15. Establish Performance Baseline with DMV Analysis\n",
    "\n",
    "Captures initial Direct Lake column states and memory usage to establish performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223acea6-6ac3-4f3a-b04d-603309daf706",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2dcd3-fe61-4466-b7dd-f746bac1e05a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 16. Execute DAX Query and Monitor Column Loading\n",
    "\n",
    "Executes DAX query and monitors which columns get loaded into memory using DMV analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbd807-3013-40a4-b584-fda3428ab4be",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "labs.clear_cache(SemanticModelName)\n",
    "\n",
    "df=sempy.fabric.evaluate_dax(\n",
    "    dataset=SemanticModelName, \n",
    "    dax_string=\"\"\"\n",
    "    \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "               \n",
    "                DimDate[MonthName] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(FactInternetSales) ,\n",
    "                \"Sum of Sales\" , [Sum of Sales] \n",
    "        )\n",
    "        ORDER BY [MonthName]\n",
    "    \"\"\")\n",
    "display(df)\n",
    "\n",
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aec05f",
   "metadata": {},
   "source": [
    "## 17. Clean Up Resources and Session Conclusion\n",
    "\n",
    "### Workshop Summary 🎉\n",
    "Congratulations! You have successfully completed Lab 1 and built a comprehensive Direct Lake semantic model. Here's what you accomplished:\n",
    "\n",
    "#### ✅ **Infrastructure Setup**\n",
    "- Created a lakehouse with proper configuration\n",
    "- Loaded Adventure Works sample data (4 tables, 80K+ rows)\n",
    "- Configured metadata synchronization\n",
    "\n",
    "#### ✅ **Model Development**  \n",
    "- Built a Direct Lake semantic model from lakehouse tables\n",
    "- Established star schema relationships (3 relationships)\n",
    "- Created business measures with proper DAX and formatting\n",
    "\n",
    "#### ✅ **User Experience Optimization**\n",
    "- Configured date table for time intelligence\n",
    "- Set logical column sorting for better visuals\n",
    "- Optimized column visibility for end users\n",
    "\n",
    "#### ✅ **Performance Analysis**\n",
    "- Implemented DMV monitoring for performance insights\n",
    "- Analyzed query execution impact on memory usage  \n",
    "- Established baseline and post-query performance comparison\n",
    "\n",
    "### Key Direct Lake Concepts Learned\n",
    "\n",
    "#### 🔄 **Real-time Analytics**\n",
    "Your model provides immediate access to lakehouse data without import delays or scheduled refreshes.\n",
    "\n",
    "#### ⚡ **Intelligent Memory Management**\n",
    "Direct Lake automatically loads only the columns needed for your queries, optimizing both performance and resource usage.\n",
    "\n",
    "#### 📊 **Enterprise-Ready Design**\n",
    "The star schema design with proper relationships, measures, and formatting provides a foundation for scalable business intelligence.\n",
    "\n",
    "### Next Steps in Your Direct Lake Journey\n",
    "\n",
    "#### 🚀 **Immediate Actions**:\n",
    "- Explore the model in Power BI Desktop or Fabric\n",
    "- Create reports using the measures and relationships you built\n",
    "- Experiment with different DAX queries to see performance patterns\n",
    "\n",
    "#### 📈 **Advanced Learning**:\n",
    "- **Lab 2**: Scale to larger datasets and understand big data scenarios\n",
    "- **Lab 3**: Analyze Delta table structure and optimization\n",
    "- **Lab 4**: Explore fallback behaviors and troubleshooting\n",
    "\n",
    "#### 🛠️ **Production Considerations**:\n",
    "- Security and access control for lakehouse data\n",
    "- Monitoring and alerting for model performance\n",
    "- Governance and lifecycle management\n",
    "\n",
    "### Resource Cleanup Importance\n",
    "The following command stops the Spark session to:\n",
    "- **💰 Save costs**: Release compute resources\n",
    "- **🧹 Clean memory**: Free up cluster resources for other users\n",
    "- **✅ Best practice**: Proper session management in Fabric notebooks\n",
    "\n",
    "### Final Thoughts\n",
    "Direct Lake represents a paradigm shift in analytics, providing the **real-time capabilities of DirectQuery** with the **performance benefits of Import mode**. You now have hands-on experience with this powerful technology!\n",
    "\n",
    "🎯 **Ready for the next lab?** Let's explore Direct Lake with big data scenarios!\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Summary\n",
    "\n",
    "### What You Accomplished\n",
    "In this lab, you successfully built a complete Direct Lake semantic model from scratch:\n",
    "\n",
    "- ✅ **Infrastructure Setup**: Created lakehouse and loaded Adventure Works data\n",
    "- ✅ **Model Creation**: Generated semantic model with automatic table discovery\n",
    "- ✅ **Data Modeling**: Established star schema relationships between fact and dimensions\n",
    "- ✅ **Business Logic**: Added essential DAX measures with proper formatting\n",
    "- ✅ **User Experience**: Configured date tables, column sorting, and visibility\n",
    "- ✅ **Performance Validation**: Tested model with business queries and DMV analysis\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "**End-to-End Direct Lake Flow:**\n",
    "```\n",
    "Adventure Works Data → Lakehouse (Delta Tables) → Direct Lake Model → Real-time Analytics\n",
    "        ↓                    ↓                         ↓                    ↓\n",
    "   CSV/Parquet         Delta Format            Semantic Layer        Power BI Reports\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Direct Lake Advantage**: Real-time data access without imports or scheduled refreshes\n",
    "- **Star Schema Power**: Proper relationships enable accurate cross-table analysis\n",
    "- **DAX Measures**: Essential for business metrics - don't rely on raw column values\n",
    "- **User Experience**: Column sorting and hiding improve report usability\n",
    "- **Performance Monitoring**: DMVs provide insights into memory usage and query patterns\n",
    "\n",
    "### Performance Results\n",
    "\n",
    "- **Data Freshness**: Real-time updates as soon as lakehouse data changes\n",
    "- **Query Performance**: Excellent response times with columnar Direct Lake access\n",
    "- **Memory Efficiency**: Only accessed columns loaded into memory (\"column temperature\")\n",
    "- **Resource Optimization**: Minimal compute overhead compared to import models\n",
    "\n",
    "### Technical Skills Gained\n",
    "\n",
    "- **Semantic Link Labs**: Programmatic model creation and management\n",
    "- **TOM (Tabular Object Model)**: Advanced model configuration capabilities\n",
    "- **DMV Analysis**: Understanding Direct Lake memory and performance patterns\n",
    "- **Error Handling**: Robust retry logic for production-ready deployments\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Continue to Lab 2** to learn about:\n",
    "- Working with billion-row datasets\n",
    "- OneLake shortcuts for cross-workspace data access\n",
    "- Direct Lake guardrails and fallback behavior\n",
    "- Advanced performance monitoring for big data scenarios\n",
    "\n",
    "**For Production Deployment:**\n",
    "- Implement proper security and access controls\n",
    "- Set up monitoring and alerting for model performance\n",
    "- Establish governance and lifecycle management processes\n",
    "- Consider refresh automation for supporting data pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
