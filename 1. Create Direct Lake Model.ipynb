{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09b6c60-d142-42f2-aa77-b714234d69d3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 1: Create Direct Lake Semantic Model\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "This lab teaches you how to create a **Direct Lake semantic model** from scratch using Microsoft Fabric. You'll learn the complete workflow from data loading to model creation and validation.\n",
    "\n",
    "### What You'll Build\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Lakehouse] --> B[Load Adventure Works Data]\n",
    "    B --> C[Create Semantic Model]\n",
    "    C --> D[Add Relationships]\n",
    "    D --> E[Create Measures]\n",
    "    E --> F[Test & Validate]\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "- **Direct Lake**: Query data directly from Delta Lake without imports\n",
    "- **Adventure Works**: Sample business dataset with customers, products, and sales\n",
    "- **Semantic Model**: Business logic layer with relationships and measures\n",
    "\n",
    "### Learning Objectives\n",
    "By completing this lab, you'll be able to:\n",
    "- ‚úÖ Set up a lakehouse and load sample data\n",
    "- ‚úÖ Create a Direct Lake semantic model programmatically  \n",
    "- ‚úÖ Define table relationships and business measures\n",
    "- ‚úÖ Validate model performance and behavior\n",
    "\n",
    "**Estimated Time**: 30-45 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd099e87-89f6-45b2-b286-d7f8a12b6a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "Install Semantic Link Labs to enable Direct Lake model creation and management capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ed5ca-a6ba-478a-b029-17b6db9b6308",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q --disable-pip-version-check semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc44ce-395c-4db3-8979-b06acf9f8ecf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2. Import Libraries and Set Variables\n",
    "\n",
    "Import required libraries and define key variables for the lakehouse and semantic model names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e841d6-a757-4029-aa71-88d4bd286c30",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as labs\n",
    "from sempy import fabric\n",
    "import sempy\n",
    "import pandas\n",
    "import json\n",
    "import time\n",
    "\n",
    "LakehouseName = \"AdventureWorks\"\n",
    "SemanticModelName = f\"{LakehouseName}_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99f94c-a564-4aca-b670-31da354b7b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3. Create or Connect to Lakehouse\n",
    "\n",
    "Check if the AdventureWorks lakehouse exists, create it if needed, and retrieve workspace identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760df625-543c-4289-b6cc-f043290d5879",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "lakehouses=labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "if LakehouseName in lakehouses.values:\n",
    "    lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]\n",
    "else:\n",
    "    lakehouseId = fabric.create_lakehouse(LakehouseName)\n",
    "\n",
    "workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "workspaceName = sempy.fabric.resolve_workspace_name(workspaceId)\n",
    "print(f\"WorkspaceId = {workspaceId}, LakehouseID = {lakehouseId}, Workspace Name = {workspaceName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6e83e-d5fd-4020-9766-c236ef176329",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Load Adventure Works Sample Data\n",
    "\n",
    "Load four Adventure Works tables (Customer, Date, Product, Sales) into the lakehouse using region-aware data sources.\n",
    "\n",
    "**Tables being loaded:**\n",
    "- DimCustomer (~18K customers)  \n",
    "- DimDate (2K+ dates)\n",
    "- DimProduct (~600 products)\n",
    "- FactInternetSales (~60K sales records)\n",
    "```\n",
    "Loaded DimCustomer\n",
    "Loaded DimDate  \n",
    "Loaded DimProduct\n",
    "Loaded FactInternetSales\n",
    "Done\n",
    "```\n",
    "\n",
    "### Behind the Scenes\n",
    "- Data is stored in **Delta format** for ACID compliance\n",
    "- **Overwrite mode** ensures clean data for the workshop\n",
    "- **OneLake integration** provides seamless cross-workspace data access\n",
    "\n",
    "üéØ **Success indicator**: All four \"Loaded\" messages followed by \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28505f17-8bf7-4400-a409-344c4d01b4ef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "capacity_name = labs.get_capacity_name()\n",
    "\n",
    "def loadDataToLakehouse(fromTable: str, toTable: str):\n",
    "    \"\"\"\n",
    "    Optimized data loading function with improved error handling and performance.\n",
    "    \n",
    "    Args:\n",
    "        fromTable: Source table name to read from\n",
    "        toTable: Target table name to write to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get lakehouse properties once and reuse\n",
    "        lakehouse_props = notebookutils.lakehouse.getWithProperties(LakehouseName)\n",
    "        workspaceId = lakehouse_props[\"workspaceId\"]\n",
    "        lakehouseId = lakehouse_props[\"id\"]\n",
    "\n",
    "        # Region-aware connection string selection\n",
    "        if capacity_name == \"FabConUS8-P1\":  # West US 3\n",
    "            conn_str = \"abfss://b1d61bbe-de20-4d3a-8075-b8e2eaacb868@onelake.dfs.fabric.microsoft.com/631e45c0-1243-4f42-920a-56bfe6ecdd6d/Tables\"\n",
    "        else:  # North Central US (default)\n",
    "            conn_str = \"abfss://16cf855f-3bf4-4312-a7a1-ccf5cb6a0121@onelake.dfs.fabric.microsoft.com/99ed86df-13d1-4008-a7f6-5768e53f4f85/Tables\"\n",
    "\n",
    "        # Read source data with format specification for better performance\n",
    "        customer_df = spark.read.format(\"delta\").load(f\"{conn_str}/{fromTable}\")\n",
    "        \n",
    "        # Cache the DataFrame if it will be used multiple times or is computation-heavy\n",
    "        customer_df.cache()\n",
    "        \n",
    "        # Write with optimized settings\n",
    "        (customer_df\n",
    "         .write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .save(f\"abfss://{workspaceId}@onelake.dfs.fabric.microsoft.com/{lakehouseId}/Tables/{toTable}\"))\n",
    "        \n",
    "        # Unpersist cached DataFrame to free memory\n",
    "        customer_df.unpersist()\n",
    "        \n",
    "        print(f\"Loaded {toTable}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {toTable}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load all tables with proper error handling\n",
    "tables_to_load = [\n",
    "    (\"DimCustomer\", \"DimCustomer\"),\n",
    "    (\"DimDate\", \"DimDate\"),\n",
    "    (\"DimProduct\", \"DimProduct\"),\n",
    "    (\"FactInternetSales\", \"FactInternetSales\")\n",
    "]\n",
    "\n",
    "for from_table, to_table in tables_to_load:\n",
    "    loadDataToLakehouse(from_table, to_table)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff2996-0b1c-4792-99cd-8d61315e65da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 5. Trigger Lakehouse Metadata Synchronization\n",
    "\n",
    "### Understanding the Challenge\n",
    "When data is loaded into a lakehouse, there can be a **delay** before the SQL Analytics Endpoint recognizes the new table schemas. This is because:\n",
    "\n",
    "- **Lakehouse storage** and **SQL endpoint** operate on different systems\n",
    "- **Schema discovery** happens asynchronously in the background\n",
    "- **Delta table metadata** needs to be synchronized across services\n",
    "\n",
    "### What This Code Solves\n",
    "The `triggerMetadataRefresh` function forces immediate synchronization by:\n",
    "\n",
    "#### Step-by-Step Process:\n",
    "1. **üîó Creates API client**: Establishes connection to Fabric REST APIs\n",
    "2. **üîç Finds SQL endpoint**: Retrieves the SQL Analytics Endpoint ID for our lakehouse\n",
    "3. **üöÄ Triggers refresh**: Sends a metadata refresh command to synchronize schemas\n",
    "4. **‚è±Ô∏è Monitors progress**: Polls the refresh status until completion\n",
    "5. **‚úÖ Confirms success**: Reports when synchronization is complete\n",
    "\n",
    "### Technical Details\n",
    "- **API endpoint**: Uses Fabric's REST API for programmatic control\n",
    "- **Batch processing**: Operations are queued and tracked with batch IDs\n",
    "- **Progress states**: Monitors transitions from \"running\" ‚Üí \"success\"\n",
    "- **Error handling**: Built-in retry logic for robust execution\n",
    "\n",
    "### Expected Output\n",
    "You'll see real-time progress updates:\n",
    "```\n",
    "Metadata refresh : running\n",
    "Metadata refresh : running  \n",
    "Metadata refresh : success\n",
    "Metadata refresh complete\n",
    "```\n",
    "\n",
    "### Why This Matters for Direct Lake\n",
    "- **Schema accuracy**: Ensures Direct Lake models can read current table structures\n",
    "- **Column metadata**: Synchronizes data types, constraints, and relationships\n",
    "- **Performance optimization**: Enables proper query planning and execution\n",
    "\n",
    "üéØ **Critical for success**: This step ensures your semantic model will have accurate table metadata!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54e1aa-a94e-4efd-b40e-f497f54589e8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "##https://medium.com/@sqltidy/delays-in-the-automatically-generated-schema-in-the-sql-analytics-endpoint-of-the-lakehouse-b01c7633035d\n",
    "\n",
    "def triggerMetadataRefresh():\n",
    "    client = fabric.FabricRestClient()\n",
    "    response = client.get(f\"/v1/workspaces/{workspaceId}/lakehouses/{lakehouseId}\")\n",
    "    sqlendpoint = response.json()['properties']['sqlEndpointProperties']['id']\n",
    "\n",
    "    # trigger sync\n",
    "    uri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}\"\n",
    "    payload = {\"commands\":[{\"$type\":\"MetadataRefreshExternalCommand\"}]}\n",
    "    response = client.post(uri,json= payload)\n",
    "    batchId = response.json()['batchId']\n",
    "\n",
    "    # Monitor Progress\n",
    "    statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "    statusresponsedata = client.get(statusuri).json()\n",
    "    progressState = statusresponsedata['progressState']\n",
    "    print(f\"Metadata refresh : {progressState}\")\n",
    "    while progressState != \"success\":\n",
    "        statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "        statusresponsedata = client.get(statusuri).json()\n",
    "        progressState = statusresponsedata['progressState']\n",
    "        print(f\"Metadata refresh : {progressState}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    print('Metadata refresh complete')\n",
    "\n",
    "triggerMetadataRefresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e16b1-9893-46d3-ac7d-93a4d1453c5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 6. Create Direct Lake Semantic Model\n",
    "\n",
    "### Understanding Semantic Models\n",
    "A **semantic model** in Microsoft Fabric is the bridge between raw data and business insights. It provides:\n",
    "\n",
    "- **üìä Business context**: Meaningful names, descriptions, and formatting\n",
    "- **üîó Relationships**: How tables connect for accurate cross-table analysis  \n",
    "- **üìà Measures**: Pre-calculated business metrics using DAX\n",
    "- **üéØ User experience**: Optimized for self-service analytics\n",
    "\n",
    "### Direct Lake vs. Other Storage Modes\n",
    "\n",
    "| Storage Mode | Data Location | Performance | Real-time | Memory Usage |\n",
    "|--------------|---------------|-------------|-----------|--------------|\n",
    "| **Direct Lake** | Lakehouse Delta tables | Excellent | ‚úÖ Real-time | Minimal |\n",
    "| Import | Semantic model cache | Fast | ‚ùå Scheduled refresh | High |\n",
    "| DirectQuery | Source system | Variable | ‚úÖ Real-time | Minimal |\n",
    "\n",
    "### Code Walkthrough\n",
    "\n",
    "#### 1. **Table Discovery**\n",
    "```python\n",
    "lakehouseTables = labs.lakehouse.get_lakehouse_tables(lakehouse=LakehouseName)[\"Table Name\"]\n",
    "```\n",
    "- Automatically discovers all tables in our lakehouse\n",
    "- Creates a list of table names for model inclusion\n",
    "\n",
    "#### 2. **Duplicate Check**\n",
    "```python\n",
    "if sempy.fabric.list_items().query(f\"`Display Name`=='{LakehouseName}_model'...\").shape[0] ==0:\n",
    "```\n",
    "- Prevents creating duplicate models with the same name\n",
    "- Uses pandas query syntax for filtering\n",
    "\n",
    "#### 3. **Model Generation**\n",
    "```python\n",
    "labs.directlake.generate_direct_lake_semantic_model(...)\n",
    "```\n",
    "- **`dataset`**: Name of the semantic model to create\n",
    "- **`lakehouse_tables`**: List of tables to include\n",
    "- **`workspace`**: Target workspace for the model\n",
    "- **`lakehouse`**: Source lakehouse ID\n",
    "- **`refresh=False`**: Don't trigger refresh during creation\n",
    "- **`overwrite=True`**: Replace existing model if found\n",
    "\n",
    "#### 4. **Error Handling**\n",
    "The code includes retry logic because:\n",
    "- **Concurrent operations**: Multiple users might access the same resources\n",
    "- **Metadata dependencies**: Tables must be fully synchronized\n",
    "- **API rate limiting**: Fabric services may need brief delays\n",
    "\n",
    "### Expected Outcome\n",
    "- ‚úÖ **New semantic model** created in your workspace\n",
    "- ‚úÖ **All four tables** automatically added with proper Direct Lake configuration\n",
    "- ‚úÖ **Ready for customization** with relationships, measures, and formatting\n",
    "\n",
    "üéØ **Success indicator**: \"Semantic model created OK\" message confirms your model is ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e9cc7-de53-4c81-8408-9fb18ef4383a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sempy import fabric\n",
    "\n",
    "#1. Generate list of ALL table names from lakehouse to add to Semantic Model\n",
    "lakehouseTables:list = labs.lakehouse.get_lakehouse_tables(lakehouse=LakehouseName)[\"Table Name\"]\n",
    "\n",
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        #2 Create the semantic model\n",
    "        if sempy.fabric.list_items().query(f\"`Display Name`=='{LakehouseName}_model' & Type=='SemanticModel'  \").shape[0] ==0:\n",
    "            labs.directlake.generate_direct_lake_semantic_model(dataset=f\"{LakehouseName}_model\",lakehouse_tables=lakehouseTables,workspace=workspaceName,lakehouse=lakehouseId,refresh=False,overwrite=True)\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error creating model... trying again.')\n",
    "        time.sleep(3)\n",
    "        triggerMetadataRefresh()\n",
    "\n",
    "print('Semantic model created OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df3373-52ca-4e45-83a6-40e1b20c184d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 7. Configure Table Relationships\n",
    "\n",
    "### Why Relationships Matter in Data Modeling\n",
    "**Relationships** are the foundation of accurate business intelligence. They define how tables connect and enable:\n",
    "\n",
    "- **üîç Cross-table filtering**: When you filter customers, see their related sales\n",
    "- **üìä Accurate aggregations**: Prevent double-counting and incorrect totals\n",
    "- **üß≠ Intuitive navigation**: Users can explore data naturally across dimensions\n",
    "- **‚ö° Optimized queries**: Fabric can optimize query execution paths\n",
    "\n",
    "### Star Schema Design Pattern\n",
    "Our Adventure Works model follows the **star schema** pattern:\n",
    "\n",
    "```\n",
    "       DimCustomer ‚îÄ‚îÄ‚îê\n",
    "                     ‚îÇ\n",
    "       DimDate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ FactInternetSales (CENTER)\n",
    "                     ‚îÇ\n",
    "       DimProduct ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Relationship Configuration Details\n",
    "\n",
    "#### 1. **Date Relationship**\n",
    "```python\n",
    "FactInternetSales.OrderDateKey ‚Üí DimDate.DateKey (Many-to-One)\n",
    "```\n",
    "- **Business meaning**: Each sale happens on one specific date\n",
    "- **Cardinality**: Many sales can occur on the same date\n",
    "- **Enables**: Time-based analysis (monthly/yearly trends)\n",
    "\n",
    "#### 2. **Customer Relationship** \n",
    "```python\n",
    "FactInternetSales.CustomerKey ‚Üí DimCustomer.CustomerKey (Many-to-One)\n",
    "```\n",
    "- **Business meaning**: Each sale belongs to one customer\n",
    "- **Cardinality**: Customers can have multiple sales\n",
    "- **Enables**: Customer segmentation and analysis\n",
    "\n",
    "#### 3. **Product Relationship**\n",
    "```python\n",
    "FactInternetSales.ProductKey ‚Üí DimProduct.ProductKey (Many-to-One)\n",
    "```\n",
    "- **Business meaning**: Each sale involves one product\n",
    "- **Cardinality**: Products can be sold multiple times\n",
    "- **Enables**: Product performance analysis\n",
    "\n",
    "### Technical Implementation\n",
    "\n",
    "#### Clean Slate Approach\n",
    "```python\n",
    "for r in tom.model.Relationships:\n",
    "    tom.model.Relationships.Remove(r)\n",
    "```\n",
    "- Removes any existing relationships to ensure clean configuration\n",
    "- Prevents conflicts from previous model iterations\n",
    "\n",
    "#### TOM (Tabular Object Model) Connection\n",
    "- **`readonly=False`**: Enables write operations on the model\n",
    "- **Context manager**: Automatically handles connection cleanup\n",
    "- **Transaction safety**: Changes are applied atomically\n",
    "\n",
    "### Expected Outcome\n",
    "- ‚úÖ **Three relationships** established following star schema best practices\n",
    "- ‚úÖ **Proper cardinality** configured for accurate data analysis\n",
    "- ‚úÖ **Foundation ready** for meaningful cross-table queries\n",
    "\n",
    "üéØ **Data modeling checkpoint**: Your model now understands how tables connect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616193a-57df-4139-91ef-c73830331555",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing relationships\n",
    "            for r in tom.model.Relationships:\n",
    "                tom.model.Relationships.Remove(r)\n",
    "\n",
    "            #2. Creates correct relationships\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"OrderDateKey\" , to_table=\"DimDate\"    , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"CustomerKey\"  , to_table=\"DimCustomer\", to_column=\"CustomerKey\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"ProductKey\"   , to_table=\"DimProduct\" , to_column=\"ProductKey\"    , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding relationships... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba5536-018c-402b-aa0f-663ee5e8d07f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 8. Add Business Intelligence Measures\n",
    "\n",
    "### Understanding DAX Measures\n",
    "**Measures** are the calculated fields that provide business insights. Unlike calculated columns, measures:\n",
    "\n",
    "- **üîÑ Calculate dynamically**: Values change based on filter context\n",
    "- **üìä Aggregate data**: Sum, count, average across filtered datasets  \n",
    "- **üí∞ Represent KPIs**: Revenue, growth rates, conversion metrics\n",
    "- **üéØ Guide decisions**: Turn raw data into actionable insights\n",
    "\n",
    "### Measure Design Principles\n",
    "\n",
    "#### 1. **Measure Cleanup Strategy**\n",
    "```python\n",
    "for m in t.Measures:\n",
    "    tom.remove_object(m)\n",
    "```\n",
    "- **Clean slate approach**: Removes existing measures to avoid conflicts\n",
    "- **Iterative development**: Allows for multiple runs during development\n",
    "- **Consistency**: Ensures predictable measure configuration\n",
    "\n",
    "#### 2. **Business Metrics Implementation**\n",
    "\n",
    "##### üìà **Sum of Sales**\n",
    "```dax\n",
    "SUM(FactInternetSales[SalesAmount])\n",
    "```\n",
    "- **Purpose**: Total revenue across filtered context\n",
    "- **Format**: Currency with thousands separators `$#,0.###############`\n",
    "- **Business use**: Revenue reporting, target tracking\n",
    "\n",
    "##### üìä **Count of Sales** \n",
    "```dax\n",
    "COUNTROWS(FactInternetSales)\n",
    "```\n",
    "- **Purpose**: Number of transactions\n",
    "- **Format**: Integer with thousands separators `#,0`\n",
    "- **Business use**: Volume analysis, conversion tracking\n",
    "\n",
    "### DAX Function Deep Dive\n",
    "\n",
    "#### SUM() vs SUMX()\n",
    "- **`SUM()`**: Aggregates a single column efficiently\n",
    "- **`SUMX()`**: Row-by-row iteration (use when calculations needed per row)\n",
    "\n",
    "#### COUNTROWS() vs COUNT()\n",
    "- **`COUNTROWS()`**: Counts all rows (including blanks)\n",
    "- **`COUNT()`**: Counts non-blank values in a specific column\n",
    "\n",
    "### Formatting Standards\n",
    "| Format String | Example Output | Use Case |\n",
    "|---------------|----------------|----------|\n",
    "| `$#,0.##` | $1,234.56 | Currency values |\n",
    "| `#,0` | 1,234 | Whole numbers |\n",
    "| `0.00%` | 12.34% | Percentages |\n",
    "\n",
    "### Expected Outcome\n",
    "- ‚úÖ **Sum of Sales**: Currency-formatted revenue measure\n",
    "- ‚úÖ **Count of Sales**: Integer-formatted transaction count  \n",
    "- ‚úÖ **Clean implementation**: No duplicate or conflicting measures\n",
    "- ‚úÖ **Ready for reporting**: Measures available in visualization tools\n",
    "\n",
    "üéØ **Business intelligence checkpoint**: Your model now calculates key business metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666631c-4221-4638-b8ff-10fee8a4f7df",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing measures\n",
    "            for t in tom.model.Tables:\n",
    "                for m in t.Measures:\n",
    "                    tom.remove_object(m)\n",
    "                    print(f\"[{m.Name}] measure removed\")\n",
    "\n",
    "            tom.add_measure(table_name=\"FactInternetSales\" ,measure_name=\"Sum of Sales\",expression=\"SUM(FactInternetSales[SalesAmount])\",format_string=\"\\$#,0.###############;(\\$#,0.###############);\\$#,0.###############\")\n",
    "            tom.add_measure(table_name=\"FactInternetSales\" ,measure_name=\"Count of Sales\",expression=\"COUNTROWS(FactInternetSales)\",format_string=\"#,0\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding measures... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ff91f-0362-4c3e-bf2e-60c64867cec8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 9. Configure Date Table for Time Intelligence\n",
    "\n",
    "### Why Mark a Date Table?\n",
    "**Date tables** are special in business intelligence because they enable **time intelligence** functions. Marking a table as a date table:\n",
    "\n",
    "- **üïê Enables DAX time functions**: TOTALYTD, SAMEPERIODLASTYEAR, DATESADD\n",
    "- **üìÖ Improves auto-formatting**: Dates display properly in visuals\n",
    "- **üîÑ Supports relative filtering**: \"Last 30 days\", \"This quarter\", \"Year over year\"\n",
    "- **‚ö° Optimizes performance**: Fabric optimizes queries involving date operations\n",
    "\n",
    "### Date Table Requirements\n",
    "For a table to be marked as a date table, it must have:\n",
    "\n",
    "- ‚úÖ **Unique date column**: No duplicate dates\n",
    "- ‚úÖ **Continuous date range**: No gaps in the date sequence\n",
    "- ‚úÖ **Date data type**: Proper datetime or date column type\n",
    "- ‚úÖ **Complete coverage**: Spans the full range of fact table dates\n",
    "\n",
    "### Our DimDate Table Structure\n",
    "The Adventure Works DimDate table includes:\n",
    "\n",
    "| Column | Purpose | Example |\n",
    "|--------|---------|---------|\n",
    "| **Date** | Primary date column | 2023-01-01 |\n",
    "| DateKey | Integer date key | 20230101 |\n",
    "| MonthName | Month display name | January |\n",
    "| MonthNumberOfYear | Month number | 1 |\n",
    "| DayOfWeek | Weekday name | Sunday |\n",
    "| DayNumberOfWeek | Weekday number | 1 |\n",
    "\n",
    "### Technical Implementation\n",
    "```python\n",
    "tom.mark_as_date_table(table_name=\"DimDate\", column_name=\"Date\")\n",
    "```\n",
    "\n",
    "#### Parameters Explained:\n",
    "- **`table_name=\"DimDate\"`**: The table containing our date dimension\n",
    "- **`column_name=\"Date\"`**: The primary date column (must be Date/DateTime type)\n",
    "\n",
    "### What Happens Behind the Scenes\n",
    "When you mark a date table, Fabric:\n",
    "\n",
    "1. **üîç Validates structure**: Checks for unique, continuous dates\n",
    "2. **üè∑Ô∏è Sets metadata**: Marks the table with special date table properties\n",
    "3. **‚ö° Optimizes storage**: Enables date-specific compression and indexing\n",
    "4. **üîß Enables functions**: Unlocks DAX time intelligence capabilities\n",
    "\n",
    "### Time Intelligence Unlocked! üöÄ\n",
    "Once marked, you can create measures like:\n",
    "```dax\n",
    "Sales YTD = TOTALYTD([Sum of Sales], DimDate[Date])\n",
    "Sales Last Year = CALCULATE([Sum of Sales], SAMEPERIODLASTYEAR(DimDate[Date]))\n",
    "Sales Growth = [Sum of Sales] - [Sales Last Year]\n",
    "```\n",
    "\n",
    "### Expected Outcome\n",
    "- ‚úÖ **DimDate marked** as official date table\n",
    "- ‚úÖ **Time intelligence enabled** for DAX calculations\n",
    "- ‚úÖ **Foundation ready** for advanced date-based analysis\n",
    "\n",
    "üéØ **Time intelligence checkpoint**: Your model now supports sophisticated date calculations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f48bf-893b-4a8f-92a1-e5e80b968840",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            tom.mark_as_date_table(table_name=\"DimDate\",column_name=\"Date\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error with date table... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb148df-8490-47fc-8e9c-6b8cbcb5fb6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 10. Configure Column Sorting for Better User Experience\n",
    "\n",
    "### The Sorting Challenge in BI\n",
    "When users create reports, they expect logical sorting behavior:\n",
    "\n",
    "- **‚ùå Default alphabetical**: \"April, August, December, February, January...\"\n",
    "- **‚úÖ Business logical**: \"January, February, March, April, May...\"\n",
    "\n",
    "Without proper sorting configuration, month names sort alphabetically, which confuses users and makes reports hard to interpret.\n",
    "\n",
    "### Sort By Column Concept\n",
    "**Sort by column** allows you to sort one column using the values from another column:\n",
    "\n",
    "| Display Column | Sort By Column | Why? |\n",
    "|----------------|----------------|------|\n",
    "| MonthName | MonthNumberOfYear | Sort \"January\" by 1, \"February\" by 2 |\n",
    "| DayOfWeek | DayNumberOfWeek | Sort \"Sunday\" by 1, \"Monday\" by 2 |\n",
    "| Product Size | SizeOrder | Sort \"Small\" by 1, \"Medium\" by 2, \"Large\" by 3 |\n",
    "\n",
    "### Our Configuration\n",
    "\n",
    "#### 1. **Month Sorting**\n",
    "```python\n",
    "tom.set_sort_by_column(table_name=\"DimDate\", column_name=\"MonthName\", sort_by_column=\"MonthNumberOfYear\")\n",
    "```\n",
    "- **Display**: \"January\", \"February\", \"March\"...\n",
    "- **Sort by**: 1, 2, 3...\n",
    "- **Result**: Chronological month order in visuals\n",
    "\n",
    "#### 2. **Weekday Sorting**\n",
    "```python  \n",
    "tom.set_sort_by_column(table_name=\"DimDate\", column_name=\"DayOfWeek\", sort_by_column=\"DayNumberOfWeek\")\n",
    "```\n",
    "- **Display**: \"Sunday\", \"Monday\", \"Tuesday\"...\n",
    "- **Sort by**: 1, 2, 3...\n",
    "- **Result**: Logical weekday progression\n",
    "\n",
    "### Technical Implementation Details\n",
    "\n",
    "#### JSON Output Inspection\n",
    "The code displays the table's JSON structure to verify:\n",
    "- ‚úÖ **Sort by relationships** are properly configured\n",
    "- ‚úÖ **Column metadata** is correctly set\n",
    "- ‚úÖ **Model structure** matches expectations\n",
    "\n",
    "#### BIM (Business Intelligence Model) Format\n",
    "The output shows the table definition in BIM format, which includes:\n",
    "- Column definitions and data types\n",
    "- Sort by column relationships  \n",
    "- Display formatting rules\n",
    "- Performance optimization settings\n",
    "\n",
    "### User Experience Impact\n",
    "\n",
    "#### Before Sort Configuration:\n",
    "```\n",
    "Chart shows: Apr, Aug, Dec, Feb, Jan, Jul, Jun...\n",
    "Users think: \"This makes no sense!\"\n",
    "```\n",
    "\n",
    "#### After Sort Configuration:\n",
    "```\n",
    "Chart shows: Jan, Feb, Mar, Apr, May, Jun, Jul...\n",
    "Users think: \"Perfect! This is what I expected.\"\n",
    "```\n",
    "\n",
    "### Expected Outcome\n",
    "- ‚úÖ **MonthName column** sorts chronologically (Jan ‚Üí Dec)\n",
    "- ‚úÖ **DayOfWeek column** sorts logically (Sun ‚Üí Sat) \n",
    "- ‚úÖ **JSON structure** displayed for verification\n",
    "- ‚úÖ **Enhanced UX** for report consumers\n",
    "\n",
    "üéØ **User experience checkpoint**: Your model now provides intuitive sorting behavior!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468514f8-1e73-4732-a22f-b0ce5f3c8eea",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "tom = labs.tom.TOMWrapper(dataset=SemanticModelName, workspace=workspaceName, readonly=False)\n",
    "tom.set_sort_by_column(table_name=\"DimDate\",column_name=\"MonthName\"       ,sort_by_column=\"MonthNumberOfYear\")\n",
    "tom.set_sort_by_column(table_name=\"DimDate\",column_name=\"DayOfWeek\"       ,sort_by_column=\"DayNumberOfWeek\")\n",
    "tom.model.SaveChanges()\n",
    "\n",
    "i:int=0\n",
    "for t in tom.model.Tables:\n",
    "    if t.Name==\"DimDate\":\n",
    "        bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "        print(bim)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7babd-5053-4dc5-97e1-4398e67dba26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 11. Optimize Model by Hiding Technical Columns\n",
    "\n",
    "### Why Hide Columns in Fact Tables?\n",
    "**Fact tables** contain both business-relevant and technical columns. Hiding technical columns improves the user experience by:\n",
    "\n",
    "- **üéØ Reducing complexity**: Users see only meaningful business columns\n",
    "- **üö´ Preventing errors**: Technical keys shouldn't be used in reports directly\n",
    "- **üìä Promoting measures**: Guides users toward proper aggregated values\n",
    "- **‚ö° Improving performance**: Reduces metadata that client tools need to process\n",
    "\n",
    "### Column Visibility Strategy\n",
    "\n",
    "#### FactInternetSales Column Analysis:\n",
    "| Column Type | Examples | Visibility | Reason |\n",
    "|-------------|----------|------------|---------|\n",
    "| **Foreign Keys** | CustomerKey, ProductKey | üîí Hidden | Use relationships instead |\n",
    "| **Technical IDs** | OrderDateKey | üîí Hidden | Use Date dimension |\n",
    "| **Raw Values** | SalesAmount, Quantity | üîí Hidden | Use measures instead |\n",
    "| **Calculated Fields** | [Sum of Sales] measure | üëÅÔ∏è Visible | Proper aggregation |\n",
    "\n",
    "### The Column Hiding Process\n",
    "\n",
    "#### 1. **Iterate Through Tables**\n",
    "```python\n",
    "for t in tom.model.Tables:\n",
    "    if t.Name in [\"FactInternetSales\"]:\n",
    "```\n",
    "- Targets specific fact tables (extensible to multiple tables)\n",
    "- Preserves dimension table columns for filtering and grouping\n",
    "\n",
    "#### 2. **Hide All Columns**\n",
    "```python\n",
    "for c in t.Columns:\n",
    "    c.IsHidden = True\n",
    "```\n",
    "- Sets the `IsHidden` property to `True` for each column\n",
    "- Columns remain in the model but don't appear in field lists\n",
    "\n",
    "#### 3. **JSON Verification**\n",
    "```python\n",
    "bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i], indent=4)\n",
    "```\n",
    "- Displays the table structure for verification\n",
    "- Shows the `IsHidden` property for each column\n",
    "\n",
    "### Best Practices for Column Visibility\n",
    "\n",
    "#### Always Hide in Fact Tables:\n",
    "- ‚úÖ **Surrogate keys**: CustomerKey, ProductKey, DateKey\n",
    "- ‚úÖ **Raw numeric values**: Use measures instead\n",
    "- ‚úÖ **Technical timestamps**: Created dates, modified dates\n",
    "\n",
    "#### Keep Visible in Dimension Tables:\n",
    "- ‚úÖ **Descriptive attributes**: Customer names, product categories\n",
    "- ‚úÖ **Natural keys**: Account numbers, product codes\n",
    "- ‚úÖ **Date components**: Year, month, quarter (from date table)\n",
    "\n",
    "### User Experience Impact\n",
    "\n",
    "#### Before Hiding:\n",
    "```\n",
    "Field List shows:\n",
    "‚îú‚îÄ‚îÄ FactInternetSales\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ CustomerKey         ‚Üê Confusing\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ProductKey          ‚Üê Confusing  \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ OrderDateKey        ‚Üê Confusing\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ SalesAmount         ‚Üê Misleading\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Quantity            ‚Üê Misleading\n",
    "```\n",
    "\n",
    "#### After Hiding:\n",
    "```\n",
    "Field List shows:\n",
    "‚îú‚îÄ‚îÄ FactInternetSales\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Sum of Sales        ‚Üê Clear measure\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Count of Sales      ‚Üê Clear measure\n",
    "‚îú‚îÄ‚îÄ DimCustomer\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ CustomerName        ‚Üê Useful for grouping\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ CustomerCity        ‚Üê Useful for filtering\n",
    "```\n",
    "\n",
    "### Expected Outcome\n",
    "- ‚úÖ **All FactInternetSales columns** hidden from user interface\n",
    "- ‚úÖ **Measures remain visible** for proper aggregation\n",
    "- ‚úÖ **Dimension columns visible** for filtering and grouping\n",
    "- ‚úÖ **JSON structure** displayed showing `IsHidden: true`\n",
    "\n",
    "üéØ **Model optimization checkpoint**: Your model now guides users toward correct analysis patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dbe0c-7d08-4c11-963f-07b44c20f401",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "i:int=0\n",
    "for t in tom.model.Tables:\n",
    "    if t.Name in [\"FactInternetSales\"]:\n",
    "        for c in t.Columns:\n",
    "            c.IsHidden=True\n",
    "\n",
    "        bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "        print(bim)\n",
    "    i=i+1\n",
    "    \n",
    "tom.model.SaveChanges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc341d-8682-4a4d-8b00-300d8723bcfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 12. Refresh Semantic Model to Apply Configuration Changes\n",
    "\n",
    "### Understanding Semantic Model Refresh\n",
    "**Refreshing** a semantic model ensures that all configuration changes are properly applied and the model is ready for use. This process:\n",
    "\n",
    "- **üíæ Commits metadata changes**: Relationships, measures, column properties\n",
    "- **üîÑ Synchronizes with lakehouse**: Ensures Direct Lake connections are active\n",
    "- **‚ö° Optimizes query plans**: Updates internal structures for better performance\n",
    "- **‚úÖ Validates configuration**: Checks that all changes are compatible\n",
    "\n",
    "### Why Refresh is Critical for Direct Lake\n",
    "Direct Lake models have unique refresh requirements:\n",
    "\n",
    "#### Configuration Refresh vs. Data Refresh:\n",
    "| Refresh Type | Purpose | When Needed | Duration |\n",
    "|--------------|---------|-------------|----------|\n",
    "| **Configuration** | Apply metadata changes | After model modifications | Seconds |\n",
    "| **Data** | Update cached data | Not needed (real-time) | N/A |\n",
    "\n",
    "#### What Happens During Refresh:\n",
    "1. **üîç Validates structure**: Checks table relationships and measure definitions\n",
    "2. **üîó Tests lakehouse connections**: Ensures Direct Lake paths are accessible  \n",
    "3. **üìä Updates metadata**: Applies visibility, sorting, and formatting rules\n",
    "4. **‚ö° Optimizes performance**: Builds internal indexes and query plans\n",
    "\n",
    "### Error Handling Strategy\n",
    "\n",
    "#### Retry Logic Implementation:\n",
    "```python\n",
    "reframeOK:bool = False\n",
    "while not reframeOK:\n",
    "    try:\n",
    "        result = labs.refresh_semantic_model(dataset=SemanticModelName)\n",
    "        reframeOK = True\n",
    "    except:\n",
    "        print('Error with reframe... trying again.')\n",
    "        triggerMetadataRefresh()\n",
    "        time.sleep(3)\n",
    "```\n",
    "\n",
    "#### Why Retry Logic is Necessary:\n",
    "- **üîÑ Async operations**: Lakehouse metadata may still be syncing\n",
    "- **‚è±Ô∏è Timing dependencies**: Model changes need coordination across services\n",
    "- **üõ°Ô∏è Transient issues**: Network or service delays can cause temporary failures\n",
    "- **üîß Metadata dependencies**: Table schemas must be fully synchronized\n",
    "\n",
    "#### Recovery Actions:\n",
    "1. **Trigger metadata refresh**: Re-sync lakehouse table information\n",
    "2. **Wait period**: Allow background processes to complete\n",
    "3. **Retry operation**: Attempt the refresh again\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "#### Success Indicators:\n",
    "- ‚úÖ **\"Custom Semantic Model reframe OK\"**: Confirms successful refresh\n",
    "- ‚úÖ **No error messages**: All configurations applied without issues\n",
    "- ‚úÖ **Model ready**: Available for querying and report creation\n",
    "\n",
    "#### What Gets Applied:\n",
    "- ‚úÖ **Table relationships**: Star schema connections active\n",
    "- ‚úÖ **DAX measures**: Business calculations available\n",
    "- ‚úÖ **Date table**: Time intelligence functions enabled\n",
    "- ‚úÖ **Column sorting**: Intuitive ordering in visuals\n",
    "- ‚úÖ **Column visibility**: Optimized field lists for users\n",
    "\n",
    "### Fabric Integration Benefits\n",
    "The refresh process leverages Fabric's integrated architecture:\n",
    "- **OneLake integration**: Direct access to lakehouse data\n",
    "- **Unified metastore**: Consistent metadata across services  \n",
    "- **Intelligent caching**: Optimized for real-time scenarios\n",
    "\n",
    "üéØ **Model readiness checkpoint**: Your Direct Lake model is now fully configured and ready for business use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed6756-a8b0-4250-9062-3266ae563054",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "reframeOK:bool=False\n",
    "while not reframeOK:\n",
    "    try:\n",
    "        result:pandas.DataFrame = labs.refresh_semantic_model(dataset=SemanticModelName)\n",
    "        reframeOK=True\n",
    "    except:\n",
    "        print('Error with reframe... trying again.')\n",
    "        triggerMetadataRefresh()\n",
    "        time.sleep(3)\n",
    "\n",
    "print('Custom Semantic Model reframe OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ccce3",
   "metadata": {},
   "source": [
    "## 13. Create Monitoring Functions for Direct Lake Analysis\n",
    "\n",
    "### Understanding Dynamic Management Views (DMVs)\n",
    "**DMVs** are special system tables that provide insights into how your Direct Lake model operates. They reveal:\n",
    "\n",
    "- **üß† Memory usage**: Which columns are loaded into memory\n",
    "- **üå°Ô∏è Column temperature**: How frequently columns are accessed (\"hot\" vs \"cold\")\n",
    "- **üíæ Storage details**: Compression ratios and data types\n",
    "- **‚ö° Performance metrics**: Query patterns and optimization opportunities\n",
    "\n",
    "### The Storage Columns DMV\n",
    "Our `runDMV()` function queries `$SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS` to show:\n",
    "\n",
    "| Column | Purpose | Example Values |\n",
    "|--------|---------|----------------|\n",
    "| **TABLE** | Table name | DimCustomer, FactInternetSales |\n",
    "| **COLUMN** | Column name | CustomerKey, SalesAmount |\n",
    "| **DATATYPE** | Storage data type | Int64, Double, String |\n",
    "| **SIZE** | Dictionary size | 1024, 4096 (bytes) |\n",
    "| **PAGEABLE** | Can be paged to disk | TRUE, FALSE |\n",
    "| **RESIDENT** | Currently in memory | TRUE, FALSE |\n",
    "| **TEMPERATURE** | Access frequency | HOT, WARM, COLD |\n",
    "| **LASTACCESSED** | Last access time | 2024-01-15 14:30:00 |\n",
    "\n",
    "### Temperature-Based Optimization\n",
    "**Column temperature** is crucial for Direct Lake performance:\n",
    "\n",
    "#### üî• **HOT Columns**:\n",
    "- Frequently accessed in queries\n",
    "- Kept in memory for fast access\n",
    "- Examples: Date columns, key measures\n",
    "\n",
    "#### üå°Ô∏è **WARM Columns**:\n",
    "- Occasionally accessed\n",
    "- May be paged in/out of memory\n",
    "- Examples: Customer attributes used in some reports\n",
    "\n",
    "#### ‚ùÑÔ∏è **COLD Columns**:\n",
    "- Rarely or never accessed\n",
    "- Kept on disk to save memory\n",
    "- Examples: Technical columns, unused attributes\n",
    "\n",
    "### Monitoring Strategy Benefits\n",
    "\n",
    "#### Before Query Execution:\n",
    "- üìä **Baseline measurement**: See initial column states\n",
    "- üß† **Memory footprint**: Understand current memory usage\n",
    "- üìà **Performance baseline**: Establish starting point\n",
    "\n",
    "#### After Query Execution:\n",
    "- üîç **Query impact analysis**: See which columns became \"hot\"\n",
    "- üíæ **Memory changes**: Track new columns loaded\n",
    "- ‚ö° **Optimization insights**: Identify performance patterns\n",
    "\n",
    "### Function Implementation Details\n",
    "\n",
    "#### Import Requirements:\n",
    "```python\n",
    "import warnings  # Handle warning messages\n",
    "import time      # Timing operations\n",
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs  # Tracing events\n",
    "from typing import Dict, List, Optional, Callable              # Type hints\n",
    "```\n",
    "\n",
    "#### DAX Query Structure:\n",
    "The DMV query uses specific system tables:\n",
    "- `$SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS`: Column-level storage information\n",
    "- **ORDER BY DICTIONARY_TEMPERATURE DESC**: Shows hottest columns first\n",
    "\n",
    "### Expected Output\n",
    "The function will display a table showing:\n",
    "```\n",
    "TABLE               COLUMN          DATATYPE  SIZE  TEMPERATURE\n",
    "FactInternetSales   SalesAmount     Double    2048  HOT\n",
    "DimDate            Date            DateTime  1024  HOT  \n",
    "DimCustomer        CustomerName    String    4096  WARM\n",
    "...\n",
    "```\n",
    "\n",
    "üéØ **Monitoring foundation**: You now have tools to understand and optimize Direct Lake performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab34d2-784f-4a4a-bf61-afdabb4c2b69",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066ff6f-7914-447d-8c02-52840649358c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 14. Explore Direct Lake Capabilities with DAX Functions\n",
    "\n",
    "### Understanding TABLETRAITS()\n",
    "**TABLETRAITS()** is a special DAX function that reveals the internal characteristics of your Direct Lake tables. It provides insights into:\n",
    "\n",
    "- **üìä Storage mode**: Confirms Direct Lake configuration\n",
    "- **üîó Data source**: Shows lakehouse connection details  \n",
    "- **üìà Table properties**: Size, partitioning, compression\n",
    "- **‚ö° Performance hints**: Optimization opportunities\n",
    "\n",
    "### What TABLETRAITS() Reveals\n",
    "\n",
    "#### Key Information Returned:\n",
    "| Property | Description | Example Values |\n",
    "|----------|-------------|----------------|\n",
    "| **Table Name** | Name of the table | DimCustomer, FactInternetSales |\n",
    "| **Storage Mode** | How data is stored | DirectLake, Import, DirectQuery |\n",
    "| **Data Source** | Source location | OneLake path, SQL connection |\n",
    "| **Partition Count** | Number of partitions | 1, 4, 12 |\n",
    "| **Row Count** | Estimated rows | 18,484 (customers), 60,398 (sales) |\n",
    "| **Size (MB)** | Storage footprint | 2.1 MB, 15.7 MB |\n",
    "\n",
    "### Direct Lake Guardrails\n",
    "The second query retrieves **Direct Lake guardrails** - the limits and thresholds that ensure optimal performance:\n",
    "\n",
    "#### Common Guardrails Include:\n",
    "- **üìè Maximum file size**: Individual parquet file limits\n",
    "- **üìä Row count limits**: Maximum rows per table/partition\n",
    "- **üß† Memory constraints**: Available memory for column loading\n",
    "- **üîÑ Refresh frequency**: How often metadata can be updated\n",
    "- **üìà Column cardinality**: Limits on unique values per column\n",
    "\n",
    "### Why These Queries Matter\n",
    "\n",
    "#### Model Validation:\n",
    "```dax\n",
    "EVALUATE TABLETRAITS()\n",
    "```\n",
    "Confirms that your model is properly configured as Direct Lake and shows the connection to your lakehouse.\n",
    "\n",
    "#### Performance Planning:\n",
    "```python\n",
    "labs.directlake.get_direct_lake_guardrails()\n",
    "```\n",
    "Shows the limits you need to stay within for optimal performance.\n",
    "\n",
    "### Expected Output Examples\n",
    "\n",
    "#### TABLETRAITS() Sample Results:\n",
    "```\n",
    "TableName           StorageMode   DataSource                    RowCount\n",
    "DimCustomer         DirectLake    OneLake://workspace/lake...   18,484\n",
    "DimDate             DirectLake    OneLake://workspace/lake...   2,556  \n",
    "DimProduct          DirectLake    OneLake://workspace/lake...   606\n",
    "FactInternetSales   DirectLake    OneLake://workspace/lake...   60,398\n",
    "```\n",
    "\n",
    "#### Guardrails Sample Results:\n",
    "```\n",
    "Guardrail                    Current Value    Limit        Status\n",
    "Max File Size               145 MB           1 GB         ‚úÖ OK\n",
    "Max Rows Per Table          60,398           100M         ‚úÖ OK  \n",
    "Available Memory            2.1 GB           8 GB         ‚úÖ OK\n",
    "Max Column Cardinality      18,484           1.6M         ‚úÖ OK\n",
    "```\n",
    "\n",
    "### Troubleshooting with TABLETRAITS()\n",
    "If a table shows **ImportMode** instead of **DirectLake**:\n",
    "- ‚ùå **Fallback occurred**: Something caused the table to fall back to import mode\n",
    "- üîç **Check guardrails**: Verify limits aren't exceeded\n",
    "- üîß **Review configuration**: Ensure proper lakehouse connections\n",
    "\n",
    "üéØ **Model verification checkpoint**: Confirm your Direct Lake configuration is working correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf138f4-df36-41c4-bb58-a6af71edd179",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=sempy.fabric.evaluate_dax(\n",
    "    dataset=SemanticModelName, \n",
    "    dax_string=\"\"\"\n",
    "    \n",
    "    evaluate tabletraits()\n",
    "    \n",
    "    \"\"\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3394566-f8a8-4ee3-885b-766db5e8a615",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=labs.directlake.get_direct_lake_guardrails()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae67a8-b41c-4c18-8898-aaf4e478e599",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 15. Establish Performance Baseline with DMV Analysis\n",
    "\n",
    "### The Importance of Baseline Measurement\n",
    "Before executing any business queries, it's crucial to establish a **performance baseline**. This initial DMV run shows:\n",
    "\n",
    "- **üß† Initial memory state**: Which columns are already loaded\n",
    "- **üìä Starting temperatures**: Current \"hot\", \"warm\", and \"cold\" column states\n",
    "- **üíæ Memory footprint**: Baseline memory usage before query execution\n",
    "- **üéØ Optimization opportunities**: Identify columns that might need attention\n",
    "\n",
    "### What You'll Observe in the Baseline\n",
    "\n",
    "#### Expected Initial State:\n",
    "Most columns should show:\n",
    "- **‚ùÑÔ∏è TEMPERATURE**: \"COLD\" (not recently accessed)\n",
    "- **üö´ RESIDENT**: \"FALSE\" (not currently in memory)  \n",
    "- **üìÖ LASTACCESSED**: Older timestamps or null values\n",
    "- **üìè SIZE**: Actual dictionary sizes for each column\n",
    "\n",
    "#### Key Columns to Watch:\n",
    "| Table | Column | Expected State | Why |\n",
    "|-------|--------|----------------|-----|\n",
    "| **FactInternetSales** | SalesAmount | COLD | Main measure column |\n",
    "| **DimDate** | Date | COLD | Primary date column |\n",
    "| **DimCustomer** | CustomerKey | COLD | Relationship key |\n",
    "| **DimProduct** | ProductKey | COLD | Relationship key |\n",
    "\n",
    "### DMV Column Analysis\n",
    "\n",
    "#### Understanding the Output:\n",
    "```\n",
    "TABLE               COLUMN          DATATYPE  SIZE   PAGEABLE  RESIDENT  TEMPERATURE\n",
    "FactInternetSales   OrderDateKey    Int64     246    TRUE      FALSE     COLD\n",
    "FactInternetSales   CustomerKey     Int64     7244   TRUE      FALSE     COLD  \n",
    "FactInternetSales   ProductKey      Int64     1064   TRUE      FALSE     COLD\n",
    "FactInternetSales   SalesAmount     Double    8192   TRUE      FALSE     COLD\n",
    "DimCustomer         CustomerKey     Int64     7244   TRUE      FALSE     COLD\n",
    "DimDate             DateKey         Int64     1024   TRUE      FALSE     COLD\n",
    "```\n",
    "\n",
    "### Storage Insights from DMV\n",
    "\n",
    "#### Data Type Optimization:\n",
    "- **Int64**: Efficient for keys and identifiers\n",
    "- **Double**: Precise for currency values\n",
    "- **String**: Variable size for text fields\n",
    "\n",
    "#### Memory Management:\n",
    "- **PAGEABLE=TRUE**: Column can be moved between memory and disk\n",
    "- **RESIDENT=FALSE**: Currently stored on disk, not in memory\n",
    "- **SIZE**: Dictionary compression size (smaller = better compression)\n",
    "\n",
    "### Baseline Benefits for Learning\n",
    "\n",
    "#### Performance Comparison:\n",
    "1. **üìä Before query**: All columns COLD and not resident\n",
    "2. **üî• After query**: Used columns become HOT and resident\n",
    "3. **üìà Delta analysis**: See exact impact of specific queries\n",
    "\n",
    "#### Memory Usage Tracking:\n",
    "- Compare memory usage before and after queries\n",
    "- Understand which columns consume the most memory\n",
    "- Identify optimization opportunities\n",
    "\n",
    "### Expected Outcome\n",
    "You'll see a comprehensive table showing:\n",
    "- ‚úÖ **All model columns** with their current storage states\n",
    "- ‚úÖ **Temperature baseline** (mostly COLD initially)\n",
    "- ‚úÖ **Memory footprint** before any business queries\n",
    "- ‚úÖ **Performance foundation** for comparison analysis\n",
    "\n",
    "üéØ **Performance baseline established**: Ready to analyze query impact on Direct Lake behavior!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223acea6-6ac3-4f3a-b04d-603309daf706",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2dcd3-fe61-4466-b7dd-f746bac1e05a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 16. Execute Business Query and Analyze Direct Lake Performance\n",
    "\n",
    "### The Complete Performance Analysis Workflow\n",
    "This final section demonstrates the full Direct Lake performance analysis cycle:\n",
    "\n",
    "1. **üßπ Clear cache**: Start with clean memory state\n",
    "2. **üìä Execute business query**: Run meaningful DAX analysis  \n",
    "3. **üîç Analyze impact**: See how query execution affects column states\n",
    "\n",
    "### Cache Clearing Strategy\n",
    "```python\n",
    "labs.clear_cache(SemanticModelName)\n",
    "```\n",
    "\n",
    "#### Why Clear Cache First?\n",
    "- **üß† Clean memory state**: Removes any previously loaded columns\n",
    "- **üìä Accurate measurement**: Ensures we see true query impact\n",
    "- **üîÑ Consistent testing**: Provides repeatable performance analysis\n",
    "- **‚ö° Real-world simulation**: Mimics first-time query execution\n",
    "\n",
    "### Business Query Analysis\n",
    "\n",
    "#### DAX Query Breakdown:\n",
    "```dax\n",
    "EVALUATE\n",
    "SUMMARIZECOLUMNS(\n",
    "    DimDate[MonthName],\n",
    "    \"Count of Transactions\", COUNTROWS(FactInternetSales),\n",
    "    \"Sum of Sales\", [Sum of Sales]\n",
    ")\n",
    "ORDER BY [MonthName]\n",
    "```\n",
    "\n",
    "#### Query Components Explained:\n",
    "\n",
    "##### üìÖ **SUMMARIZECOLUMNS()**:\n",
    "- **Purpose**: Creates cross-table aggregations efficiently\n",
    "- **Performance**: Optimized for Direct Lake scenarios\n",
    "- **Flexibility**: Handles multiple measures and dimensions\n",
    "\n",
    "##### üóìÔ∏è **DimDate[MonthName]**:\n",
    "- **Role**: Grouping dimension (shows data by month)\n",
    "- **Sort behavior**: Uses our configured sort-by-column (MonthNumberOfYear)\n",
    "- **Expected impact**: Will make DimDate columns \"HOT\"\n",
    "\n",
    "##### üìä **\"Count of Transactions\"**: \n",
    "- **Formula**: `COUNTROWS(FactInternetSales)`\n",
    "- **Purpose**: Shows transaction volume per month\n",
    "- **Expected impact**: Will load FactInternetSales into memory\n",
    "\n",
    "##### üí∞ **\"Sum of Sales\"**:\n",
    "- **Formula**: Our custom `[Sum of Sales]` measure\n",
    "- **Purpose**: Shows revenue by month  \n",
    "- **Expected impact**: Will make SalesAmount column \"HOT\"\n",
    "\n",
    "##### üìà **ORDER BY [MonthName]**:\n",
    "- **Behavior**: Uses our sort-by-column configuration\n",
    "- **Result**: Chronological month order (Jan, Feb, Mar...)\n",
    "- **User experience**: Intuitive time-series presentation\n",
    "\n",
    "### Expected Business Results\n",
    "The query should return data like:\n",
    "```\n",
    "MonthName    Count of Transactions    Sum of Sales\n",
    "January      1,234                   $456,789.12\n",
    "February     1,567                   $567,890.23  \n",
    "March        1,890                   $678,901.34\n",
    "...\n",
    "```\n",
    "\n",
    "### Performance Impact Analysis\n",
    "\n",
    "#### After Query Execution - Expected Changes:\n",
    "| Table | Column | Before | After | Why |\n",
    "|-------|--------|--------|-------|-----|\n",
    "| **DimDate** | MonthName | COLD | HOT | Used for grouping |\n",
    "| **DimDate** | MonthNumberOfYear | COLD | HOT | Used for sorting |\n",
    "| **FactInternetSales** | SalesAmount | COLD | HOT | Used in Sum measure |\n",
    "| **FactInternetSales** | OrderDateKey | COLD | HOT | Used for relationship |\n",
    "\n",
    "#### Memory Usage Patterns:\n",
    "- **üî• HOT columns**: Recently accessed, kept in memory\n",
    "- **üìà RESIDENT=TRUE**: Columns now loaded in memory\n",
    "- **‚è∞ LASTACCESSED**: Updated to current timestamp\n",
    "- **üíæ Memory increase**: Overall model memory footprint grows\n",
    "\n",
    "### Learning Objectives Achieved\n",
    "\n",
    "#### Direct Lake Behavior Understanding:\n",
    "- ‚úÖ **Real-time data access**: No import delay, immediate results\n",
    "- ‚úÖ **Intelligent caching**: Only needed columns loaded into memory\n",
    "- ‚úÖ **Performance optimization**: Subsequent queries using same columns will be faster\n",
    "- ‚úÖ **Resource efficiency**: Unused columns remain on disk\n",
    "\n",
    "#### Performance Monitoring Mastery:\n",
    "- ‚úÖ **Before/after analysis**: Clear view of query impact\n",
    "- ‚úÖ **Memory optimization**: Understanding of column temperature\n",
    "- ‚úÖ **Cache behavior**: How Direct Lake manages memory\n",
    "- ‚úÖ **Query planning**: Insights for future optimization\n",
    "\n",
    "üéØ **Workshop completion**: You've successfully created, configured, and analyzed a production-ready Direct Lake semantic model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbd807-3013-40a4-b584-fda3428ab4be",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "labs.clear_cache(SemanticModelName)\n",
    "\n",
    "df=sempy.fabric.evaluate_dax(\n",
    "    dataset=SemanticModelName, \n",
    "    dax_string=\"\"\"\n",
    "    \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "               \n",
    "                DimDate[MonthName] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(FactInternetSales) ,\n",
    "                \"Sum of Sales\" , [Sum of Sales] \n",
    "        )\n",
    "        ORDER BY [MonthName]\n",
    "    \"\"\")\n",
    "display(df)\n",
    "\n",
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aec05f",
   "metadata": {},
   "source": [
    "## 17. Clean Up Resources and Session Conclusion\n",
    "\n",
    "### Workshop Summary üéâ\n",
    "Congratulations! You have successfully completed Lab 1 and built a comprehensive Direct Lake semantic model. Here's what you accomplished:\n",
    "\n",
    "#### ‚úÖ **Infrastructure Setup**\n",
    "- Created a lakehouse with proper configuration\n",
    "- Loaded Adventure Works sample data (4 tables, 80K+ rows)\n",
    "- Configured metadata synchronization\n",
    "\n",
    "#### ‚úÖ **Model Development**  \n",
    "- Built a Direct Lake semantic model from lakehouse tables\n",
    "- Established star schema relationships (3 relationships)\n",
    "- Created business measures with proper DAX and formatting\n",
    "\n",
    "#### ‚úÖ **User Experience Optimization**\n",
    "- Configured date table for time intelligence\n",
    "- Set logical column sorting for better visuals\n",
    "- Optimized column visibility for end users\n",
    "\n",
    "#### ‚úÖ **Performance Analysis**\n",
    "- Implemented DMV monitoring for performance insights\n",
    "- Analyzed query execution impact on memory usage  \n",
    "- Established baseline and post-query performance comparison\n",
    "\n",
    "### Key Direct Lake Concepts Learned\n",
    "\n",
    "#### üîÑ **Real-time Analytics**\n",
    "Your model provides immediate access to lakehouse data without import delays or scheduled refreshes.\n",
    "\n",
    "#### ‚ö° **Intelligent Memory Management**\n",
    "Direct Lake automatically loads only the columns needed for your queries, optimizing both performance and resource usage.\n",
    "\n",
    "#### üìä **Enterprise-Ready Design**\n",
    "The star schema design with proper relationships, measures, and formatting provides a foundation for scalable business intelligence.\n",
    "\n",
    "### Next Steps in Your Direct Lake Journey\n",
    "\n",
    "#### üöÄ **Immediate Actions**:\n",
    "- Explore the model in Power BI Desktop or Fabric\n",
    "- Create reports using the measures and relationships you built\n",
    "- Experiment with different DAX queries to see performance patterns\n",
    "\n",
    "#### üìà **Advanced Learning**:\n",
    "- **Lab 2**: Scale to larger datasets and understand big data scenarios\n",
    "- **Lab 3**: Analyze Delta table structure and optimization\n",
    "- **Lab 4**: Explore fallback behaviors and troubleshooting\n",
    "\n",
    "#### üõ†Ô∏è **Production Considerations**:\n",
    "- Security and access control for lakehouse data\n",
    "- Monitoring and alerting for model performance\n",
    "- Governance and lifecycle management\n",
    "\n",
    "### Resource Cleanup Importance\n",
    "The following command stops the Spark session to:\n",
    "- **üí∞ Save costs**: Release compute resources\n",
    "- **üßπ Clean memory**: Free up cluster resources for other users\n",
    "- **‚úÖ Best practice**: Proper session management in Fabric notebooks\n",
    "\n",
    "### Final Thoughts\n",
    "Direct Lake represents a paradigm shift in analytics, providing the **real-time capabilities of DirectQuery** with the **performance benefits of Import mode**. You now have hands-on experience with this powerful technology!\n",
    "\n",
    "üéØ **Ready for the next lab?** Let's explore Direct Lake with big data scenarios!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
