{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09b6c60-d142-42f2-aa77-b714234d69d3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 1: Create Direct Lake Semantic Model\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "This lab teaches you how to create a **Direct Lake semantic model** from scratch using Microsoft Fabric. You'll learn the complete workflow from data loading to model creation and validation.\n",
    "\n",
    "### What You'll Build\n",
    "\n",
    "**Workshop Flow:**\n",
    "```\n",
    "1. Lakehouse Setup\n",
    "   ‚Üì\n",
    "2. Load Adventure Works Data  \n",
    "   ‚Üì\n",
    "3. Create Semantic Model\n",
    "   ‚Üì\n",
    "4. Add Relationships\n",
    "   ‚Üì\n",
    "5. Create Measures\n",
    "   ‚Üì\n",
    "6. Test & Validate\n",
    "```\n",
    "\n",
    "**End Result:** A fully functional Direct Lake semantic model ready for Power BI reporting with real-time data access.\n",
    "\n",
    "### Key Concepts\n",
    "- **Direct Lake**: Query data directly from Delta Lake without imports\n",
    "- **Adventure Works**: Sample business dataset with customers, products, and sales\n",
    "- **Semantic Model**: Business logic layer with relationships and measures\n",
    "\n",
    "### Learning Objectives\n",
    "By completing this lab, you'll be able to:\n",
    "- ‚úÖ Set up a lakehouse and load sample data\n",
    "- ‚úÖ Create a Direct Lake semantic model programmatically  \n",
    "- ‚úÖ Define table relationships and business measures\n",
    "- ‚úÖ Validate model performance and behavior\n",
    "\n",
    "**Estimated Time**: 30-45 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd099e87-89f6-45b2-b286-d7f8a12b6a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "Install Semantic Link Labs to enable Direct Lake model creation and management capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ed5ca-a6ba-478a-b029-17b6db9b6308",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q --disable-pip-version-check semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc44ce-395c-4db3-8979-b06acf9f8ecf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2. Import Libraries and Set Variables\n",
    "\n",
    "Import required libraries and define key variables for the lakehouse and semantic model names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e841d6-a757-4029-aa71-88d4bd286c30",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as labs\n",
    "from sempy import fabric\n",
    "import sempy\n",
    "import pandas\n",
    "import json\n",
    "import time\n",
    "\n",
    "LakehouseName = \"AdventureWorks\"\n",
    "SemanticModelName = f\"{LakehouseName}_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99f94c-a564-4aca-b670-31da354b7b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3. Create or Connect to Lakehouse\n",
    "\n",
    "Check if the AdventureWorks lakehouse exists, create it if needed, and retrieve workspace identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760df625-543c-4289-b6cc-f043290d5879",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "lakehouses=labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "if LakehouseName in lakehouses.values:\n",
    "    lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]\n",
    "else:\n",
    "    lakehouseId = fabric.create_lakehouse(LakehouseName)\n",
    "\n",
    "workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "workspaceName = sempy.fabric.resolve_workspace_name(workspaceId)\n",
    "print(f\"WorkspaceId = {workspaceId}, LakehouseID = {lakehouseId}, Workspace Name = {workspaceName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6e83e-d5fd-4020-9766-c236ef176329",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Load Adventure Works Sample Data\n",
    "\n",
    "Load four Adventure Works tables (Customer, Date, Product, Sales) into the lakehouse using region-aware data sources.\n",
    "\n",
    "**Tables being loaded:**\n",
    "- DimCustomer (~18K customers)  \n",
    "- DimDate (2K+ dates)\n",
    "- DimProduct (~600 products)\n",
    "- FactInternetSales (~60K sales records)\n",
    "```\n",
    "Loaded DimCustomer\n",
    "Loaded DimDate  \n",
    "Loaded DimProduct\n",
    "Loaded FactInternetSales\n",
    "Done\n",
    "```\n",
    "\n",
    "### Behind the Scenes\n",
    "- Data is stored in **Delta format** for ACID compliance\n",
    "- **Overwrite mode** ensures clean data for the workshop\n",
    "- **OneLake integration** provides seamless cross-workspace data access\n",
    "\n",
    "üéØ **Success indicator**: All four \"Loaded\" messages followed by \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28505f17-8bf7-4400-a409-344c4d01b4ef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "capacity_name = labs.get_capacity_name()\n",
    "\n",
    "def loadDataToLakehouse(fromTable: str, toTable: str):\n",
    "    \"\"\"\n",
    "    Optimized data loading function with improved error handling and performance.\n",
    "    \n",
    "    Args:\n",
    "        fromTable: Source table name to read from\n",
    "        toTable: Target table name to write to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get lakehouse properties once and reuse\n",
    "        lakehouse_props = notebookutils.lakehouse.getWithProperties(LakehouseName)\n",
    "        workspaceId = lakehouse_props[\"workspaceId\"]\n",
    "        lakehouseId = lakehouse_props[\"id\"]\n",
    "\n",
    "        # Region-aware connection string selection\n",
    "        if capacity_name == \"FabConUS8-P1\":  # West US 3\n",
    "            conn_str = \"abfss://b1d61bbe-de20-4d3a-8075-b8e2eaacb868@onelake.dfs.fabric.microsoft.com/631e45c0-1243-4f42-920a-56bfe6ecdd6d/Tables\"\n",
    "        else:  # North Central US (default)\n",
    "            conn_str = \"abfss://16cf855f-3bf4-4312-a7a1-ccf5cb6a0121@onelake.dfs.fabric.microsoft.com/99ed86df-13d1-4008-a7f6-5768e53f4f85/Tables\"\n",
    "\n",
    "        # Read source data with format specification for better performance\n",
    "        customer_df = spark.read.format(\"delta\").load(f\"{conn_str}/{fromTable}\")\n",
    "        \n",
    "        # Cache the DataFrame if it will be used multiple times or is computation-heavy\n",
    "        customer_df.cache()\n",
    "        \n",
    "        # Write with optimized settings\n",
    "        (customer_df\n",
    "         .write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .save(f\"abfss://{workspaceId}@onelake.dfs.fabric.microsoft.com/{lakehouseId}/Tables/{toTable}\"))\n",
    "        \n",
    "        # Unpersist cached DataFrame to free memory\n",
    "        customer_df.unpersist()\n",
    "        \n",
    "        print(f\"Loaded {toTable}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {toTable}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load all tables with proper error handling\n",
    "tables_to_load = [\n",
    "    (\"DimCustomer\", \"DimCustomer\"),\n",
    "    (\"DimDate\", \"DimDate\"),\n",
    "    (\"DimProduct\", \"DimProduct\"),\n",
    "    (\"FactInternetSales\", \"FactInternetSales\")\n",
    "]\n",
    "\n",
    "for from_table, to_table in tables_to_load:\n",
    "    loadDataToLakehouse(from_table, to_table)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff2996-0b1c-4792-99cd-8d61315e65da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 5. Trigger Metadata Synchronization\n",
    "\n",
    "Force synchronization between lakehouse storage and SQL Analytics Endpoint to ensure schema accuracy for the semantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54e1aa-a94e-4efd-b40e-f497f54589e8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "##https://medium.com/@sqltidy/delays-in-the-automatically-generated-schema-in-the-sql-analytics-endpoint-of-the-lakehouse-b01c7633035d\n",
    "\n",
    "def triggerMetadataRefresh():\n",
    "    client = fabric.FabricRestClient()\n",
    "    response = client.get(f\"/v1/workspaces/{workspaceId}/lakehouses/{lakehouseId}\")\n",
    "    sqlendpoint = response.json()['properties']['sqlEndpointProperties']['id']\n",
    "\n",
    "    # trigger sync\n",
    "    uri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}\"\n",
    "    payload = {\"commands\":[{\"$type\":\"MetadataRefreshExternalCommand\"}]}\n",
    "    response = client.post(uri,json= payload)\n",
    "    batchId = response.json()['batchId']\n",
    "\n",
    "    # Monitor Progress\n",
    "    statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "    statusresponsedata = client.get(statusuri).json()\n",
    "    progressState = statusresponsedata['progressState']\n",
    "    print(f\"Metadata refresh : {progressState}\")\n",
    "    while progressState != \"success\":\n",
    "        statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "        statusresponsedata = client.get(statusuri).json()\n",
    "        progressState = statusresponsedata['progressState']\n",
    "        print(f\"Metadata refresh : {progressState}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    print('Metadata refresh complete')\n",
    "\n",
    "triggerMetadataRefresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e16b1-9893-46d3-ac7d-93a4d1453c5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 6. Create Direct Lake Semantic Model\n",
    "\n",
    "Generate a semantic model from all lakehouse tables with automatic table discovery and error handling for robust creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e9cc7-de53-4c81-8408-9fb18ef4383a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sempy import fabric\n",
    "\n",
    "#1. Generate list of ALL table names from lakehouse to add to Semantic Model\n",
    "lakehouseTables:list = labs.lakehouse.get_lakehouse_tables(lakehouse=LakehouseName)[\"Table Name\"]\n",
    "\n",
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        #2 Create the semantic model\n",
    "        if sempy.fabric.list_items().query(f\"`Display Name`=='{LakehouseName}_model' & Type=='SemanticModel'  \").shape[0] ==0:\n",
    "            labs.directlake.generate_direct_lake_semantic_model(dataset=f\"{LakehouseName}_model\",lakehouse_tables=lakehouseTables,workspace=workspaceName,lakehouse=lakehouseId,refresh=False,overwrite=True)\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error creating model... trying again.')\n",
    "        time.sleep(3)\n",
    "        triggerMetadataRefresh()\n",
    "\n",
    "print('Semantic model created OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df3373-52ca-4e45-83a6-40e1b20c184d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 7. Configure Table Relationships\n",
    "\n",
    "Establish star schema relationships between fact and dimension tables for accurate cross-table analysis and business intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616193a-57df-4139-91ef-c73830331555",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing relationships\n",
    "            for r in tom.model.Relationships:\n",
    "                tom.model.Relationships.Remove(r)\n",
    "\n",
    "            #2. Creates correct relationships\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"OrderDateKey\" , to_table=\"DimDate\"    , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"CustomerKey\"  , to_table=\"DimCustomer\", to_column=\"CustomerKey\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"ProductKey\"   , to_table=\"DimProduct\" , to_column=\"ProductKey\"    , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding relationships... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba5536-018c-402b-aa0f-663ee5e8d07f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 8. Add Business Intelligence Measures\n",
    "\n",
    "Create essential DAX measures (Sum of Sales, Count of Sales) with proper formatting for business reporting and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666631c-4221-4638-b8ff-10fee8a4f7df",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing measures\n",
    "            for t in tom.model.Tables:\n",
    "                for m in t.Measures:\n",
    "                    tom.remove_object(m)\n",
    "                    print(f\"[{m.Name}] measure removed\")\n",
    "\n",
    "            tom.add_measure(table_name=\"FactInternetSales\" ,measure_name=\"Sum of Sales\",expression=\"SUM(FactInternetSales[SalesAmount])\",format_string=\"\\$#,0.###############;(\\$#,0.###############);\\$#,0.###############\")\n",
    "            tom.add_measure(table_name=\"FactInternetSales\" ,measure_name=\"Count of Sales\",expression=\"COUNTROWS(FactInternetSales)\",format_string=\"#,0\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding measures... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ff91f-0362-4c3e-bf2e-60c64867cec8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 9. Configure Date Table for Time Intelligence\n",
    "\n",
    "Mark the DimDate table as a date table to enable time-based analysis functions and calendar features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f48bf-893b-4a8f-92a1-e5e80b968840",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            tom.mark_as_date_table(table_name=\"DimDate\",column_name=\"Date\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error with date table... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb148df-8490-47fc-8e9c-6b8cbcb5fb6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 10. Configure Column Sorting\n",
    "\n",
    "Set logical sorting order for MonthName and DayOfWeek columns to ensure intuitive chronological display in reports.\n",
    "- **Sort by**: 1, 2, 3...\n",
    "- **Result**: Logical weekday progression\n",
    "\n",
    "### Technical Implementation Details\n",
    "\n",
    "#### JSON Output Inspection\n",
    "The code displays the table's JSON structure to verify:\n",
    "- ‚úÖ **Sort by relationships** are properly configured\n",
    "- ‚úÖ **Column metadata** is correctly set\n",
    "- ‚úÖ **Model structure** matches expectations\n",
    "\n",
    "#### BIM (Business Intelligence Model) Format\n",
    "The output shows the table definition in BIM format, which includes:\n",
    "- Column definitions and data types\n",
    "- Sort by column relationships  \n",
    "- Display formatting rules\n",
    "- Performance optimization settings\n",
    "\n",
    "### User Experience Impact\n",
    "\n",
    "#### Before Sort Configuration:\n",
    "```\n",
    "Chart shows: Apr, Aug, Dec, Feb, Jan, Jul, Jun...\n",
    "Users think: \"This makes no sense!\"\n",
    "```\n",
    "\n",
    "#### After Sort Configuration:\n",
    "```\n",
    "Chart shows: Jan, Feb, Mar, Apr, May, Jun, Jul...\n",
    "Users think: \"Perfect! This is what I expected.\"\n",
    "```\n",
    "\n",
    "### Expected Outcome\n",
    "- ‚úÖ **MonthName column** sorts chronologically (Jan ‚Üí Dec)\n",
    "- ‚úÖ **DayOfWeek column** sorts logically (Sun ‚Üí Sat) \n",
    "- ‚úÖ **JSON structure** displayed for verification\n",
    "- ‚úÖ **Enhanced UX** for report consumers\n",
    "\n",
    "üéØ **User experience checkpoint**: Your model now provides intuitive sorting behavior!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468514f8-1e73-4732-a22f-b0ce5f3c8eea",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "tom = labs.tom.TOMWrapper(dataset=SemanticModelName, workspace=workspaceName, readonly=False)\n",
    "tom.set_sort_by_column(table_name=\"DimDate\",column_name=\"MonthName\"       ,sort_by_column=\"MonthNumberOfYear\")\n",
    "tom.set_sort_by_column(table_name=\"DimDate\",column_name=\"DayOfWeek\"       ,sort_by_column=\"DayNumberOfWeek\")\n",
    "tom.model.SaveChanges()\n",
    "\n",
    "i:int=0\n",
    "for t in tom.model.Tables:\n",
    "    if t.Name==\"DimDate\":\n",
    "        bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "        print(bim)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7babd-5053-4dc5-97e1-4398e67dba26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 11. Optimize Model by Hiding Technical Columns\n",
    "\n",
    "### Why Hide Columns in Fact Tables?\n",
    "**Fact tables** contain both business-relevant and technical columns. Hiding technical columns improves the user experience by:\n",
    "\n",
    "- **üéØ Reducing complexity**: Users see only meaningful business columns\n",
    "- **üö´ Preventing errors**: Technical keys shouldn't be used in reports directly\n",
    "- **üìä Promoting measures**: Guides users toward proper aggregated values\n",
    "- **‚ö° Improving performance**: Reduces metadata that client tools need to process\n",
    "\n",
    "### Column Visibility Strategy\n",
    "\n",
    "#### FactInternetSales Column Analysis:\n",
    "| Column Type | Examples | Visibility | Reason |\n",
    "|-------------|----------|------------|---------|\n",
    "| **Foreign Keys** | CustomerKey, ProductKey | üîí Hidden | Use relationships instead |\n",
    "| **Technical IDs** | OrderDateKey | üîí Hidden | Use Date dimension |\n",
    "| **Raw Values** | SalesAmount, Quantity | üîí Hidden | Use measures instead |\n",
    "| **Calculated Fields** | [Sum of Sales] measure | üëÅÔ∏è Visible | Proper aggregation |\n",
    "\n",
    "### The Column Hiding Process\n",
    "\n",
    "#### 1. **Iterate Through Tables**\n",
    "```python\n",
    "for t in tom.model.Tables:\n",
    "    if t.Name in [\"FactInternetSales\"]:\n",
    "```\n",
    "- Targets specific fact tables (extensible to multiple tables)\n",
    "- Preserves dimension table columns for filtering and grouping\n",
    "\n",
    "#### 2. **Hide All Columns**\n",
    "```python\n",
    "for c in t.Columns:\n",
    "    c.IsHidden = True\n",
    "```\n",
    "- Sets the `IsHidden` property to `True` for each column\n",
    "- Columns remain in the model but don't appear in field lists\n",
    "\n",
    "#### 3. **JSON Verification**\n",
    "```python\n",
    "bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i], indent=4)\n",
    "```\n",
    "- Displays the table structure for verification\n",
    "- Shows the `IsHidden` property for each column\n",
    "\n",
    "### Best Practices for Column Visibility\n",
    "\n",
    "#### Always Hide in Fact Tables:\n",
    "- ‚úÖ **Surrogate keys**: CustomerKey, ProductKey, DateKey\n",
    "- ‚úÖ **Raw numeric values**: Use measures instead\n",
    "- ‚úÖ **Technical timestamps**: Created dates, modified dates\n",
    "\n",
    "#### Keep Visible in Dimension Tables:\n",
    "- ‚úÖ **Descriptive attributes**: Customer names, product categories\n",
    "- ‚úÖ **Natural keys**: Account numbers, product codes\n",
    "- ‚úÖ **Date components**: Year, month, quarter (from date table)\n",
    "\n",
    "### User Experience Impact\n",
    "\n",
    "#### Before Hiding:\n",
    "```\n",
    "Field List shows:\n",
    "‚îú‚îÄ‚îÄ FactInternetSales\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ CustomerKey         ‚Üê Confusing\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ProductKey          ‚Üê Confusing  \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ OrderDateKey        ‚Üê Confusing\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ SalesAmount         ‚Üê Misleading\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Quantity            ‚Üê Misleading\n",
    "```\n",
    "\n",
    "#### After Hiding:\n",
    "```\n",
    "Field List shows:\n",
    "‚îú‚îÄ‚îÄ FactInternetSales\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Sum of Sales        ‚Üê Clear measure\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Count of Sales      ‚Üê Clear measure\n",
    "‚îú‚îÄ‚îÄ DimCustomer\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ CustomerName        ‚Üê Useful for grouping\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ CustomerCity        ‚Üê Useful for filtering\n",
    "```\n",
    "\n",
    "### Expected Outcome\n",
    "- ‚úÖ **All FactInternetSales columns** hidden from user interface\n",
    "- ‚úÖ **Measures remain visible** for proper aggregation\n",
    "- ‚úÖ **Dimension columns visible** for filtering and grouping\n",
    "- ‚úÖ **JSON structure** displayed showing `IsHidden: true`\n",
    "\n",
    "üéØ **Model optimization checkpoint**: Your model now guides users toward correct analysis patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dbe0c-7d08-4c11-963f-07b44c20f401",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "i:int=0\n",
    "for t in tom.model.Tables:\n",
    "    if t.Name in [\"FactInternetSales\"]:\n",
    "        for c in t.Columns:\n",
    "            c.IsHidden=True\n",
    "\n",
    "        bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "        print(bim)\n",
    "    i=i+1\n",
    "    \n",
    "tom.model.SaveChanges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc341d-8682-4a4d-8b00-300d8723bcfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 12. Refresh Semantic Model to Apply Configuration Changes\n",
    "\n",
    "### Understanding Semantic Model Refresh\n",
    "**Refreshing** a semantic model ensures that all configuration changes are properly applied and the model is ready for use. This process:\n",
    "\n",
    "- **üíæ Commits metadata changes**: Relationships, measures, column properties\n",
    "- **üîÑ Synchronizes with lakehouse**: Ensures Direct Lake connections are active\n",
    "- **‚ö° Optimizes query plans**: Updates internal structures for better performance\n",
    "- **‚úÖ Validates configuration**: Checks that all changes are compatible\n",
    "\n",
    "### Why Refresh is Critical for Direct Lake\n",
    "Direct Lake models have unique refresh requirements:\n",
    "\n",
    "#### Configuration Refresh vs. Data Refresh:\n",
    "| Refresh Type | Purpose | When Needed | Duration |\n",
    "|--------------|---------|-------------|----------|\n",
    "| **Configuration** | Apply metadata changes | After model modifications | Seconds |\n",
    "| **Data** | Update cached data | Not needed (real-time) | N/A |\n",
    "\n",
    "#### What Happens During Refresh:\n",
    "1. **üîç Validates structure**: Checks table relationships and measure definitions\n",
    "2. **üîó Tests lakehouse connections**: Ensures Direct Lake paths are accessible  \n",
    "3. **üìä Updates metadata**: Applies visibility, sorting, and formatting rules\n",
    "4. **‚ö° Optimizes performance**: Builds internal indexes and query plans\n",
    "\n",
    "### Error Handling Strategy\n",
    "\n",
    "#### Retry Logic Implementation:\n",
    "```python\n",
    "reframeOK:bool = False\n",
    "while not reframeOK:\n",
    "    try:\n",
    "        result = labs.refresh_semantic_model(dataset=SemanticModelName)\n",
    "        reframeOK = True\n",
    "    except:\n",
    "        print('Error with reframe... trying again.')\n",
    "        triggerMetadataRefresh()\n",
    "        time.sleep(3)\n",
    "```\n",
    "\n",
    "#### Why Retry Logic is Necessary:\n",
    "- **üîÑ Async operations**: Lakehouse metadata may still be syncing\n",
    "- **‚è±Ô∏è Timing dependencies**: Model changes need coordination across services\n",
    "- **üõ°Ô∏è Transient issues**: Network or service delays can cause temporary failures\n",
    "- **üîß Metadata dependencies**: Table schemas must be fully synchronized\n",
    "\n",
    "#### Recovery Actions:\n",
    "1. **Trigger metadata refresh**: Re-sync lakehouse table information\n",
    "2. **Wait period**: Allow background processes to complete\n",
    "3. **Retry operation**: Attempt the refresh again\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "#### Success Indicators:\n",
    "- ‚úÖ **\"Custom Semantic Model reframe OK\"**: Confirms successful refresh\n",
    "- ‚úÖ **No error messages**: All configurations applied without issues\n",
    "- ‚úÖ **Model ready**: Available for querying and report creation\n",
    "\n",
    "#### What Gets Applied:\n",
    "- ‚úÖ **Table relationships**: Star schema connections active\n",
    "- ‚úÖ **DAX measures**: Business calculations available\n",
    "- ‚úÖ **Date table**: Time intelligence functions enabled\n",
    "- ‚úÖ **Column sorting**: Intuitive ordering in visuals\n",
    "- ‚úÖ **Column visibility**: Optimized field lists for users\n",
    "\n",
    "### Fabric Integration Benefits\n",
    "The refresh process leverages Fabric's integrated architecture:\n",
    "- **OneLake integration**: Direct access to lakehouse data\n",
    "- **Unified metastore**: Consistent metadata across services  \n",
    "- **Intelligent caching**: Optimized for real-time scenarios\n",
    "\n",
    "üéØ **Model readiness checkpoint**: Your Direct Lake model is now fully configured and ready for business use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed6756-a8b0-4250-9062-3266ae563054",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "reframeOK:bool=False\n",
    "while not reframeOK:\n",
    "    try:\n",
    "        result:pandas.DataFrame = labs.refresh_semantic_model(dataset=SemanticModelName)\n",
    "        reframeOK=True\n",
    "    except:\n",
    "        print('Error with reframe... trying again.')\n",
    "        triggerMetadataRefresh()\n",
    "        time.sleep(3)\n",
    "\n",
    "print('Custom Semantic Model reframe OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ccce3",
   "metadata": {},
   "source": [
    "## 13. Create Monitoring Functions for Direct Lake Analysis\n",
    "\n",
    "### Understanding Dynamic Management Views (DMVs)\n",
    "**DMVs** are special system tables that provide insights into how your Direct Lake model operates. They reveal:\n",
    "\n",
    "- **üß† Memory usage**: Which columns are loaded into memory\n",
    "- **üå°Ô∏è Column temperature**: How frequently columns are accessed (\"hot\" vs \"cold\")\n",
    "- **üíæ Storage details**: Compression ratios and data types\n",
    "- **‚ö° Performance metrics**: Query patterns and optimization opportunities\n",
    "\n",
    "### The Storage Columns DMV\n",
    "Our `runDMV()` function queries `$SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS` to show:\n",
    "\n",
    "| Column | Purpose | Example Values |\n",
    "|--------|---------|----------------|\n",
    "| **TABLE** | Table name | DimCustomer, FactInternetSales |\n",
    "| **COLUMN** | Column name | CustomerKey, SalesAmount |\n",
    "| **DATATYPE** | Storage data type | Int64, Double, String |\n",
    "| **SIZE** | Dictionary size | 1024, 4096 (bytes) |\n",
    "| **PAGEABLE** | Can be paged to disk | TRUE, FALSE |\n",
    "| **RESIDENT** | Currently in memory | TRUE, FALSE |\n",
    "| **TEMPERATURE** | Access frequency | HOT, WARM, COLD |\n",
    "| **LASTACCESSED** | Last access time | 2024-01-15 14:30:00 |\n",
    "\n",
    "### Temperature-Based Optimization\n",
    "**Column temperature** is crucial for Direct Lake performance:\n",
    "\n",
    "#### üî• **HOT Columns**:\n",
    "- Frequently accessed in queries\n",
    "- Kept in memory for fast access\n",
    "- Examples: Date columns, key measures\n",
    "\n",
    "#### üå°Ô∏è **WARM Columns**:\n",
    "- Occasionally accessed\n",
    "- May be paged in/out of memory\n",
    "- Examples: Customer attributes used in some reports\n",
    "\n",
    "#### ‚ùÑÔ∏è **COLD Columns**:\n",
    "- Rarely or never accessed\n",
    "- Kept on disk to save memory\n",
    "- Examples: Technical columns, unused attributes\n",
    "\n",
    "### Monitoring Strategy Benefits\n",
    "\n",
    "#### Before Query Execution:\n",
    "- üìä **Baseline measurement**: See initial column states\n",
    "- üß† **Memory footprint**: Understand current memory usage\n",
    "- üìà **Performance baseline**: Establish starting point\n",
    "\n",
    "#### After Query Execution:\n",
    "- üîç **Query impact analysis**: See which columns became \"hot\"\n",
    "- üíæ **Memory changes**: Track new columns loaded\n",
    "- ‚ö° **Optimization insights**: Identify performance patterns\n",
    "\n",
    "### Function Implementation Details\n",
    "\n",
    "#### Import Requirements:\n",
    "```python\n",
    "import warnings  # Handle warning messages\n",
    "import time      # Timing operations\n",
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs  # Tracing events\n",
    "from typing import Dict, List, Optional, Callable              # Type hints\n",
    "```\n",
    "\n",
    "#### DAX Query Structure:\n",
    "The DMV query uses specific system tables:\n",
    "- `$SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS`: Column-level storage information\n",
    "- **ORDER BY DICTIONARY_TEMPERATURE DESC**: Shows hottest columns first\n",
    "\n",
    "### Expected Output\n",
    "The function will display a table showing:\n",
    "```\n",
    "TABLE               COLUMN          DATATYPE  SIZE  TEMPERATURE\n",
    "FactInternetSales   SalesAmount     Double    2048  HOT\n",
    "DimDate            Date            DateTime  1024  HOT  \n",
    "DimCustomer        CustomerName    String    4096  WARM\n",
    "...\n",
    "```\n",
    "\n",
    "üéØ **Monitoring foundation**: You now have tools to understand and optimize Direct Lake performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab34d2-784f-4a4a-bf61-afdabb4c2b69",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066ff6f-7914-447d-8c02-52840649358c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 14. Explore Direct Lake Capabilities with DAX Functions\n",
    "\n",
    "### Understanding TABLETRAITS()\n",
    "**TABLETRAITS()** is a special DAX function that reveals the internal characteristics of your Direct Lake tables. It provides insights into:\n",
    "\n",
    "- **üìä Storage mode**: Confirms Direct Lake configuration\n",
    "- **üîó Data source**: Shows lakehouse connection details  \n",
    "- **üìà Table properties**: Size, partitioning, compression\n",
    "- **‚ö° Performance hints**: Optimization opportunities\n",
    "\n",
    "### What TABLETRAITS() Reveals\n",
    "\n",
    "#### Key Information Returned:\n",
    "| Property | Description | Example Values |\n",
    "|----------|-------------|----------------|\n",
    "| **Table Name** | Name of the table | DimCustomer, FactInternetSales |\n",
    "| **Storage Mode** | How data is stored | DirectLake, Import, DirectQuery |\n",
    "| **Data Source** | Source location | OneLake path, SQL connection |\n",
    "| **Partition Count** | Number of partitions | 1, 4, 12 |\n",
    "| **Row Count** | Estimated rows | 18,484 (customers), 60,398 (sales) |\n",
    "| **Size (MB)** | Storage footprint | 2.1 MB, 15.7 MB |\n",
    "\n",
    "### Direct Lake Guardrails\n",
    "The second query retrieves **Direct Lake guardrails** - the limits and thresholds that ensure optimal performance:\n",
    "\n",
    "#### Common Guardrails Include:\n",
    "- **üìè Maximum file size**: Individual parquet file limits\n",
    "- **üìä Row count limits**: Maximum rows per table/partition\n",
    "- **üß† Memory constraints**: Available memory for column loading\n",
    "- **üîÑ Refresh frequency**: How often metadata can be updated\n",
    "- **üìà Column cardinality**: Limits on unique values per column\n",
    "\n",
    "### Why These Queries Matter\n",
    "\n",
    "#### Model Validation:\n",
    "```dax\n",
    "EVALUATE TABLETRAITS()\n",
    "```\n",
    "Confirms that your model is properly configured as Direct Lake and shows the connection to your lakehouse.\n",
    "\n",
    "#### Performance Planning:\n",
    "```python\n",
    "labs.directlake.get_direct_lake_guardrails()\n",
    "```\n",
    "Shows the limits you need to stay within for optimal performance.\n",
    "\n",
    "### Expected Output Examples\n",
    "\n",
    "#### TABLETRAITS() Sample Results:\n",
    "```\n",
    "TableName           StorageMode   DataSource                    RowCount\n",
    "DimCustomer         DirectLake    OneLake://workspace/lake...   18,484\n",
    "DimDate             DirectLake    OneLake://workspace/lake...   2,556  \n",
    "DimProduct          DirectLake    OneLake://workspace/lake...   606\n",
    "FactInternetSales   DirectLake    OneLake://workspace/lake...   60,398\n",
    "```\n",
    "\n",
    "#### Guardrails Sample Results:\n",
    "```\n",
    "Guardrail                    Current Value    Limit        Status\n",
    "Max File Size               145 MB           1 GB         ‚úÖ OK\n",
    "Max Rows Per Table          60,398           100M         ‚úÖ OK  \n",
    "Available Memory            2.1 GB           8 GB         ‚úÖ OK\n",
    "Max Column Cardinality      18,484           1.6M         ‚úÖ OK\n",
    "```\n",
    "\n",
    "### Troubleshooting with TABLETRAITS()\n",
    "If a table shows **ImportMode** instead of **DirectLake**:\n",
    "- ‚ùå **Fallback occurred**: Something caused the table to fall back to import mode\n",
    "- üîç **Check guardrails**: Verify limits aren't exceeded\n",
    "- üîß **Review configuration**: Ensure proper lakehouse connections\n",
    "\n",
    "üéØ **Model verification checkpoint**: Confirm your Direct Lake configuration is working correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf138f4-df36-41c4-bb58-a6af71edd179",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=sempy.fabric.evaluate_dax(\n",
    "    dataset=SemanticModelName, \n",
    "    dax_string=\"\"\"\n",
    "    \n",
    "    evaluate tabletraits()\n",
    "    \n",
    "    \"\"\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3394566-f8a8-4ee3-885b-766db5e8a615",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=labs.directlake.get_direct_lake_guardrails()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae67a8-b41c-4c18-8898-aaf4e478e599",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 15. Establish Performance Baseline with DMV Analysis\n",
    "\n",
    "### The Importance of Baseline Measurement\n",
    "Before executing any business queries, it's crucial to establish a **performance baseline**. This initial DMV run shows:\n",
    "\n",
    "- **üß† Initial memory state**: Which columns are already loaded\n",
    "- **üìä Starting temperatures**: Current \"hot\", \"warm\", and \"cold\" column states\n",
    "- **üíæ Memory footprint**: Baseline memory usage before query execution\n",
    "- **üéØ Optimization opportunities**: Identify columns that might need attention\n",
    "\n",
    "### What You'll Observe in the Baseline\n",
    "\n",
    "#### Expected Initial State:\n",
    "Most columns should show:\n",
    "- **‚ùÑÔ∏è TEMPERATURE**: \"COLD\" (not recently accessed)\n",
    "- **üö´ RESIDENT**: \"FALSE\" (not currently in memory)  \n",
    "- **üìÖ LASTACCESSED**: Older timestamps or null values\n",
    "- **üìè SIZE**: Actual dictionary sizes for each column\n",
    "\n",
    "#### Key Columns to Watch:\n",
    "| Table | Column | Expected State | Why |\n",
    "|-------|--------|----------------|-----|\n",
    "| **FactInternetSales** | SalesAmount | COLD | Main measure column |\n",
    "| **DimDate** | Date | COLD | Primary date column |\n",
    "| **DimCustomer** | CustomerKey | COLD | Relationship key |\n",
    "| **DimProduct** | ProductKey | COLD | Relationship key |\n",
    "\n",
    "### DMV Column Analysis\n",
    "\n",
    "#### Understanding the Output:\n",
    "```\n",
    "TABLE               COLUMN          DATATYPE  SIZE   PAGEABLE  RESIDENT  TEMPERATURE\n",
    "FactInternetSales   OrderDateKey    Int64     246    TRUE      FALSE     COLD\n",
    "FactInternetSales   CustomerKey     Int64     7244   TRUE      FALSE     COLD  \n",
    "FactInternetSales   ProductKey      Int64     1064   TRUE      FALSE     COLD\n",
    "FactInternetSales   SalesAmount     Double    8192   TRUE      FALSE     COLD\n",
    "DimCustomer         CustomerKey     Int64     7244   TRUE      FALSE     COLD\n",
    "DimDate             DateKey         Int64     1024   TRUE      FALSE     COLD\n",
    "```\n",
    "\n",
    "### Storage Insights from DMV\n",
    "\n",
    "#### Data Type Optimization:\n",
    "- **Int64**: Efficient for keys and identifiers\n",
    "- **Double**: Precise for currency values\n",
    "- **String**: Variable size for text fields\n",
    "\n",
    "#### Memory Management:\n",
    "- **PAGEABLE=TRUE**: Column can be moved between memory and disk\n",
    "- **RESIDENT=FALSE**: Currently stored on disk, not in memory\n",
    "- **SIZE**: Dictionary compression size (smaller = better compression)\n",
    "\n",
    "### Baseline Benefits for Learning\n",
    "\n",
    "#### Performance Comparison:\n",
    "1. **üìä Before query**: All columns COLD and not resident\n",
    "2. **üî• After query**: Used columns become HOT and resident\n",
    "3. **üìà Delta analysis**: See exact impact of specific queries\n",
    "\n",
    "#### Memory Usage Tracking:\n",
    "- Compare memory usage before and after queries\n",
    "- Understand which columns consume the most memory\n",
    "- Identify optimization opportunities\n",
    "\n",
    "### Expected Outcome\n",
    "You'll see a comprehensive table showing:\n",
    "- ‚úÖ **All model columns** with their current storage states\n",
    "- ‚úÖ **Temperature baseline** (mostly COLD initially)\n",
    "- ‚úÖ **Memory footprint** before any business queries\n",
    "- ‚úÖ **Performance foundation** for comparison analysis\n",
    "\n",
    "üéØ **Performance baseline established**: Ready to analyze query impact on Direct Lake behavior!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223acea6-6ac3-4f3a-b04d-603309daf706",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2dcd3-fe61-4466-b7dd-f746bac1e05a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 16. Execute Business Query and Analyze Direct Lake Performance\n",
    "\n",
    "### The Complete Performance Analysis Workflow\n",
    "This final section demonstrates the full Direct Lake performance analysis cycle:\n",
    "\n",
    "1. **üßπ Clear cache**: Start with clean memory state\n",
    "2. **üìä Execute business query**: Run meaningful DAX analysis  \n",
    "3. **üîç Analyze impact**: See how query execution affects column states\n",
    "\n",
    "### Cache Clearing Strategy\n",
    "```python\n",
    "labs.clear_cache(SemanticModelName)\n",
    "```\n",
    "\n",
    "#### Why Clear Cache First?\n",
    "- **üß† Clean memory state**: Removes any previously loaded columns\n",
    "- **üìä Accurate measurement**: Ensures we see true query impact\n",
    "- **üîÑ Consistent testing**: Provides repeatable performance analysis\n",
    "- **‚ö° Real-world simulation**: Mimics first-time query execution\n",
    "\n",
    "### Business Query Analysis\n",
    "\n",
    "#### DAX Query Breakdown:\n",
    "```dax\n",
    "EVALUATE\n",
    "SUMMARIZECOLUMNS(\n",
    "    DimDate[MonthName],\n",
    "    \"Count of Transactions\", COUNTROWS(FactInternetSales),\n",
    "    \"Sum of Sales\", [Sum of Sales]\n",
    ")\n",
    "ORDER BY [MonthName]\n",
    "```\n",
    "\n",
    "#### Query Components Explained:\n",
    "\n",
    "##### üìÖ **SUMMARIZECOLUMNS()**:\n",
    "- **Purpose**: Creates cross-table aggregations efficiently\n",
    "- **Performance**: Optimized for Direct Lake scenarios\n",
    "- **Flexibility**: Handles multiple measures and dimensions\n",
    "\n",
    "##### üóìÔ∏è **DimDate[MonthName]**:\n",
    "- **Role**: Grouping dimension (shows data by month)\n",
    "- **Sort behavior**: Uses our configured sort-by-column (MonthNumberOfYear)\n",
    "- **Expected impact**: Will make DimDate columns \"HOT\"\n",
    "\n",
    "##### üìä **\"Count of Transactions\"**: \n",
    "- **Formula**: `COUNTROWS(FactInternetSales)`\n",
    "- **Purpose**: Shows transaction volume per month\n",
    "- **Expected impact**: Will load FactInternetSales into memory\n",
    "\n",
    "##### üí∞ **\"Sum of Sales\"**:\n",
    "- **Formula**: Our custom `[Sum of Sales]` measure\n",
    "- **Purpose**: Shows revenue by month  \n",
    "- **Expected impact**: Will make SalesAmount column \"HOT\"\n",
    "\n",
    "##### üìà **ORDER BY [MonthName]**:\n",
    "- **Behavior**: Uses our sort-by-column configuration\n",
    "- **Result**: Chronological month order (Jan, Feb, Mar...)\n",
    "- **User experience**: Intuitive time-series presentation\n",
    "\n",
    "### Expected Business Results\n",
    "The query should return data like:\n",
    "```\n",
    "MonthName    Count of Transactions    Sum of Sales\n",
    "January      1,234                   $456,789.12\n",
    "February     1,567                   $567,890.23  \n",
    "March        1,890                   $678,901.34\n",
    "...\n",
    "```\n",
    "\n",
    "### Performance Impact Analysis\n",
    "\n",
    "#### After Query Execution - Expected Changes:\n",
    "| Table | Column | Before | After | Why |\n",
    "|-------|--------|--------|-------|-----|\n",
    "| **DimDate** | MonthName | COLD | HOT | Used for grouping |\n",
    "| **DimDate** | MonthNumberOfYear | COLD | HOT | Used for sorting |\n",
    "| **FactInternetSales** | SalesAmount | COLD | HOT | Used in Sum measure |\n",
    "| **FactInternetSales** | OrderDateKey | COLD | HOT | Used for relationship |\n",
    "\n",
    "#### Memory Usage Patterns:\n",
    "- **üî• HOT columns**: Recently accessed, kept in memory\n",
    "- **üìà RESIDENT=TRUE**: Columns now loaded in memory\n",
    "- **‚è∞ LASTACCESSED**: Updated to current timestamp\n",
    "- **üíæ Memory increase**: Overall model memory footprint grows\n",
    "\n",
    "### Learning Objectives Achieved\n",
    "\n",
    "#### Direct Lake Behavior Understanding:\n",
    "- ‚úÖ **Real-time data access**: No import delay, immediate results\n",
    "- ‚úÖ **Intelligent caching**: Only needed columns loaded into memory\n",
    "- ‚úÖ **Performance optimization**: Subsequent queries using same columns will be faster\n",
    "- ‚úÖ **Resource efficiency**: Unused columns remain on disk\n",
    "\n",
    "#### Performance Monitoring Mastery:\n",
    "- ‚úÖ **Before/after analysis**: Clear view of query impact\n",
    "- ‚úÖ **Memory optimization**: Understanding of column temperature\n",
    "- ‚úÖ **Cache behavior**: How Direct Lake manages memory\n",
    "- ‚úÖ **Query planning**: Insights for future optimization\n",
    "\n",
    "üéØ **Workshop completion**: You've successfully created, configured, and analyzed a production-ready Direct Lake semantic model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbd807-3013-40a4-b584-fda3428ab4be",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "labs.clear_cache(SemanticModelName)\n",
    "\n",
    "df=sempy.fabric.evaluate_dax(\n",
    "    dataset=SemanticModelName, \n",
    "    dax_string=\"\"\"\n",
    "    \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "               \n",
    "                DimDate[MonthName] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(FactInternetSales) ,\n",
    "                \"Sum of Sales\" , [Sum of Sales] \n",
    "        )\n",
    "        ORDER BY [MonthName]\n",
    "    \"\"\")\n",
    "display(df)\n",
    "\n",
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aec05f",
   "metadata": {},
   "source": [
    "## 17. Clean Up Resources and Session Conclusion\n",
    "\n",
    "### Workshop Summary üéâ\n",
    "Congratulations! You have successfully completed Lab 1 and built a comprehensive Direct Lake semantic model. Here's what you accomplished:\n",
    "\n",
    "#### ‚úÖ **Infrastructure Setup**\n",
    "- Created a lakehouse with proper configuration\n",
    "- Loaded Adventure Works sample data (4 tables, 80K+ rows)\n",
    "- Configured metadata synchronization\n",
    "\n",
    "#### ‚úÖ **Model Development**  \n",
    "- Built a Direct Lake semantic model from lakehouse tables\n",
    "- Established star schema relationships (3 relationships)\n",
    "- Created business measures with proper DAX and formatting\n",
    "\n",
    "#### ‚úÖ **User Experience Optimization**\n",
    "- Configured date table for time intelligence\n",
    "- Set logical column sorting for better visuals\n",
    "- Optimized column visibility for end users\n",
    "\n",
    "#### ‚úÖ **Performance Analysis**\n",
    "- Implemented DMV monitoring for performance insights\n",
    "- Analyzed query execution impact on memory usage  \n",
    "- Established baseline and post-query performance comparison\n",
    "\n",
    "### Key Direct Lake Concepts Learned\n",
    "\n",
    "#### üîÑ **Real-time Analytics**\n",
    "Your model provides immediate access to lakehouse data without import delays or scheduled refreshes.\n",
    "\n",
    "#### ‚ö° **Intelligent Memory Management**\n",
    "Direct Lake automatically loads only the columns needed for your queries, optimizing both performance and resource usage.\n",
    "\n",
    "#### üìä **Enterprise-Ready Design**\n",
    "The star schema design with proper relationships, measures, and formatting provides a foundation for scalable business intelligence.\n",
    "\n",
    "### Next Steps in Your Direct Lake Journey\n",
    "\n",
    "#### üöÄ **Immediate Actions**:\n",
    "- Explore the model in Power BI Desktop or Fabric\n",
    "- Create reports using the measures and relationships you built\n",
    "- Experiment with different DAX queries to see performance patterns\n",
    "\n",
    "#### üìà **Advanced Learning**:\n",
    "- **Lab 2**: Scale to larger datasets and understand big data scenarios\n",
    "- **Lab 3**: Analyze Delta table structure and optimization\n",
    "- **Lab 4**: Explore fallback behaviors and troubleshooting\n",
    "\n",
    "#### üõ†Ô∏è **Production Considerations**:\n",
    "- Security and access control for lakehouse data\n",
    "- Monitoring and alerting for model performance\n",
    "- Governance and lifecycle management\n",
    "\n",
    "### Resource Cleanup Importance\n",
    "The following command stops the Spark session to:\n",
    "- **üí∞ Save costs**: Release compute resources\n",
    "- **üßπ Clean memory**: Free up cluster resources for other users\n",
    "- **‚úÖ Best practice**: Proper session management in Fabric notebooks\n",
    "\n",
    "### Final Thoughts\n",
    "Direct Lake represents a paradigm shift in analytics, providing the **real-time capabilities of DirectQuery** with the **performance benefits of Import mode**. You now have hands-on experience with this powerful technology!\n",
    "\n",
    "üéØ **Ready for the next lab?** Let's explore Direct Lake with big data scenarios!\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Summary\n",
    "\n",
    "### What You Accomplished\n",
    "In this lab, you successfully built a complete Direct Lake semantic model from scratch:\n",
    "\n",
    "- ‚úÖ **Infrastructure Setup**: Created lakehouse and loaded Adventure Works data\n",
    "- ‚úÖ **Model Creation**: Generated semantic model with automatic table discovery\n",
    "- ‚úÖ **Data Modeling**: Established star schema relationships between fact and dimensions\n",
    "- ‚úÖ **Business Logic**: Added essential DAX measures with proper formatting\n",
    "- ‚úÖ **User Experience**: Configured date tables, column sorting, and visibility\n",
    "- ‚úÖ **Performance Validation**: Tested model with business queries and DMV analysis\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "**End-to-End Direct Lake Flow:**\n",
    "```\n",
    "Adventure Works Data ‚Üí Lakehouse (Delta Tables) ‚Üí Direct Lake Model ‚Üí Real-time Analytics\n",
    "        ‚Üì                    ‚Üì                         ‚Üì                    ‚Üì\n",
    "   CSV/Parquet         Delta Format            Semantic Layer        Power BI Reports\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Direct Lake Advantage**: Real-time data access without imports or scheduled refreshes\n",
    "- **Star Schema Power**: Proper relationships enable accurate cross-table analysis\n",
    "- **DAX Measures**: Essential for business metrics - don't rely on raw column values\n",
    "- **User Experience**: Column sorting and hiding improve report usability\n",
    "- **Performance Monitoring**: DMVs provide insights into memory usage and query patterns\n",
    "\n",
    "### Performance Results\n",
    "\n",
    "- **Data Freshness**: Real-time updates as soon as lakehouse data changes\n",
    "- **Query Performance**: Excellent response times with columnar Direct Lake access\n",
    "- **Memory Efficiency**: Only accessed columns loaded into memory (\"column temperature\")\n",
    "- **Resource Optimization**: Minimal compute overhead compared to import models\n",
    "\n",
    "### Technical Skills Gained\n",
    "\n",
    "- **Semantic Link Labs**: Programmatic model creation and management\n",
    "- **TOM (Tabular Object Model)**: Advanced model configuration capabilities\n",
    "- **DMV Analysis**: Understanding Direct Lake memory and performance patterns\n",
    "- **Error Handling**: Robust retry logic for production-ready deployments\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Continue to Lab 2** to learn about:\n",
    "- Working with billion-row datasets\n",
    "- OneLake shortcuts for cross-workspace data access\n",
    "- Direct Lake guardrails and fallback behavior\n",
    "- Advanced performance monitoring for big data scenarios\n",
    "\n",
    "**For Production Deployment:**\n",
    "- Implement proper security and access controls\n",
    "- Set up monitoring and alerting for model performance\n",
    "- Establish governance and lifecycle management processes\n",
    "- Consider refresh automation for supporting data pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
