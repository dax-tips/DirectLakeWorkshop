{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09b6c60-d142-42f2-aa77-b714234d69d3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 1: Create Direct Lake custom semantic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a880ae9-56d7-4aeb-99c1-5455f81bbb0a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "In this lab you will use [Semantic Link Labs](https://github.com/microsoft/semantic-link-labs) to perform the following tasks:\n",
    "\n",
    "- Create a Lakehouse\n",
    "- Create a custom semantic model\n",
    "- Customize the semantic model by: \n",
    "    - Adding measures \n",
    "    - Adding relationsips\n",
    "    - Marking a Date Table\n",
    "    - Setting sortby columns\n",
    "    - Hiding columns\n",
    "- Run DAX Query and DMV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd099e87-89f6-45b2-b286-d7f8a12b6a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Install Semantic Link Labs Python Library\n",
    "This step installs Semantic Link Library which is a Python library design for use in Microsoft Fabric Notebooks.  The library extends the capabilities of [Semantic Link](https://learn.microsoft.com/en-us/fabric/data-science/semantic-link-overview) offering additional functionalities to seamlessly integrate alongside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ed5ca-a6ba-478a-b029-17b6db9b6308",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q --disable-pip-version-check semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc44ce-395c-4db3-8979-b06acf9f8ecf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2. Install Python Libraries\n",
    "This step does the following:\n",
    "- Sets up libraries that will be used iater in the script for various functions related to data processing, manipulation and handling\n",
    "- Creates a populates the following variables\n",
    "    - LakehouseName - Used as the name for the Lakehouse that will be created later in this script\n",
    "    - SemanticModelName = Used as the  name for the Semantic Model that will be created later in this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e841d6-a757-4029-aa71-88d4bd286c30",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as labs\n",
    "from sempy import fabric\n",
    "import sempy\n",
    "import pandas\n",
    "import json\n",
    "import time\n",
    "\n",
    "LakehouseName = \"AdventureWorks\"\n",
    "SemanticModelName = f\"{LakehouseName}_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99f94c-a564-4aca-b670-31da354b7b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3. Create Lakehouse\n",
    "This step uses Semantic Link Labs to:\n",
    "1. Check for the exsistance of a Lakehouse\n",
    "2. If a Lakehouse doesn't exist a new one is created using the name from the LakeHouseName variable created in the preious step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760df625-543c-4289-b6cc-f043290d5879",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "lakehouses=labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "if LakehouseName in lakehouses.values:\n",
    "    lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]\n",
    "else:\n",
    "    lakehouseId = fabric.create_lakehouse(LakehouseName)\n",
    "\n",
    "workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "workspaceName = sempy.fabric.resolve_workspace_name(workspaceId)\n",
    "print(f\"WorkspaceId = {workspaceId}, LakehouseID = {lakehouseId}, Workspace Name = {workspaceName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6e83e-d5fd-4020-9766-c236ef176329",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Copy data from source into local lakehouse\n",
    "The following code cell: \n",
    "1.  Defines a function `loadDataToLakehouse` \n",
    "2.  The function:\n",
    "    a. Transfers data from a specified source table to a target table within a lakehouse environment. \n",
    "    b. Retrieves the workspace ID and lakehouse ID from the lakehouse properties, reads data from the source table, and overwrites the target table with this data.\n",
    "\n",
    "The function is invoked four times to load data from different source tables to their respective target tables:\n",
    "1. \"adw_DimCustomer\" to \"DimCustomer\"\n",
    "2. \"adw_DimDate\" to \"DimDate\"\n",
    "3. \"adw_DimProduct\" to \"DimProduct\"\n",
    "4. \"adw_FactInternetSales\" to \"FactInternetSales\"\n",
    "\n",
    "Finally, the code prints \"Done\" to indicate the completion of the data loading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28505f17-8bf7-4400-a409-344c4d01b4ef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "capacity_name = labs.get_capacity_name()\n",
    "\n",
    "def loadDataToLakehouse(fromTable:str,toTable:str):\n",
    "    workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "    lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]\n",
    "\n",
    "    #North Central US\n",
    "    conn_str = \"abfss://16cf855f-3bf4-4312-a7a1-ccf5cb6a0121@onelake.dfs.fabric.microsoft.com/99ed86df-13d1-4008-a7f6-5768e53f4f85/Tables\"\n",
    "    if capacity_name == \"FabConUS8-P1\": #West US 3\n",
    "        conn_str = \"abfss://b1d61bbe-de20-4d3a-8075-b8e2eaacb868@onelake.dfs.fabric.microsoft.com/631e45c0-1243-4f42-920a-56bfe6ecdd6d/Tables\"\n",
    "\n",
    "    customer_df =spark.read.load(f\"{conn_str}/{fromTable}\")\n",
    "    customer_df.write.mode(\"overwrite\").save(f\"abfss://{workspaceId}@onelake.dfs.fabric.microsoft.com/{lakehouseId}/Tables/{toTable}\")\n",
    "    print(f\"Loaded {toTable}\")\n",
    "\n",
    "loadDataToLakehouse(\"adw_DimCustomer\"       ,\"DimCustomer\")\n",
    "loadDataToLakehouse(\"adw_DimDate\"           ,\"DimDate\")\n",
    "loadDataToLakehouse(\"adw_DimProduct\"        ,\"DimProduct\")\n",
    "loadDataToLakehouse(\"adw_FactInternetSales\" ,\"FactInternetSales\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff2996-0b1c-4792-99cd-8d61315e65da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 5. Trigger background job to sync Lakehouse tables\n",
    "The following code cell creates the function `triggerMetadataRefresh` that automatically generates the schema in the SQL analytics endpoint.  The function does the following:\n",
    "1. It creates a FabricRestClient instance to interact with the API.\n",
    "2. It retrieves metadata for a specific lakehouse within a workspace using their respective IDs.\n",
    "3. It extracts the SQL endpoint property ID from the response.\n",
    "4. It triggers a metadata refresh on the lakehouse by sending a POST request with the necessary payload.\n",
    "5. It monitors the progress of the metadata refresh by repeatedly querying the status until it succeeds.\n",
    "6. It prints the progress state at each step and indicates when the process is complete.\n",
    "7. Finally, it calls the `triggerMetadataRefresh` function to execute the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54e1aa-a94e-4efd-b40e-f497f54589e8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "##https://medium.com/@sqltidy/delays-in-the-automatically-generated-schema-in-the-sql-analytics-endpoint-of-the-lakehouse-b01c7633035d\n",
    "\n",
    "def triggerMetadataRefresh():\n",
    "    client = fabric.FabricRestClient()\n",
    "    response = client.get(f\"/v1/workspaces/{workspaceId}/lakehouses/{lakehouseId}\")\n",
    "    sqlendpoint = response.json()['properties']['sqlEndpointProperties']['id']\n",
    "\n",
    "    # trigger sync\n",
    "    uri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}\"\n",
    "    payload = {\"commands\":[{\"$type\":\"MetadataRefreshExternalCommand\"}]}\n",
    "    response = client.post(uri,json= payload)\n",
    "    batchId = response.json()['batchId']\n",
    "\n",
    "    # Monitor Progress\n",
    "    statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "    statusresponsedata = client.get(statusuri).json()\n",
    "    progressState = statusresponsedata['progressState']\n",
    "    print(f\"Metadata refresh : {progressState}\")\n",
    "    while progressState != \"success\":\n",
    "        statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "        statusresponsedata = client.get(statusuri).json()\n",
    "        progressState = statusresponsedata['progressState']\n",
    "        print(progressState)\n",
    "        time.sleep(1)\n",
    "\n",
    "    print('Metadata refresh complete')\n",
    "\n",
    "triggerMetadataRefresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e16b1-9893-46d3-ac7d-93a4d1453c5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 6. Create Custom Semantic Model from Lakehouse\n",
    "The following code cell:\n",
    "1.  Creates a list variable and assigns a list of tables from the Lakehouse to the variable\n",
    "2.  Checks for the existance of a Semantic Model\n",
    "3.  If the Semantic Model does not exist, the model is create and all the tables from the preiously create varaible is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e9cc7-de53-4c81-8408-9fb18ef4383a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sempy import fabric\n",
    "\n",
    "#1. Generate list of ALL table names from lakehouse to add to Semantic Model\n",
    "lakehouseTables:list = labs.lakehouse.get_lakehouse_tables(lakehouse=LakehouseName)[\"Table Name\"]\n",
    "\n",
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        #2 Create the semantic model\n",
    "        if sempy.fabric.list_items().query(f\"`Display Name`=='{LakehouseName}_model' & Type=='SemanticModel'  \").shape[0] ==0:\n",
    "            labs.directlake.generate_direct_lake_semantic_model(dataset=f\"{LakehouseName}_model\",lakehouse_tables=lakehouseTables,workspace=workspaceName,lakehouse=lakehouseId,refresh=False,overwrite=True)\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error creating model... trying again.')\n",
    "        time.sleep(3)\n",
    "        triggerMetadataRefresh()\n",
    "\n",
    "print('Semantic model created OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df3373-52ca-4e45-83a6-40e1b20c184d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 7. Add model relationships\n",
    "The code block \n",
    "1. opens a connection to a semantic model.\n",
    "2. Removes any existing relationships in the model.\n",
    "3. Adds new relationships between tables in the model:\n",
    "- Links \"FactInternetSales.OrderDateKey\" to \"DimDate.DateKey\" with a many-to-one cardinality.\n",
    "- Links \"FactInternetSales.CustomerKey\" to \"DimCustomer.CustomerKey\" with a many-to-one cardinality.\n",
    "- Links \"FactInternetSales.ProductKey\" to \"DimProduct.ProductKey\" with a many-to-one cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616193a-57df-4139-91ef-c73830331555",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing relationships\n",
    "            for r in tom.model.Relationships:\n",
    "                tom.model.Relationships.Remove(r)\n",
    "\n",
    "            #2. Creates correct relationships\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"OrderDateKey\" , to_table=\"DimDate\"    , to_column=\"DateKey\"       , from_cardinality=\"many\" , to_cardinality=\"one\")\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"CustomerKey\"  , to_table=\"DimCustomer\", to_column=\"CustomerKey\"   , from_cardinality=\"many\" , to_cardinality=\"one\")\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"ProductKey\"   , to_table=\"DimProduct\" , to_column=\"ProductKey\"    , from_cardinality=\"many\" , to_cardinality=\"one\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding relationships... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba5536-018c-402b-aa0f-663ee5e8d07f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 8. Add model measures\n",
    "The code block \n",
    "1. Opens a connection to a semantic model.\n",
    "2. Removes any existing measures in the model.\n",
    "3. Adds a new measure Sum Of Sales to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666631c-4221-4638-b8ff-10fee8a4f7df",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing measures\n",
    "            for t in tom.model.Tables:\n",
    "                for m in t.Measures:\n",
    "                    tom.remove_object(m)\n",
    "                    print(f\"[{m.Name}] measure removed\")\n",
    "\n",
    "            tom.add_measure(table_name=\"FactInternetSales\" ,measure_name=\"Sum of Sales\",expression=\"SUM(FactInternetSales[SalesAmount])\",format_string=\"\\$#,0.###############;(\\$#,0.###############);\\$#,0.###############\")\n",
    "            tom.add_measure(table_name=\"FactInternetSales\" ,measure_name=\"Count of Sales\",expression=\"COUNTROWS(FactInternetSales)\",format_string=\"#,0\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding measures... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ff91f-0362-4c3e-bf2e-60c64867cec8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 9. Mark DimDate as Date Table\n",
    "This code block:\n",
    "\n",
    "1.  Opens connection to semantic model.\n",
    "2.  Marks DimDate table as Date Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f48bf-893b-4a8f-92a1-e5e80b968840",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            tom.mark_as_date_table(table_name=\"DimDate\",column_name=\"Date\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error with date table... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb148df-8490-47fc-8e9c-6b8cbcb5fb6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 10. Set Sort by Cols\n",
    "This code block:\n",
    "\n",
    "1. Imports and uses the json library\n",
    "2. Sets the sorting order for columns \"MonthName\" and \"DayOfWeek\" in the \"DimDate\" table using the columns \"MonthNumberOfYear\" and \"DayNumberOfWeek\" respectively.\n",
    "3. Saves the changes to the model.\n",
    "4. Iterates through the tables in the model to find the \"DimDate\" table.\n",
    "5. Once the \"DimDate\" table is found, it converts its structure to a JSON format and prints it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468514f8-1e73-4732-a22f-b0ce5f3c8eea",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "tom = labs.tom.TOMWrapper(dataset=SemanticModelName, workspace=workspaceName, readonly=False)\n",
    "tom.set_sort_by_column(table_name=\"DimDate\",column_name=\"MonthName\"       ,sort_by_column=\"MonthNumberOfYear\")\n",
    "tom.set_sort_by_column(table_name=\"DimDate\",column_name=\"DayOfWeek\"       ,sort_by_column=\"DayNumberOfWeek\")\n",
    "tom.model.SaveChanges()\n",
    "\n",
    "i:int=0\n",
    "for t in tom.model.Tables:\n",
    "    if t.Name==\"DimDate\":\n",
    "        bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "        print(bim)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7babd-5053-4dc5-97e1-4398e67dba26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 11. Hide Fact Table columns\n",
    "This code block:\n",
    "1. Iterates through all tables in the `tom.model.Tables` collection. \n",
    "2. For the table named \"FactInternetSales\", it sets the `IsHidden` property of each column to `True`. \n",
    "3. It then converts the table's information to a JSON format and prints it. The index `i` is incremented after processing each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dbe0c-7d08-4c11-963f-07b44c20f401",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "i:int=0\n",
    "for t in tom.model.Tables:\n",
    "    if t.Name in [\"FactInternetSales\"]:\n",
    "        for c in t.Columns:\n",
    "            c.IsHidden=True\n",
    "\n",
    "        bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "        print(bim)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc341d-8682-4a4d-8b00-300d8723bcfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 12. Reframe model to update changes\n",
    "This code block attempts to reframe the Semantic model in a loop until successful, catching exceptions and retrying every 3 seconds.  Upon, success, it prints a confirmation message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed6756-a8b0-4250-9062-3266ae563054",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "reframeOK:bool=False\n",
    "while not reframeOK:\n",
    "    try:\n",
    "        result:pandas.DataFrame = labs.refresh_semantic_model(dataset=SemanticModelName)\n",
    "        reframeOK=True\n",
    "    except:\n",
    "        print('Error with reframe... trying again.')\n",
    "        triggerMetadataRefresh()\n",
    "        time.sleep(3)\n",
    "\n",
    "print('Custom Semantic Model reframe OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ccce3",
   "metadata": {},
   "source": [
    "## 13. Create function to run DMV\n",
    "This code cell:\n",
    "1. Import neccessary modules that will be used later in the code\n",
    "2. Defines a function that executes a DAX Query \n",
    "3. Defines another function that captures server timings based on the query execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab34d2-784f-4a4a-bf61-afdabb4c2b69",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066ff6f-7914-447d-8c02-52840649358c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 14. DAX Queries\n",
    "The following code block executes a DAX query that returns the results of the useful tabletraits() DAX function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf138f4-df36-41c4-bb58-a6af71edd179",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=sempy.fabric.evaluate_dax(\n",
    "    dataset=SemanticModelName, \n",
    "    dax_string=\"\"\"\n",
    "    \n",
    "    evaluate tabletraits()\n",
    "    \n",
    "    \"\"\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3394566-f8a8-4ee3-885b-766db5e8a615",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=labs.directlake.get_direct_lake_guardrails()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae67a8-b41c-4c18-8898-aaf4e478e599",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 15. Run DMV to check column details\n",
    "This code cell executes a function that was created earlier in the notebook, which executes a DMV function to describe the state of our custom semantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223acea6-6ac3-4f3a-b04d-603309daf706",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2dcd3-fe61-4466-b7dd-f746bac1e05a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 16. Run DAX Query on custom semantic model\n",
    "The code cell:\n",
    "1.  Clears the semantic model cache\n",
    "2.  Executes a DAX query and displays the results\n",
    "3.  Re-runs the DMV from step 16 so we can compare the before/after to see the effect of running a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbd807-3013-40a4-b584-fda3428ab4be",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "labs.clear_cache(SemanticModelName)\n",
    "\n",
    "df=sempy.fabric.evaluate_dax(\n",
    "    dataset=SemanticModelName, \n",
    "    dax_string=\"\"\"\n",
    "    \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "               \n",
    "                DimDate[MonthName] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(FactInternetSales) ,\n",
    "                \"Sum of Sales\" , [Sum of Sales] \n",
    "        )\n",
    "        ORDER BY [MonthName]\n",
    "    \"\"\")\n",
    "display(df)\n",
    "\n",
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aec05f",
   "metadata": {},
   "source": [
    "## 17. Stop the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
