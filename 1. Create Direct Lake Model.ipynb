{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09b6c60-d142-42f2-aa77-b714234d69d3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 1: Create a Direct Lake Semantic Model\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lab walks through the complete process of building a Direct Lake semantic model from scratch using Microsoft Fabric. Starting from an empty lakehouse, you will load sample data, create a semantic model, define relationships and measures, and validate the model using DMV queries.\n",
    "\n",
    "### Workshop Flow\n",
    "\n",
    "1. Create a lakehouse and load Adventure Works sample data\n",
    "2. Create a Direct Lake semantic model from the lakehouse tables\n",
    "3. Define star schema relationships between fact and dimension tables\n",
    "4. Add business measures and configure the model for reporting\n",
    "5. Validate model behaviour using DAX queries and DMV analysis\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Direct Lake** allows Power BI to query data directly from Delta Lake files without importing data into the model\n",
    "- **Adventure Works** is a sample business dataset containing customers, products, dates, and internet sales\n",
    "- **Semantic model** is the business logic layer that sits on top of the data, providing relationships, measures, and formatting\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Set up a Fabric lakehouse and load sample data into Delta tables\n",
    "- Create a Direct Lake semantic model programmatically using Semantic Link Labs\n",
    "- Define table relationships and business measures\n",
    "- Use DMV queries to monitor column loading and memory usage\n",
    "\n",
    "**Estimated duration:** 30-45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "*Deutsche Version:*\n",
    "\n",
    "# Lab 1: Ein Direct Lake Semantic Model erstellen\n",
    "\n",
    "## Uebersicht\n",
    "\n",
    "In diesem Lab wird der vollstaendige Prozess zum Erstellen eines Direct Lake Semantic Models von Grund auf mit Microsoft Fabric durchlaufen. Ausgehend von einem leeren Lakehouse laden Sie Beispieldaten, erstellen ein Semantic Model, definieren Beziehungen und Measures und validieren das Modell mithilfe von DMV-Abfragen.\n",
    "\n",
    "### Ablauf des Workshops\n",
    "\n",
    "1. Ein Lakehouse erstellen und Adventure Works-Beispieldaten laden\n",
    "2. Ein Direct Lake Semantic Model aus den Lakehouse-Tabellen erstellen\n",
    "3. Sternschema-Beziehungen zwischen Fakten- und Dimensionstabellen definieren\n",
    "4. Geschaeftskennzahlen hinzufuegen und das Modell fuer das Reporting konfigurieren\n",
    "5. Modellverhalten mithilfe von DAX-Abfragen und DMV-Analyse validieren\n",
    "\n",
    "### Wichtige Konzepte\n",
    "\n",
    "- **Direct Lake** ermoeglicht es Power BI, Daten direkt aus Delta Lake-Dateien abzufragen, ohne Daten in das Modell zu importieren\n",
    "- **Adventure Works** ist ein Beispiel-Geschaeftsdatensatz mit Kunden, Produkten, Daten und Internetverkaeufen\n",
    "- **Semantic Model** ist die Geschaeftslogikschicht ueber den Daten, die Beziehungen, Measures und Formatierungen bereitstellt\n",
    "\n",
    "### Lernziele\n",
    "\n",
    "- Ein Fabric Lakehouse einrichten und Beispieldaten in Delta-Tabellen laden\n",
    "- Ein Direct Lake Semantic Model programmatisch mit Semantic Link Labs erstellen\n",
    "- Tabellenbeziehungen und Geschaeftskennzahlen definieren\n",
    "- DMV-Abfragen verwenden, um das Laden von Spalten und die Speichernutzung zu ueberwachen\n",
    "\n",
    "**Geschaetzte Dauer:** 30-45 Minuten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd099e87-89f6-45b2-b286-d7f8a12b6a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "Install the Semantic Link Labs library, which provides functions for creating and managing Direct Lake semantic models in Microsoft Fabric.\n",
    "\n",
    "---\n",
    "\n",
    "*Installieren Sie die Semantic Link Labs-Bibliothek, die Funktionen zum Erstellen und Verwalten von Direct Lake Semantic Models in Microsoft Fabric bereitstellt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ed5ca-a6ba-478a-b029-17b6db9b6308",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc44ce-395c-4db3-8979-b06acf9f8ecf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Import Libraries and Set Variables\n",
    "\n",
    "Import the required Python libraries and define variables for the lakehouse name and semantic model name used throughout this lab.\n",
    "\n",
    "---\n",
    "\n",
    "*Importieren Sie die erforderlichen Python-Bibliotheken und definieren Sie Variablen fuer den Lakehouse-Namen und den Semantic Model-Namen, die in diesem Lab verwendet werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e841d6-a757-4029-aa71-88d4bd286c30",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as labs\n",
    "from sempy import fabric\n",
    "import sempy\n",
    "import pandas\n",
    "import json\n",
    "import time\n",
    "\n",
    "LakehouseName = \"AdventureWorks\"\n",
    "SemanticModelName = f\"{LakehouseName}_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99f94c-a564-4aca-b670-31da354b7b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Create or Connect to Lakehouse\n",
    "\n",
    "Check whether the AdventureWorks lakehouse already exists. If not, create it. Then retrieve the workspace and lakehouse identifiers needed for subsequent steps.\n",
    "\n",
    "---\n",
    "\n",
    "*Pruefen Sie, ob das AdventureWorks Lakehouse bereits existiert. Falls nicht, erstellen Sie es. Rufen Sie dann die Workspace- und Lakehouse-Kennungen ab, die fuer die folgenden Schritte benoetigt werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760df625-543c-4289-b6cc-f043290d5879",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "lakehouses=labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "if LakehouseName in lakehouses.values:\n",
    "    lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]\n",
    "else:\n",
    "    lakehouseId = fabric.create_lakehouse(LakehouseName)\n",
    "\n",
    "workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "workspaceName = sempy.fabric.resolve_workspace_name(workspaceId)\n",
    "print(f\"WorkspaceId = {workspaceId}, LakehouseID = {lakehouseId}, Workspace Name = {workspaceName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6e83e-d5fd-4020-9766-c236ef176329",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 4: Load Adventure Works Sample Data\n",
    "\n",
    "Load four Adventure Works tables into the lakehouse: DimCustomer, DimDate, DimProduct, and FactInternetSales. The data is sourced from region-aware endpoints and written in Delta format using overwrite mode to ensure a clean starting state.\n",
    "\n",
    "**Tables loaded:**\n",
    "| Table | Approximate Rows |\n",
    "|:------|:-----------------|\n",
    "| DimCustomer | 18,000 |\n",
    "| DimDate | 2,500 |\n",
    "| DimProduct | 600 |\n",
    "| FactInternetSales | 60,000 |\n",
    "\n",
    "**Expected output:** Four \"Loaded\" messages followed by \"Done\".\n",
    "\n",
    "---\n",
    "\n",
    "*Laden Sie vier Adventure Works-Tabellen in das Lakehouse: DimCustomer, DimDate, DimProduct und FactInternetSales. Die Daten stammen von regionsbewussten Endpunkten und werden im Delta-Format im Ueberschreibmodus geschrieben, um einen sauberen Ausgangszustand sicherzustellen.*\n",
    "\n",
    "*Erwartete Ausgabe: Vier \"Loaded\"-Meldungen gefolgt von \"Done\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28505f17-8bf7-4400-a409-344c4d01b4ef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "capacity_name = labs.get_capacity_name()\n",
    "\n",
    "def loadDataToLakehouse(fromTable: str, toTable: str):\n",
    "    \"\"\"\n",
    "    Optimized data loading function with improved error handling and performance.\n",
    "    \n",
    "    Args:\n",
    "        fromTable: Source table name to read from\n",
    "        toTable: Target table name to write to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get lakehouse properties once and reuse\n",
    "        lakehouse_props = notebookutils.lakehouse.getWithProperties(LakehouseName)\n",
    "        workspaceId = lakehouse_props[\"workspaceId\"]\n",
    "        lakehouseId = lakehouse_props[\"id\"]\n",
    "\n",
    "        # Region-aware connection string selection\n",
    "        if capacity_name == \"FabConUS8-P1\":  # West US 3\n",
    "            conn_str = \"abfss://b1d61bbe-de20-4d3a-8075-b8e2eaacb868@onelake.dfs.fabric.microsoft.com/631e45c0-1243-4f42-920a-56bfe6ecdd6d/Tables\"\n",
    "        else:  # North Central US (default)\n",
    "            conn_str = \"abfss://16cf855f-3bf4-4312-a7a1-ccf5cb6a0121@onelake.dfs.fabric.microsoft.com/99ed86df-13d1-4008-a7f6-5768e53f4f85/Tables\"\n",
    "\n",
    "        # Read source data with format specification for better performance\n",
    "        customer_df = spark.read.format(\"delta\").load(f\"{conn_str}/{fromTable}\")\n",
    "        \n",
    "        # Cache the DataFrame if it will be used multiple times or is computation-heavy\n",
    "        customer_df.cache()\n",
    "        \n",
    "        # Write with optimized settings\n",
    "        (customer_df\n",
    "         .write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .save(f\"abfss://{workspaceId}@onelake.dfs.fabric.microsoft.com/{lakehouseId}/Tables/{toTable}\"))\n",
    "        \n",
    "        # Unpersist cached DataFrame to free memory\n",
    "        customer_df.unpersist()\n",
    "        \n",
    "        print(f\"Loaded {toTable}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {toTable}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load all tables with proper error handling\n",
    "tables_to_load = [\n",
    "    (\"DimCustomer\", \"DimCustomer\"),\n",
    "    (\"DimDate\", \"DimDate\"),\n",
    "    (\"DimProduct\", \"DimProduct\"),\n",
    "    (\"FactInternetSales\", \"FactInternetSales\")\n",
    "]\n",
    "\n",
    "for from_table, to_table in tables_to_load:\n",
    "    loadDataToLakehouse(from_table, to_table)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff2996-0b1c-4792-99cd-8d61315e65da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 5: Trigger Metadata Synchronisation\n",
    "\n",
    "Force a synchronisation between the lakehouse storage layer and the SQL Analytics Endpoint. This ensures that the table schemas are up to date before creating the semantic model.\n",
    "\n",
    "---\n",
    "\n",
    "*Erzwingen Sie eine Synchronisation zwischen der Lakehouse-Speicherschicht und dem SQL Analytics Endpoint. Dadurch wird sichergestellt, dass die Tabellenschemata aktuell sind, bevor das Semantic Model erstellt wird.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54e1aa-a94e-4efd-b40e-f497f54589e8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "##https://medium.com/@sqltidy/delays-in-the-automatically-generated-schema-in-the-sql-analytics-endpoint-of-the-lakehouse-b01c7633035d\n",
    "\n",
    "def triggerMetadataRefresh():\n",
    "    client = fabric.FabricRestClient()\n",
    "    response = client.get(f\"/v1/workspaces/{workspaceId}/lakehouses/{lakehouseId}\")\n",
    "    sqlendpoint = response.json()['properties']['sqlEndpointProperties']['id']\n",
    "\n",
    "    # trigger sync\n",
    "    uri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}\"\n",
    "    payload = {\"commands\":[{\"$type\":\"MetadataRefreshExternalCommand\"}]}\n",
    "    response = client.post(uri,json= payload)\n",
    "    batchId = response.json()['batchId']\n",
    "\n",
    "    # Monitor Progress\n",
    "    statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "    statusresponsedata = client.get(statusuri).json()\n",
    "    progressState = statusresponsedata['progressState']\n",
    "    print(f\"Metadata refresh : {progressState}\")\n",
    "    while progressState != \"success\":\n",
    "        statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "        statusresponsedata = client.get(statusuri).json()\n",
    "        progressState = statusresponsedata['progressState']\n",
    "        print(f\"Metadata refresh : {progressState}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    print('Metadata refresh complete')\n",
    "\n",
    "triggerMetadataRefresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e16b1-9893-46d3-ac7d-93a4d1453c5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 6: Create the Direct Lake Semantic Model\n",
    "\n",
    "Generate a new Direct Lake semantic model from the lakehouse tables. The model is created using the DL/SQL connection type, which references the SQL Analytics Endpoint.\n",
    "\n",
    "---\n",
    "\n",
    "*Erstellen Sie ein neues Direct Lake Semantic Model aus den Lakehouse-Tabellen. Das Modell wird mit dem DL/SQL-Verbindungstyp erstellt, der auf den SQL Analytics Endpoint verweist.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e9cc7-de53-4c81-8408-9fb18ef4383a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sempy import fabric\n",
    "\n",
    "#1. Generate list of ALL table names from lakehouse to add to Semantic Model\n",
    "lakehouseTables:list = labs.lakehouse.get_lakehouse_tables(lakehouse=LakehouseName)[\"Table Name\"]\n",
    "\n",
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        #2 Create the semantic model\n",
    "        if sempy.fabric.list_items().query(f\"`Display Name`=='{LakehouseName}_model' & Type=='SemanticModel'  \").shape[0] ==0:\n",
    "            labs.directlake.generate_direct_lake_semantic_model(dataset=f\"{LakehouseName}_model\",lakehouse_tables=lakehouseTables,workspace=workspaceName,lakehouse=lakehouseId,refresh=False,overwrite=True)\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error creating model... trying again.')\n",
    "        time.sleep(3)\n",
    "        triggerMetadataRefresh()\n",
    "\n",
    "print('Semantic model created OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df3373-52ca-4e45-83a6-40e1b20c184d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 7: Configure Table Relationships\n",
    "\n",
    "Define star schema relationships linking the fact table (FactInternetSales) to its dimension tables (DimCustomer, DimDate, DimProduct). These relationships enable cross-table filtering in DAX queries.\n",
    "\n",
    "---\n",
    "\n",
    "*Definieren Sie Sternschema-Beziehungen, die die Faktentabelle (FactInternetSales) mit ihren Dimensionstabellen (DimCustomer, DimDate, DimProduct) verknuepfen. Diese Beziehungen ermoeglichen tabellenuebergreifendes Filtern in DAX-Abfragen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616193a-57df-4139-91ef-c73830331555",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing relationships\n",
    "            for r in tom.model.Relationships:\n",
    "                tom.model.Relationships.Remove(r)\n",
    "\n",
    "            #2. Creates correct relationships\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"OrderDateKey\" , to_table=\"DimDate\"    , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"CustomerKey\"  , to_table=\"DimCustomer\", to_column=\"CustomerKey\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"FactInternetSales\", from_column=\"ProductKey\"   , to_table=\"DimProduct\" , to_column=\"ProductKey\"    , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding relationships... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba5536-018c-402b-aa0f-663ee5e8d07f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 8: Add Business Measures\n",
    "\n",
    "Create a set of DAX measures (such as Total Sales, Order Count, and Average Order Value) to provide meaningful aggregations for reporting and validation later in this lab.\n",
    "\n",
    "---\n",
    "\n",
    "*Erstellen Sie eine Reihe von DAX-Measures (wie Gesamtumsatz, Bestellanzahl und durchschnittlicher Bestellwert), um aussagekraeftige Aggregationen fuer das Reporting und die spaetere Validierung in diesem Lab bereitzustellen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666631c-4221-4638-b8ff-10fee8a4f7df",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing measures\n",
    "            for t in tom.model.Tables:\n",
    "                for m in t.Measures:\n",
    "                    tom.remove_object(m)\n",
    "                    print(f\"[{m.Name}] measure removed\")\n",
    "\n",
    "            tom.add_measure(table_name=\"FactInternetSales\" ,measure_name=\"Sum of Sales\",expression=\"SUM(FactInternetSales[SalesAmount])\",format_string=\"\\$#,0.###############;(\\$#,0.###############);\\$#,0.###############\")\n",
    "            tom.add_measure(table_name=\"FactInternetSales\" ,measure_name=\"Count of Sales\",expression=\"COUNTROWS(FactInternetSales)\",format_string=\"#,0\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding measures... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ff91f-0362-4c3e-bf2e-60c64867cec8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 9. Configure Date Table for Time Intelligence\n",
    "\n",
    "Marks DimDate table as date table to enable time-based analysis functions and calendar features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f48bf-893b-4a8f-92a1-e5e80b968840",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            tom.mark_as_date_table(table_name=\"DimDate\",column_name=\"Date\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error with date table... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb148df-8490-47fc-8e9c-6b8cbcb5fb6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 10: Configure Column Sorting\n",
    "\n",
    "Set sort-by-column properties on date table columns so that month names and other textual date fields appear in chronological order rather than alphabetical order in visualisations.\n",
    "\n",
    "---\n",
    "\n",
    "*Legen Sie Sortierungseigenschaften fuer Datumstabellenspalten fest, damit Monatsnamen und andere textuelle Datumsfelder in Visualisierungen in chronologischer statt alphabetischer Reihenfolge angezeigt werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468514f8-1e73-4732-a22f-b0ce5f3c8eea",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "tom = labs.tom.TOMWrapper(dataset=SemanticModelName, workspace=workspaceName, readonly=False)\n",
    "tom.set_sort_by_column(table_name=\"DimDate\",column_name=\"MonthName\"       ,sort_by_column=\"MonthNumberOfYear\")\n",
    "tom.set_sort_by_column(table_name=\"DimDate\",column_name=\"DayOfWeek\"       ,sort_by_column=\"DayNumberOfWeek\")\n",
    "tom.model.SaveChanges()\n",
    "\n",
    "i:int=0\n",
    "for t in tom.model.Tables:\n",
    "    if t.Name==\"DimDate\":\n",
    "        bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "        print(bim)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7babd-5053-4dc5-97e1-4398e67dba26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 9: Configure Date Table for Time Intelligence\n",
    "\n",
    "Mark the DimDate table as a date table. This enables DAX time intelligence functions such as year-over-year comparisons and running totals.\n",
    "\n",
    "---\n",
    "\n",
    "*Markieren Sie die DimDate-Tabelle als Datumstabelle. Dies aktiviert DAX-Zeitintelligenzfunktionen wie Jahresvergleiche und laufende Summen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dbe0c-7d08-4c11-963f-07b44c20f401",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "i:int=0\n",
    "for t in tom.model.Tables:\n",
    "    if t.Name in [\"FactInternetSales\"]:\n",
    "        for c in t.Columns:\n",
    "            c.IsHidden=True\n",
    "\n",
    "        bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "        print(bim)\n",
    "    i=i+1\n",
    "    \n",
    "tom.model.SaveChanges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc341d-8682-4a4d-8b00-300d8723bcfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 11: Hide Fact Table Columns\n",
    "\n",
    "Hide the raw columns on the fact table to guide report authors towards using the defined measures instead of dragging raw columns into visualisations.\n",
    "\n",
    "---\n",
    "\n",
    "*Blenden Sie die Rohspalten der Faktentabelle aus, um Berichtsautoren dazu zu leiten, die definierten Measures zu verwenden, anstatt Rohspalten in Visualisierungen zu ziehen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed6756-a8b0-4250-9062-3266ae563054",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "reframeOK:bool=False\n",
    "while not reframeOK:\n",
    "    try:\n",
    "        result:pandas.DataFrame = labs.refresh_semantic_model(dataset=SemanticModelName)\n",
    "        reframeOK=True\n",
    "    except:\n",
    "        print('Error with reframe... trying again.')\n",
    "        triggerMetadataRefresh()\n",
    "        time.sleep(3)\n",
    "\n",
    "print('Custom Semantic Model reframe OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ccce3",
   "metadata": {},
   "source": [
    "## Step 12: Frame the Semantic Model\n",
    "\n",
    "Trigger framing on the semantic model so that it is ready to serve queries. Framing establishes the initial connection between the model and the underlying Delta tables.\n",
    "\n",
    "---\n",
    "\n",
    "*Loesen Sie das Framing des Semantic Models aus, damit es bereit ist, Abfragen zu bedienen. Framing stellt die initiale Verbindung zwischen dem Modell und den zugrunde liegenden Delta-Tabellen her.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab34d2-784f-4a4a-bf61-afdabb4c2b69",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066ff6f-7914-447d-8c02-52840649358c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 13: Set Up DMV Monitoring Function\n",
    "\n",
    "Define a helper function that queries Dynamic Management Views (DMVs) to inspect column temperature and memory usage. This function will be used in the following steps to observe how Direct Lake loads data on demand.\n",
    "\n",
    "---\n",
    "\n",
    "*Definieren Sie eine Hilfsfunktion, die Dynamic Management Views (DMVs) abfragt, um Spaltentemperatur und Speichernutzung zu ueberpruefen. Diese Funktion wird in den folgenden Schritten verwendet, um zu beobachten, wie Direct Lake Daten bei Bedarf laedt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf138f4-df36-41c4-bb58-a6af71edd179",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=sempy.fabric.evaluate_dax(\n",
    "    dataset=SemanticModelName, \n",
    "    dax_string=\"\"\"\n",
    "    \n",
    "    evaluate tabletraits()\n",
    "    \n",
    "    \"\"\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3394566-f8a8-4ee3-885b-766db5e8a615",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "## Step 14: Explore Direct Lake Table Traits and Guardrails\n",
    "\n",
    "Use the TABLETRAITS() DAX function and guardrails queries to verify that the model is operating in Direct Lake mode and to review the applicable capacity limits.\n",
    "\n",
    "---\n",
    "\n",
    "*Verwenden Sie die DAX-Funktion TABLETRAITS() und Guardrails-Abfragen, um zu ueberpruefen, ob das Modell im Direct Lake-Modus arbeitet, und um die geltenden Kapazitaetsgrenzen zu pruefen.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae67a8-b41c-4c18-8898-aaf4e478e599",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 15: Establish a Performance Baseline with DMV Analysis\n",
    "\n",
    "Capture the current state of Direct Lake columns and memory usage before running any queries. This provides a baseline to compare against after query execution.\n",
    "\n",
    "---\n",
    "\n",
    "*Erfassen Sie den aktuellen Zustand der Direct Lake-Spalten und der Speichernutzung, bevor Abfragen ausgefuehrt werden. Dies bietet eine Basislinie fuer den Vergleich nach der Abfrageausfuehrung.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223acea6-6ac3-4f3a-b04d-603309daf706",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2dcd3-fe61-4466-b7dd-f746bac1e05a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 16: Execute a DAX Query and Monitor Column Loading\n",
    "\n",
    "Run a DAX query against the model and then use the DMV monitoring function to observe which columns were loaded into memory. This demonstrates Direct Lake's on-demand column loading behaviour.\n",
    "\n",
    "---\n",
    "\n",
    "*Fuehren Sie eine DAX-Abfrage gegen das Modell aus und verwenden Sie dann die DMV-Ueberwachungsfunktion, um zu beobachten, welche Spalten in den Speicher geladen wurden. Dies demonstriert das bedarfsgesteuerte Spaltenladeverhalten von Direct Lake.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbd807-3013-40a4-b584-fda3428ab4be",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "labs.clear_cache(SemanticModelName)\n",
    "\n",
    "df=sempy.fabric.evaluate_dax(\n",
    "    dataset=SemanticModelName, \n",
    "    dax_string=\"\"\"\n",
    "    \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "               \n",
    "                DimDate[MonthName] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(FactInternetSales) ,\n",
    "                \"Sum of Sales\" , [Sum of Sales] \n",
    "        )\n",
    "        ORDER BY [MonthName]\n",
    "    \"\"\")\n",
    "display(df)\n",
    "\n",
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca69f26",
   "metadata": {},
   "source": [
    "## Step 17: Stop the Spark Session\n",
    "\n",
    "---\n",
    "\n",
    "*Spark-Sitzung beenden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aec05f",
   "metadata": {},
   "source": [
    "## Lab 1 Summary\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "In this lab you built a complete Direct Lake semantic model from scratch:\n",
    "\n",
    "- **Infrastructure:** Created a lakehouse and loaded four Adventure Works tables (approximately 80,000 rows in total)\n",
    "- **Model creation:** Generated a Direct Lake semantic model with automatic table discovery\n",
    "- **Data modelling:** Defined star schema relationships between the fact table and three dimension tables\n",
    "- **Business logic:** Added DAX measures with appropriate formatting\n",
    "- **User experience:** Configured date table marking, column sorting, and column visibility\n",
    "- **Validation:** Used DMV queries to observe on-demand column loading and memory usage\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Adventure Works Data --> Lakehouse (Delta Tables) --> Direct Lake Model --> Power BI Reports\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Direct Lake queries data directly from Delta files, avoiding the need for scheduled imports\n",
    "- Columns are loaded into memory only when a query requires them\n",
    "- DMV queries provide visibility into which columns are loaded and how much memory is consumed\n",
    "- Framing establishes the initial link between the semantic model and the underlying Delta tables\n",
    "\n",
    "### Next Lab\n",
    "\n",
    "Continue to **Lab 2** to work with billion-row datasets and OneLake shortcuts.\n",
    "\n",
    "---\n",
    "\n",
    "*Deutsche Version:*\n",
    "\n",
    "### Was Sie erreicht haben\n",
    "\n",
    "In diesem Lab haben Sie ein vollstaendiges Direct Lake Semantic Model von Grund auf erstellt:\n",
    "\n",
    "- **Infrastruktur:** Ein Lakehouse erstellt und vier Adventure Works-Tabellen geladen (insgesamt ca. 80.000 Zeilen)\n",
    "- **Modellerstellung:** Ein Direct Lake Semantic Model mit automatischer Tabellenerkennung generiert\n",
    "- **Datenmodellierung:** Sternschema-Beziehungen zwischen der Faktentabelle und drei Dimensionstabellen definiert\n",
    "- **Geschaeftslogik:** DAX-Measures mit entsprechender Formatierung hinzugefuegt\n",
    "- **Benutzererfahrung:** Datumstabellenmarkierung, Spaltensortierung und Spaltensichtbarkeit konfiguriert\n",
    "- **Validierung:** DMV-Abfragen verwendet, um das bedarfsgesteuerte Laden von Spalten und die Speichernutzung zu beobachten\n",
    "\n",
    "### Wichtige Erkenntnisse\n",
    "\n",
    "- Direct Lake fragt Daten direkt aus Delta-Dateien ab und vermeidet geplante Importe\n",
    "- Spalten werden nur dann in den Speicher geladen, wenn eine Abfrage sie benoetigt\n",
    "- DMV-Abfragen bieten Einblick in geladene Spalten und Speicherverbrauch\n",
    "- Framing stellt die initiale Verbindung zwischen Semantic Model und Delta-Tabellen her\n",
    "\n",
    "### Naechstes Lab\n",
    "\n",
    "Weiter zu **Lab 2**, um mit Milliarden-Zeilen-Datensaetzen und OneLake-Shortcuts zu arbeiten."
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
