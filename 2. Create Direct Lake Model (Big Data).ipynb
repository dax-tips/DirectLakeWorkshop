{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b3d893-db98-4929-97a7-40148f6269a4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 2: Direct Lake with Big Data - Billion Row Analytics\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "This lab demonstrates Direct Lake's **enterprise-scale capabilities** by working with **billion-row datasets**. You'll learn how Direct Lake handles massive data volumes, create OneLake shortcuts for cross-workspace data access, and understand when Direct Lake falls back to SQL Endpoint mode.\n",
    "\n",
    "### What You'll Build\n",
    "\n",
    "**Workshop Flow:**\n",
    "```\n",
    "1. Setup Big Data Environment\n",
    "   ‚Üì\n",
    "2. Create OneLake Shortcuts\n",
    "   ‚Üì\n",
    "3. Build Billion-Row Semantic Model\n",
    "   ‚Üì\n",
    "4. Performance Testing & Tracing\n",
    "   ‚Üì\n",
    "5. Analyze Fallback Scenarios\n",
    "   ‚Üì\n",
    "6. Optimize for Scale\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "- **OneLake Shortcuts**: Access data across workspaces without duplication\n",
    "- **Direct Lake Guardrails**: Understanding billion-row table limits\n",
    "- **Fallback Behavior**: When Direct Lake uses SQL Endpoint mode\n",
    "- **Column Temperature**: Memory management for large datasets\n",
    "\n",
    "### Learning Objectives\n",
    "By completing this lab, you'll be able to:\n",
    "- ‚úÖ Create OneLake shortcuts for cross-workspace data access\n",
    "- ‚úÖ Build semantic models with billion-row fact tables\n",
    "- ‚úÖ Monitor Direct Lake performance with advanced tracing\n",
    "- ‚úÖ Understand and troubleshoot fallback scenarios\n",
    "- ‚úÖ Optimize queries for massive datasets\n",
    "\n",
    "### Dataset Scale\n",
    "| Table | Rows | Purpose |\n",
    "|:------|:-----|:--------|\n",
    "| **fact_myevents_1bln** | 1 billion | Standard fact table |\n",
    "| **fact_myevents_2bln** | 2 billion | Stress test limits |\n",
    "| **fact_myevents_1bln_partitioned_datekey** | 1 billion | Optimized with partitioning |\n",
    "| **dim_Date** | ~3,650 | Date dimension |\n",
    "| **dim_Geography** | ~200 | Geography dimension |\n",
    "\n",
    "**Estimated Time**: 60-90 minutes  \n",
    "**Prerequisites**: Lab 1 completion, access to Big Data workspace\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd099e87-89f6-45b2-b286-d7f8a12b6a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "Install Semantic Link Labs with enhanced big data capabilities for billion-row analytics and OneLake shortcut management.\n",
    "Working with billion-row tables introduces unique challenges:\n",
    "- **Memory management** becomes critical\n",
    "- **Query optimization** requires detailed tracing\n",
    "- **Fallback behavior** needs monitoring\n",
    "- **Cross-workspace access** requires shortcut expertise\n",
    "\n",
    "**Expected outcome**: Enhanced library ready for enterprise-scale analytics and performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ed5ca-a6ba-478a-b029-17b6db9b6308",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q --disable-pip-version-check semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc44ce-395c-4db3-8979-b06acf9f8ecf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2. Import Libraries and Set Variables\n",
    "\n",
    "Import required libraries and configure environment variables for big data workspace and region-aware data source selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e841d6-a757-4029-aa71-88d4bd286c30",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as labs\n",
    "from sempy import fabric\n",
    "import sempy\n",
    "import pandas\n",
    "import json\n",
    "import time\n",
    "\n",
    "LakehouseName = \"BigData\"\n",
    "SemanticModelName = f\"{LakehouseName}_model\"\n",
    "\n",
    "capacity_name = labs.get_capacity_name()\n",
    "\n",
    "Shortcut_LakehouseName = \"BigDemoDB\"\n",
    "Shortcut_WorkspaceName = \"DL Labs - Data [North Central US]\"\n",
    "if capacity_name == \"FabConUS8-P1\":\n",
    "    Shortcut_WorkspaceName = \"DL Labs - Data [West US 3]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99f94c-a564-4aca-b670-31da354b7b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3. Create Lakehouse for Big Data\n",
    "\n",
    "Create a lightweight lakehouse that will use OneLake shortcuts to access billion-row tables without data duplication.\n",
    "```\n",
    "Source Workspace        Target Workspace\n",
    "‚îú‚îÄ‚îÄ BigDemoDB           ‚îú‚îÄ‚îÄ BigData\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 1bln_table ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ   ‚îú‚îÄ‚îÄ 1bln_table (COPY - Expensive!)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 2bln_table ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ   ‚îî‚îÄ‚îÄ 2bln_table (COPY - Expensive!)\n",
    "```\n",
    "\n",
    "#### OneLake Shortcut Approach (‚úÖ Recommended):\n",
    "```\n",
    "Source Workspace        Target Workspace  \n",
    "‚îú‚îÄ‚îÄ BigDemoDB           ‚îú‚îÄ‚îÄ BigData\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 1bln_table ‚Üê‚îÄ‚îÄ‚îÄ ‚îÇ   ‚îú‚îÄ‚îÄ 1bln_table (SHORTCUT - Efficient!)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 2bln_table ‚Üê‚îÄ‚îÄ‚îÄ ‚îÇ   ‚îî‚îÄ‚îÄ 2bln_table (SHORTCUT - Efficient!)\n",
    "```\n",
    "\n",
    "### Expected Infrastructure\n",
    "After execution, you'll have:\n",
    "- ‚úÖ **Empty lakehouse** named \"BigData\" ready for shortcuts\n",
    "- ‚úÖ **Workspace identifiers** for cross-workspace operations\n",
    "- ‚úÖ **Foundation** for billion-row data access\n",
    "\n",
    "üéØ **Infrastructure checkpoint**: Lakehouse ready for big data shortcuts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760df625-543c-4289-b6cc-f043290d5879",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "lakehouses=labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "if LakehouseName in lakehouses.values:\n",
    "    lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]\n",
    "else:\n",
    "    lakehouseId = fabric.create_lakehouse(LakehouseName)\n",
    "\n",
    "workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "workspaceName = sempy.fabric.resolve_workspace_name(workspaceId)\n",
    "print(f\"WorkspaceId = {workspaceId}, LakehouseID = {lakehouseId}, Workspace Name = {workspaceName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d9d90-0444-472b-a870-ed4a2425227a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Create OneLake Shortcuts for Big Data Access\n",
    "\n",
    "### Understanding OneLake Shortcuts\n",
    "**OneLake shortcuts** are virtual connections that provide seamless access to data across Fabric workspaces without physical data movement:\n",
    "\n",
    "#### Shortcut Benefits for Big Data:\n",
    "- **üìä Instant access**: No waiting for billion-row data copies\n",
    "- **üí∞ Zero storage cost**: Reference data without duplication  \n",
    "- **üîÑ Real-time consistency**: Always access current data\n",
    "- **üõ°Ô∏è Security inheritance**: Maintains source permissions\n",
    "- **‚ö° Performance**: Direct Delta Lake access\n",
    "\n",
    "### Shortcut Cleanup Strategy\n",
    "```python\n",
    "for index, row in labs.lakehouse.list_shortcuts(lakehouse=LakehouseName).iterrows():\n",
    "    labs.lakehouse.delete_shortcut(shortcut_name=row[\"Shortcut Name\"], lakehouse=LakehouseName)\n",
    "```\n",
    "- **üßπ Clean slate**: Removes existing shortcuts to prevent conflicts\n",
    "- **üîÑ Idempotent**: Allows multiple runs without issues\n",
    "- **‚úÖ Consistent**: Ensures predictable shortcut configuration\n",
    "\n",
    "### Big Data Tables Being Connected\n",
    "\n",
    "#### Fact Tables (Billion+ Rows):\n",
    "| Shortcut Name | Purpose | Rows | Special Features |\n",
    "|---------------|---------|------|------------------|\n",
    "| **fact_myevents_1bln** | Standard billion-row analytics | ~1B | V-Order optimized |\n",
    "| **fact_myevents_1bln_no_vorder** | Performance comparison | ~1B | Without V-Order |\n",
    "| **fact_myevents_1bln_partitioned_datekey** | Partitioning benefits | ~1B | Date-key partitioned |\n",
    "| **fact_myevents_2bln** | Stress testing limits | ~2B | Maximum scale test |\n",
    "\n",
    "#### Dimension Tables:\n",
    "| Shortcut Name | Purpose | Rows | Relationship Target |\n",
    "|---------------|---------|------|-------------------|\n",
    "| **dim_Date** | Time dimension | ~3,650 | DateKey relationships |\n",
    "| **dim_Geography** | Location dimension | ~200 | GeographyID relationships |\n",
    "\n",
    "### OneLake Shortcut Creation Process\n",
    "```python\n",
    "labs.lakehouse.create_shortcut_onelake(\n",
    "    table_name=\"fact_myevents_1bln\",\n",
    "    source_lakehouse=Shortcut_LakehouseName,\n",
    "    source_workspace=Shortcut_WorkspaceName,\n",
    "    destination_lakehouse=LakehouseName\n",
    ")\n",
    "```\n",
    "\n",
    "#### Parameters Explained:\n",
    "- **`table_name`**: The exact table name in the source lakehouse\n",
    "- **`source_lakehouse`**: \"BigDemoDB\" - contains the billion-row datasets\n",
    "- **`source_workspace`**: Region-specific workspace for optimal performance\n",
    "- **`destination_lakehouse`**: \"BigData\" - our local workspace lakehouse\n",
    "\n",
    "### Expected Outcome\n",
    "After execution, you'll see:\n",
    "```\n",
    "Deleted shortcut existing_table_1    # Cleanup messages\n",
    "Deleted shortcut existing_table_2\n",
    "Adding shortcuts complete.           # Success confirmation\n",
    "```\n",
    "\n",
    "Your lakehouse now has **virtual access** to billions of rows without consuming local storage!\n",
    "\n",
    "üéØ **Big data access checkpoint**: Billion-row tables accessible via zero-copy shortcuts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3367b-c08a-4104-b04c-1410d8850f51",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#1. Remove any existing shortcuts\n",
    "for index, row in labs.lakehouse.list_shortcuts(lakehouse=LakehouseName).iterrows():\n",
    "    labs.lakehouse.delete_shortcut(shortcut_name=row[\"Shortcut Name\"],lakehouse=LakehouseName)\n",
    "    print(f\"Deleted shortcut {row['Shortcut Name']}\")\n",
    "\n",
    "#2. Creates correct shortcuts\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"fact_myevents_1bln\"                      ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"fact_myevents_1bln_no_vorder\"            ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"fact_myevents_1bln_partitioned_datekey\"  ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"fact_myevents_2bln\"                      ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"dim_Date\"                                ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"dim_Geography\"                           ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "\n",
    "print('Adding shortcuts complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff2996-0b1c-4792-99cd-8d61315e65da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 5. Synchronize Big Data Table Metadata\n",
    "\n",
    "Forces lakehouse metadata refresh to recognize billion-row shortcuts and their schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb2e8e-a2b9-411f-b69d-3ea129e06c10",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Triggers REST API call to refresh lakehouse metadata and table schemas.\n",
    "3. **üìä Progress Monitoring**: Poll batch status every second until success\n",
    "4. **‚úÖ Completion Validation**: Confirm all billion-row tables are properly cataloged\n",
    "\n",
    "#### Why REST API vs. Automatic?\n",
    "- **‚è±Ô∏è Timing control**: Force immediate sync rather than waiting for background processes\n",
    "- **üîç Visibility**: Real-time progress monitoring for large metadata operations\n",
    "- **üõ°Ô∏è Reliability**: Guaranteed completion before proceeding to model creation\n",
    "- **üîÑ Repeatability**: Can be re-run if any step fails\n",
    "\n",
    "#### Big Data Considerations:\n",
    "- **Longer sync times**: Billion-row tables require more metadata processing\n",
    "- **Resource usage**: Background jobs may consume more compute during sync\n",
    "- **Cross-workspace complexity**: Shortcut metadata requires additional validation\n",
    "\n",
    "**Expected behavior**: Periodic \"running\" status updates followed by \"success\" for complete metadata sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54e1aa-a94e-4efd-b40e-f497f54589e8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "##https://medium.com/@sqltidy/delays-in-the-automatically-generated-schema-in-the-sql-analytics-endpoint-of-the-lakehouse-b01c7633035d\n",
    "\n",
    "def triggerMetadataRefresh():\n",
    "    client = fabric.FabricRestClient()\n",
    "    response = client.get(f\"/v1/workspaces/{workspaceId}/lakehouses/{lakehouseId}\")\n",
    "    sqlendpoint = response.json()['properties']['sqlEndpointProperties']['id']\n",
    "\n",
    "    # trigger sync\n",
    "    uri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}\"\n",
    "    payload = {\"commands\":[{\"$type\":\"MetadataRefreshExternalCommand\"}]}\n",
    "    response = client.post(uri,json= payload)\n",
    "    batchId = response.json()['batchId']\n",
    "\n",
    "    # Monitor Progress\n",
    "    statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "    statusresponsedata = client.get(statusuri).json()\n",
    "    progressState = statusresponsedata['progressState']\n",
    "    print(f\"Metadata refresh : {progressState}\")\n",
    "    while progressState != \"success\":\n",
    "        statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "        statusresponsedata = client.get(statusuri).json()\n",
    "        progressState = statusresponsedata['progressState']\n",
    "        print(f\"Metadata refresh : {progressState}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    print('Metadata refresh complete')\n",
    "\n",
    "triggerMetadataRefresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e16b1-9893-46d3-ac7d-93a4d1453c5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 6. Create Big Data Direct Lake Semantic Model\n",
    "\n",
    "Creates semantic model with automatic discovery of billion-row shortcut tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e9cc7-de53-4c81-8408-9fb18ef4383a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "Discovers lakehouse tables and creates semantic model with retry logic for robust big data deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df3373-52ca-4e45-83a6-40e1b20c184d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 7. Configure Relationships for Big Data Analytics\n",
    "\n",
    "Establishes efficient many-to-one relationships between billion-row facts and dimension tables.\n",
    "- **‚ö° Index optimization**: Proper relationships enable Direct Lake query optimization\n",
    "- **üß† Memory planning**: Relationship columns will be loaded into memory during queries\n",
    "\n",
    "### Multi-Fact Table Relationships\n",
    "This model demonstrates **multiple billion-row fact tables** sharing the same dimensions:\n",
    "\n",
    "```\n",
    "    dim_Date ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ fact_myevents_1bln (1B rows)\n",
    "                 ‚îú‚îÄ‚îÄ‚îÄ‚îÄ fact_myevents_2bln (2B rows)  \n",
    "                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ fact_myevents_1bln_partitioned_datekey (1B rows)\n",
    "                 \n",
    "    dim_Geography ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ fact_myevents_1bln (1B rows)\n",
    "                  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ fact_myevents_2bln (2B rows)\n",
    "                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ fact_myevents_1bln_partitioned_datekey (1B rows)\n",
    "```\n",
    "\n",
    "### Relationship Configuration Details\n",
    "\n",
    "#### Date Relationships (Time Intelligence):\n",
    "- **fact_myevents_1bln.DateKey ‚Üí dim_Date.DateKey**\n",
    "- **fact_myevents_2bln.DateKey ‚Üí dim_Date.DateKey**  \n",
    "- **fact_myevents_1bln_partitioned_datekey.DateKey ‚Üí dim_Date.DateKey**\n",
    "\n",
    "**Performance Impact**: DateKey is typically an integer with good cardinality distribution, making it efficient for billion-row joins.\n",
    "\n",
    "#### Geography Relationships (Dimensional Analysis):\n",
    "- **fact_myevents_1bln.GeographyID ‚Üí dim_Geography.GeographyID**\n",
    "- **fact_myevents_2bln.GeographyID ‚Üí dim_Geography.GeographyID**\n",
    "- **fact_myevents_1bln_partitioned_datekey.GeographyID ‚Üí dim_Geography.GeographyID**\n",
    "\n",
    "**Performance Impact**: GeographyID has lower cardinality (~200 values), making it very efficient for filtering and grouping.\n",
    "\n",
    "### Big Data Relationship Best Practices\n",
    "\n",
    "#### ‚úÖ **Efficient Patterns**:\n",
    "- **Many-to-One cardinality**: Billion fact rows to thousands of dimension rows\n",
    "- **Integer keys**: Faster joins and smaller memory footprint\n",
    "- **Indexed columns**: Keys that benefit from Delta Lake optimization\n",
    "\n",
    "#### ‚ùå **Patterns to Avoid**:\n",
    "- **Many-to-Many relationships**: Expensive with billion-row tables\n",
    "- **String-based keys**: Larger memory footprint and slower joins\n",
    "- **High-cardinality dimensions**: Can trigger fallback to SQL Endpoint\n",
    "\n",
    "### Memory and Performance Implications\n",
    "When relationships are used in queries:\n",
    "- **Key columns loaded**: DateKey and GeographyID columns from fact tables enter memory\n",
    "- **Join optimization**: Direct Lake optimizes join execution paths\n",
    "- **Filter propagation**: Efficient filtering from dimensions to billion-row facts\n",
    "\n",
    "**Expected outcome**: Six relationships configured enabling efficient cross-table analysis across multiple billion-row fact tables.\n",
    "\n",
    "üéØ **Big data relationships checkpoint**: Multi-billion row fact tables connected efficiently to shared dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616193a-57df-4139-91ef-c73830331555",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing relationships\n",
    "            for r in tom.model.Relationships:\n",
    "                tom.model.Relationships.Remove(r)\n",
    "\n",
    "            #2. Creates correct relationships\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln\"                    , from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln\"                    , from_column=\"GeographyID\" , to_table=\"dim_Geography\"  , to_column=\"GeographyID\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "\n",
    "            tom.add_relationship(from_table=\"fact_myevents_2bln\"                    , from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"fact_myevents_2bln\"                    , from_column=\"GeographyID\" , to_table=\"dim_Geography\"  , to_column=\"GeographyID\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln_partitioned_datekey\", from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln_partitioned_datekey\", from_column=\"GeographyID\" , to_table=\"dim_Geography\"  , to_column=\"GeographyID\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding relationships... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba5536-018c-402b-aa0f-663ee5e8d07f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 8. Create Performance-Optimized Measures for Big Data\n",
    "\n",
    "Creates strategic measures for billion-row performance comparison and scale analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666631c-4221-4638-b8ff-10fee8a4f7df",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "Removes existing measures and adds optimized measures for 1B and 2B row performance comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ff91f-0362-4c3e-bf2e-60c64867cec8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 9. Configure Date Intelligence for Big Data Analytics\n",
    "\n",
    "### Time Intelligence with Billion-Row Datasets\n",
    "**Date tables** become even more critical when working with billion-row fact tables because:\n",
    "\n",
    "- **üìÖ Time-based filtering**: Essential for managing large dataset query performance\n",
    "- **üìä Aggregation efficiency**: Date hierarchies enable efficient drill-down on large datasets\n",
    "- **‚ö° Partition alignment**: Date partitioning often aligns with time intelligence patterns\n",
    "- **üéØ User experience**: Time filters help users focus on relevant data subsets\n",
    "\n",
    "### Big Data Date Table Configuration\n",
    "```python\n",
    "tom.mark_as_date_table(table_name=\"dim_Date\", column_name=\"DateKey\")\n",
    "```\n",
    "\n",
    "#### Why DateKey Instead of Date?\n",
    "- **üîó Relationship compatibility**: Matches the integer keys in billion-row fact tables\n",
    "- **‚ö° Performance optimization**: Integer joins are faster than datetime joins with large datasets\n",
    "- **üíæ Storage efficiency**: Smaller memory footprint for relationship columns\n",
    "- **üìä Index optimization**: Integer keys benefit more from Delta Lake indexing\n",
    "\n",
    "### Time Intelligence Impact on Big Data Queries\n",
    "\n",
    "#### Efficient Time Filtering:\n",
    "With proper date table configuration, queries like:\n",
    "```dax\n",
    "CALCULATE([Sum of Sales (2bln)], DATESINPERIOD(dim_Date[DateKey], TODAY(), -1, YEAR))\n",
    "```\n",
    "Can efficiently filter billion-row datasets to specific time periods.\n",
    "\n",
    "#### Performance Benefits:\n",
    "- **Partition elimination**: Date-based filters can eliminate entire partitions\n",
    "- **Memory reduction**: Only relevant time periods loaded into memory\n",
    "- **Query optimization**: Direct Lake can optimize date-based predicates\n",
    "\n",
    "### Big Data Time Intelligence Scenarios\n",
    "\n",
    "#### Typical Use Cases:\n",
    "| Time Period | Impact on 2B Row Table | Performance Benefit |\n",
    "|-------------|------------------------|-------------------|\n",
    "| **Last 30 days** | ~164M rows (5%) | 95% data elimination |\n",
    "| **Current year** | ~730M rows (37%) | 63% data elimination |\n",
    "| **Year-over-year** | ~1.46B rows (73%) | Comparative analysis |\n",
    "\n",
    "### Expected Configuration Benefits\n",
    "Once the date table is marked:\n",
    "- ‚úÖ **Time intelligence functions** available for billion-row analysis\n",
    "- ‚úÖ **Efficient filtering** on large fact tables\n",
    "- ‚úÖ **Optimized memory usage** through time-based data elimination\n",
    "- ‚úÖ **Better user experience** with familiar date hierarchies\n",
    "\n",
    "üéØ **Big data time intelligence checkpoint**: Date table optimized for efficient billion-row time-based analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f48bf-893b-4a8f-92a1-e5e80b968840",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            tom.mark_as_date_table(table_name=\"dim_Date\",column_name=\"DateKey\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error with date table... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb148df-8490-47fc-8e9c-6b8cbcb5fb6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 10. Configure Logical Sorting for Big Data Visualizations\n",
    "\n",
    "### Sorting Importance in Big Data Contexts\n",
    "With **billion-row datasets**, proper sorting becomes crucial for user experience because:\n",
    "\n",
    "- **üìä Large result sets**: Time-based reports may return thousands of rows\n",
    "- **üéØ Pattern recognition**: Logical ordering helps identify trends in big data\n",
    "- **‚ö° Performance impact**: Proper sorting can leverage existing table ordering\n",
    "- **üë• User adoption**: Intuitive ordering reduces confusion with large datasets\n",
    "\n",
    "### Big Data Dimension Sorting Configuration\n",
    "\n",
    "#### Month Name Sorting:\n",
    "```python\n",
    "tom.set_sort_by_column(table_name=\"dim_Date\", column_name=\"MonthName\", sort_by_column=\"Month\")\n",
    "```\n",
    "- **Display column**: MonthName (\"January\", \"February\", \"March\"...)\n",
    "- **Sort column**: Month (1, 2, 3...)\n",
    "- **Big data impact**: When aggregating billion rows by month, results display logically\n",
    "\n",
    "#### Weekday Sorting:\n",
    "```python\n",
    "tom.set_sort_by_column(table_name=\"dim_Date\", column_name=\"WeekDayName\", sort_by_column=\"Weekday\")\n",
    "```\n",
    "- **Display column**: WeekDayName (\"Sunday\", \"Monday\", \"Tuesday\"...)\n",
    "- **Sort column**: Weekday (1, 2, 3...)\n",
    "- **Big data impact**: Weekly aggregations of large datasets display in intuitive order\n",
    "\n",
    "### Performance Considerations for Big Data Sorting\n",
    "\n",
    "#### Memory Impact:\n",
    "- **Sort columns loaded**: Both display and sort-by columns enter memory during queries\n",
    "- **Relationship efficiency**: Sorting columns are often used in time-based relationships\n",
    "- **Query optimization**: Proper sorting can leverage Delta Lake file ordering\n",
    "\n",
    "#### User Experience with Large Results:\n",
    "| Query Result Size | Sorting Benefit | Example |\n",
    "|------------------|-----------------|---------|\n",
    "| **Monthly aggregation** | 12-60 rows | Chronological progression |\n",
    "| **Daily aggregation** | 365-1,095 rows | Calendar order clarity |\n",
    "| **Hourly aggregation** | 8,760+ rows | Time series continuity |\n",
    "\n",
    "### JSON Verification Output\n",
    "The code displays the complete table structure showing:\n",
    "- ‚úÖ **Sort relationships** properly configured\n",
    "- ‚úÖ **Column metadata** including sort-by properties\n",
    "- ‚úÖ **Performance settings** for large-scale operations\n",
    "\n",
    "**Expected outcome**: Date dimension columns configured for logical sorting, ensuring intuitive ordering in billion-row aggregation results.\n",
    "\n",
    "üéØ **Big data UX checkpoint**: Sorting optimized for large-scale analytics user experience!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e3959-51e3-481c-b730-bde8482bd746",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        tom = labs.tom.TOMWrapper(dataset=SemanticModelName, workspace=workspaceName, readonly=False)\n",
    "        tom.set_sort_by_column(table_name=\"dim_Date\",column_name=\"MonthName\"       ,sort_by_column=\"Month\")\n",
    "        tom.set_sort_by_column(table_name=\"dim_Date\",column_name=\"WeekDayName\"     ,sort_by_column=\"Weekday\")\n",
    "        tom.model.SaveChanges()\n",
    "\n",
    "        #Show BIM data for dim_Date table\n",
    "        i:int=0\n",
    "        for t in tom.model.Tables:\n",
    "            if t.Name==\"dim_Date\":\n",
    "                bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "                print(bim)\n",
    "            i=i+1\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error with sort by cols... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7babd-5053-4dc5-97e1-4398e67dba26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 11. Optimize Big Data Model by Hiding Fact Table Columns\n",
    "\n",
    "Hides all fact table columns to prevent accidental memory overload from billion-row column access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e49ec-bd01-467f-ad6f-5bff8f068fcc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        i:int=0\n",
    "        for t in tom.model.Tables:\n",
    "            if t.Name in [\"fact_myevents_1bln\",\"fact_myevents_2bln\",\"fact_myevents_1bln_partitioned_datekey\"]:\n",
    "                for c in t.Columns:\n",
    "                    c.IsHidden=True\n",
    "\n",
    "                bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "                print(bim)\n",
    "            i=i+1\n",
    "        tom.model.SaveChanges()\n",
    "        completedOK=True\n",
    "    except:\n",
    "        print('Error with hiding cols... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc341d-8682-4a4d-8b00-300d8723bcfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 12. Refresh Big Data Model and Validate Configuration\n",
    "\n",
    "Refreshes semantic model and validates proper configuration for billion-row analytics.\n",
    "\n",
    "### Big Data Model Refresh Considerations\n",
    "Refreshing a semantic model with **billion-row tables** requires special attention to:\n",
    "\n",
    "- **‚è±Ô∏è Extended timing**: Billion-row metadata validation takes longer\n",
    "- **üß† Memory planning**: Initial refresh prepares memory allocation strategies\n",
    "- **üîó Shortcut validation**: Ensures cross-workspace connections are stable\n",
    "- **üõ°Ô∏è Guardrail checking**: Validates tables remain within Direct Lake limits\n",
    "\n",
    "### Why Refresh is More Critical for Big Data\n",
    "\n",
    "#### Validation Processes:\n",
    "- **Schema consistency**: Ensures billion-row table schemas are properly detected\n",
    "- **Relationship integrity**: Validates joins work correctly with large datasets\n",
    "- **Memory allocation**: Prepares Direct Lake for potential large column loads\n",
    "- **Performance optimization**: Updates query execution plans for big data scenarios\n",
    "\n",
    "#### Big Data Refresh Challenges:\n",
    "| Challenge | Impact | Mitigation |\n",
    "|-----------|---------|------------|\n",
    "| **Metadata complexity** | Longer refresh times | Patient retry logic |\n",
    "| **Cross-workspace shortcuts** | Connection validation delays | Automatic metadata sync |\n",
    "| **Memory preparation** | Initial allocation overhead | Staged loading approach |\n",
    "\n",
    "### Enhanced Error Handling for Big Data\n",
    "```python\n",
    "while not reframeOK:\n",
    "    try:\n",
    "        result = labs.refresh_semantic_model(dataset=SemanticModelName)\n",
    "        reframeOK = True\n",
    "    except:\n",
    "        triggerMetadataRefresh()  # Re-sync billion-row table metadata\n",
    "        time.sleep(3)             # Allow background processes to complete\n",
    "```\n",
    "\n",
    "#### Why Robust Error Handling Matters:\n",
    "- **Billion-row complexity**: More opportunities for transient failures\n",
    "- **Resource coordination**: Multiple services must coordinate for large datasets\n",
    "- **Network dependencies**: Cross-workspace shortcuts may have connectivity issues\n",
    "- **Memory allocation**: Initial memory planning may require multiple attempts\n",
    "\n",
    "### Post-Refresh Validation\n",
    "After successful refresh, the model is ready for:\n",
    "- ‚úÖ **Billion-row queries**: Can handle large-scale aggregations\n",
    "- ‚úÖ **Cross-table analysis**: Relationships function with massive datasets\n",
    "- ‚úÖ **Memory-efficient operation**: Optimized for large column loading\n",
    "- ‚úÖ **Fallback monitoring**: Ready for guardrail limit testing\n",
    "\n",
    "**Expected outcome**: \"Custom Semantic Model reframe OK\" confirms your big data model is ready for billion-row analytics!\n",
    "\n",
    "üéØ **Big data model readiness checkpoint**: Billion-row semantic model fully configured and validated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed6756-a8b0-4250-9062-3266ae563054",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "reframeOK:bool=False\n",
    "while not reframeOK:\n",
    "    try:\n",
    "        result:pandas.DataFrame = labs.refresh_semantic_model(dataset=SemanticModelName)\n",
    "        reframeOK=True\n",
    "    except:\n",
    "        print('Error with reframe... trying again.')\n",
    "        triggerMetadataRefresh()\n",
    "        time.sleep(3)\n",
    "\n",
    "print('Custom Semantic Model reframe OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f25b5-949d-4794-b565-f4d8fea1c089",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 13. Advanced Performance Tracing for Big Data Analytics\n",
    "\n",
    "Creates enhanced tracing function for comprehensive big data performance monitoring and analysis.\n",
    "\n",
    "### Why Advanced Tracing is Essential for Big Data\n",
    "With **billion-row datasets**, understanding query execution becomes critical for:\n",
    "\n",
    "- **üîç Fallback detection**: When and why Direct Lake falls back to SQL Endpoint\n",
    "- **‚è±Ô∏è Performance analysis**: Detailed timing breakdown for large-scale operations\n",
    "- **üß† Memory monitoring**: Track column loading patterns during billion-row queries\n",
    "- **üõ°Ô∏è Guardrail monitoring**: Understand limit breaches and their impacts\n",
    "\n",
    "### Enhanced Tracing Capabilities\n",
    "\n",
    "#### DMV (Dynamic Management Views) Analysis:\n",
    "The `runDMV()` function provides deep insights into:\n",
    "- **Column temperature**: Which billion-row columns are \"HOT\", \"WARM\", or \"COLD\"\n",
    "- **Memory residency**: What's currently loaded vs. on-disk\n",
    "- **Dictionary sizes**: Compression effectiveness for large columns\n",
    "- **Access patterns**: When columns were last used in queries\n",
    "\n",
    "#### Advanced Query Tracing:\n",
    "The `runQueryWithTrace()` function provides:\n",
    "- **Server-side timing**: Detailed execution breakdown\n",
    "- **Storage engine events**: Direct Lake vs. SQL Endpoint execution paths  \n",
    "- **Memory operations**: Column loading and caching behavior\n",
    "- **Fallback triggers**: Exact reasons for SQL Endpoint fallback\n",
    "\n",
    "### Big Data Tracing Function Parameters\n",
    "\n",
    "#### Flexible Analysis Options:\n",
    "```python\n",
    "runQueryWithTrace(\n",
    "    expr: str,                    # DAX query to execute\n",
    "    workspaceName: str,           # Workspace context\n",
    "    SemanticModelName: str,       # Target model\n",
    "    Result: bool = True,          # Show query results\n",
    "    Trace: bool = True,           # Show server timings\n",
    "    DMV: bool = True,             # Show column states\n",
    "    ClearCache: bool = True       # Start with clean memory\n",
    ")\n",
    "```\n",
    "\n",
    "#### Why Each Parameter Matters for Big Data:\n",
    "\n",
    "##### **`ClearCache=True`**:\n",
    "- **Clean baseline**: Ensures accurate measurement of billion-row column loading\n",
    "- **Reproducible tests**: Consistent starting point for performance analysis\n",
    "- **Memory reset**: Clears any previously loaded large columns\n",
    "\n",
    "##### **`Trace=True`**:\n",
    "- **Execution path visibility**: See if query uses Direct Lake or falls back\n",
    "- **Performance bottlenecks**: Identify slow operations with large datasets\n",
    "- **Resource usage**: Understand memory and CPU impact\n",
    "\n",
    "##### **`DMV=True`**:\n",
    "- **Before/after comparison**: See impact of billion-row queries on column states\n",
    "- **Memory evolution**: Track which columns become \"HOT\" during execution\n",
    "- **Optimization insights**: Identify opportunities for performance tuning\n",
    "\n",
    "### Event Filtering for Big Data\n",
    "```python\n",
    "def filter_func(e):\n",
    "    if e.EventSubclass.ToString() == \"VertiPaqScanInternal\":\n",
    "        return False  # Filter out excessive internal scanning events\n",
    "    return True\n",
    "```\n",
    "\n",
    "This filtering prevents **trace log flooding** when working with billion-row tables, focusing on meaningful events.\n",
    "\n",
    "### Expected Tracing Benefits\n",
    "With these functions, you can:\n",
    "- ‚úÖ **Monitor fallback scenarios**: Understand when billion-row queries trigger SQL Endpoint\n",
    "- ‚úÖ **Analyze memory patterns**: See how large columns are loaded and cached\n",
    "- ‚úÖ **Compare performance**: Benchmark different query approaches on big data\n",
    "- ‚úÖ **Optimize queries**: Identify the most efficient patterns for large datasets\n",
    "\n",
    "üéØ **Big data monitoring checkpoint**: Advanced tracing tools ready for billion-row performance analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab34d2-784f-4a4a-bf61-afdabb4c2b69",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)\n",
    "\n",
    "def filter_func(e):\n",
    "    retVal:bool=True\n",
    "    if e.EventSubclass.ToString() == \"VertiPaqScanInternal\":\n",
    "        retVal=False      \n",
    "    #     #if e.EventSubClass.ToString() == \"VertiPaqScanInternal\":\n",
    "    #     retVal=False\n",
    "    return retVal\n",
    "\n",
    "# define events to trace and their corresponding columns\n",
    "def runQueryWithTrace (expr:str,workspaceName:str,SemanticModelName:str,Result:Optional[bool]=True,Trace:Optional[bool]=True,DMV:Optional[bool]=True,ClearCache:Optional[bool]=True) -> pandas.DataFrame :\n",
    "    event_schema = fabric.Trace.get_default_query_trace_schema()\n",
    "    event_schema.update({\"ExecutionMetrics\":[\"EventClass\",\"TextData\"]})\n",
    "    del event_schema['VertiPaqSEQueryBegin']\n",
    "    del event_schema['VertiPaqSEQueryCacheMatch']\n",
    "    del event_schema['DirectQueryBegin']\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    WorkspaceName = workspaceName\n",
    "    SemanticModelName = SemanticModelName\n",
    "\n",
    "    if ClearCache:\n",
    "        labs.clear_cache(SemanticModelName)\n",
    "\n",
    "    with fabric.create_trace_connection(SemanticModelName,WorkspaceName) as trace_connection:\n",
    "        # create trace on server with specified events\n",
    "        with trace_connection.create_trace(\n",
    "            event_schema=event_schema, \n",
    "            name=\"Simple Query Trace\",\n",
    "            filter_predicate=filter_func,\n",
    "            stop_event=\"QueryEnd\"\n",
    "            ) as trace:\n",
    "\n",
    "            trace.start()\n",
    "\n",
    "            df=sempy.fabric.evaluate_dax(\n",
    "                dataset=SemanticModelName, \n",
    "                dax_string=expr)\n",
    "\n",
    "            if Result:\n",
    "                displayHTML(f\"<H2>####### DAX QUERY RESULT #######</H2>\")\n",
    "                display(df)\n",
    "\n",
    "            # Wait 5 seconds for trace data to arrive\n",
    "            time.sleep(5)\n",
    "\n",
    "            # stop Trace and collect logs\n",
    "            final_trace_logs = trace.stop()\n",
    "\n",
    "    if Trace:\n",
    "        displayHTML(f\"<H2>####### SERVER TIMINGS #######</H2>\")\n",
    "        display(final_trace_logs)\n",
    "    \n",
    "    if DMV:\n",
    "        displayHTML(f\"<H2>####### SHOW DMV RESULTS #######</H2>\")\n",
    "        runDMV()\n",
    "    \n",
    "    return final_trace_logs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4e8d4",
   "metadata": {},
   "source": [
    "## 14. Validate Big Data Model Configuration\n",
    "\n",
    "Validates model configuration and Direct Lake mode operation for billion-row tables.\n",
    "\n",
    "### Big Data Model Validation Strategy\n",
    "Before executing billion-row queries, validate that your model is properly configured for large-scale operations:\n",
    "\n",
    "#### TABLETRAITS() for Big Data Validation:\n",
    "This function becomes especially important for billion-row tables because it reveals:\n",
    "- **Storage mode confirmation**: Ensures tables are actually in Direct Lake mode\n",
    "- **Partition information**: Shows how billion-row tables are organized\n",
    "- **File size details**: Validates files are within Direct Lake limits\n",
    "- **Compression statistics**: Shows V-Order optimization effectiveness\n",
    "\n",
    "#### Expected TABLETRAITS() Results for Big Data:\n",
    "| Table | Storage Mode | Row Count | File Size | Partitions |\n",
    "|-------|-------------|-----------|-----------|------------|\n",
    "| **fact_myevents_1bln** | DirectLake | ~1B | Multiple files <1GB each | Multiple |\n",
    "| **fact_myevents_2bln** | DirectLake | ~2B | Multiple files <1GB each | Multiple |\n",
    "| **fact_myevents_1bln_partitioned_datekey** | DirectLake | ~1B | Date-partitioned files | By DateKey |\n",
    "\n",
    "### Direct Lake Guardrails for Big Data\n",
    "The guardrails query becomes critical for billion-row scenarios:\n",
    "\n",
    "#### Key Guardrails to Monitor:\n",
    "- **Maximum file size**: Individual parquet files must be <1GB\n",
    "- **Row count limits**: Tables approaching 100M+ rows per partition\n",
    "- **Memory constraints**: Available memory for column dictionaries\n",
    "- **Cardinality limits**: Unique values per column thresholds\n",
    "\n",
    "#### Potential Guardrail Violations:\n",
    "| Guardrail | Billion-Row Risk | Impact |\n",
    "|-----------|-----------------|---------|\n",
    "| **File size >1GB** | Large fact tables | Forces fallback to SQL Endpoint |\n",
    "| **High cardinality** | Geography, Product IDs | May trigger import mode |\n",
    "| **Memory exhaustion** | Multiple concurrent users | Automatic fallback protection |\n",
    "\n",
    "### Fallback Detection Strategy\n",
    "If TABLETRAITS() shows **ImportMode** instead of **DirectLake** for any billion-row table:\n",
    "- üö® **Guardrail breach**: Table exceeded Direct Lake limits\n",
    "- üîÑ **Automatic fallback**: System protected against performance issues\n",
    "- üõ†Ô∏è **Optimization needed**: Consider partitioning or data reduction\n",
    "\n",
    "**Expected validation outcome**: All tables show DirectLake mode, confirming readiness for billion-row analytics.\n",
    "\n",
    "üéØ **Big data validation checkpoint**: Model configuration confirmed ready for large-scale Direct Lake operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf138f4-df36-41c4-bb58-a6af71edd179",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=sempy.fabric.evaluate_dax(\n",
    "    dataset=SemanticModelName, \n",
    "    dax_string=\"\"\"\n",
    "    \n",
    "    evaluate tabletraits()\n",
    "    \n",
    "    \"\"\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3394566-f8a8-4ee3-885b-766db5e8a615",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=labs.directlake.get_direct_lake_guardrails()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae67a8-b41c-4c18-8898-aaf4e478e599",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 15. Establish Big Data Performance Baseline\n",
    "\n",
    "Establishes performance baseline for billion-row analytics before executing large-scale queries.\n",
    "\n",
    "### Baseline Importance for Billion-Row Analytics\n",
    "Establishing a **performance baseline** before executing billion-row queries is crucial because:\n",
    "\n",
    "- **üìä Memory footprint**: See initial state before large columns are loaded\n",
    "- **üå°Ô∏è Temperature tracking**: All columns should start \"COLD\" for accurate measurement\n",
    "- **üß† Capacity planning**: Understand available memory before billion-row operations\n",
    "- **üìà Change detection**: Measure the exact impact of large-scale queries\n",
    "\n",
    "### Expected Baseline for Big Data Tables\n",
    "\n",
    "#### Initial Column States:\n",
    "All billion-row table columns should show:\n",
    "- **‚ùÑÔ∏è TEMPERATURE**: \"COLD\" (not accessed yet)\n",
    "- **üö´ RESIDENT**: \"FALSE\" (not in memory)\n",
    "- **üìÖ LASTACCESSED**: Null or very old timestamps\n",
    "- **üìè SIZE**: Actual dictionary compression sizes\n",
    "\n",
    "#### Critical Columns to Monitor:\n",
    "| Table | Column | Expected Size | Importance |\n",
    "|-------|--------|---------------|------------|\n",
    "| **fact_myevents_1bln** | Quantity_ThisYear | Large (1B values) | Primary measure column |\n",
    "| **fact_myevents_1bln** | DateKey | Medium (date range) | Relationship key |\n",
    "| **fact_myevents_2bln** | Quantity_ThisYear | Very Large (2B values) | Stress test column |\n",
    "| **fact_myevents_2bln** | GeographyID | Small (low cardinality) | Efficient dimension key |\n",
    "\n",
    "### Memory Capacity Analysis\n",
    "The baseline DMV reveals:\n",
    "- **Available memory**: How much memory is available for column loading\n",
    "- **Current usage**: Memory already consumed by model metadata\n",
    "- **Compression ratios**: How effectively billion-row columns are compressed\n",
    "- **Cardinality patterns**: Which columns have high vs. low unique value counts\n",
    "\n",
    "### Baseline Insights for Performance Planning\n",
    "\n",
    "#### Memory Allocation Estimates:\n",
    "- **1B row column**: May require 2-8GB depending on data type and compression\n",
    "- **2B row column**: May require 4-16GB, potentially triggering fallback\n",
    "- **Date keys**: Usually very efficient due to limited range\n",
    "- **Geography IDs**: Minimal memory due to low cardinality\n",
    "\n",
    "#### Performance Prediction:\n",
    "Based on baseline dictionary sizes:\n",
    "- **Small dictionaries** (<100MB): Will load quickly into memory\n",
    "- **Medium dictionaries** (100MB-1GB): Moderate loading time\n",
    "- **Large dictionaries** (>1GB): May trigger fallback to SQL Endpoint\n",
    "\n",
    "**Expected baseline outcome**: Complete view of billion-row table column states before any query execution, establishing foundation for performance analysis.\n",
    "\n",
    "üéØ **Big data baseline checkpoint**: Billion-row performance measurement foundation established!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223acea6-6ac3-4f3a-b04d-603309daf706",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2dcd3-fe61-4466-b7dd-f746bac1e05a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 16. Execute Billion-Row Analytics with Performance Monitoring\n",
    "\n",
    "Executes comprehensive billion-row analytics with detailed performance monitoring and stress testing.\n",
    "\n",
    "### Big Data Query Execution Strategy\n",
    "This section demonstrates **real-world billion-row analytics** while monitoring Direct Lake behavior and potential fallback scenarios.\n",
    "\n",
    "### Performance Analysis Framework\n",
    "Each query provides comprehensive analysis through:\n",
    "- **üìä Query results**: Actual business insights from billion-row datasets\n",
    "- **‚è±Ô∏è Server timing traces**: Detailed execution path and performance metrics\n",
    "- **üß† Memory impact analysis**: See which columns become \"HOT\" and enter memory\n",
    "- **üõ°Ô∏è Fallback detection**: Monitor if/when queries fall back to SQL Endpoint\n",
    "\n",
    "### Query Progression Strategy\n",
    "\n",
    "#### Graduated Scale Testing:\n",
    "1. **1 billion rows**: Establish baseline performance\n",
    "2. **2 billion rows**: Test Direct Lake limits\n",
    "3. **Combined analysis**: Multi-table billion-row queries\n",
    "\n",
    "This progression helps identify **performance thresholds** and **fallback triggers**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235fe7b5-d8b4-4ed6-9874-6875962859b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### 16.1 Baseline Performance: 1 Billion Row Analytics\n",
    "\n",
    "### Query Analysis: 1 Billion Row Table\n",
    "This query establishes **baseline performance** for billion-row Direct Lake analytics:\n",
    "\n",
    "```dax\n",
    "EVALUATE\n",
    "SUMMARIZECOLUMNS(\n",
    "    dim_Date[FirstDateofMonth],\n",
    "    \"Count of Transactions\", COUNTROWS(fact_myevents_1bln),\n",
    "    \"Sum of Sales\", [Sum of Sales (1bln)]\n",
    ")\n",
    "ORDER BY [FirstDateofMonth]\n",
    "```\n",
    "\n",
    "#### Query Component Analysis:\n",
    "\n",
    "##### **SUMMARIZECOLUMNS()** with 1B Rows:\n",
    "- **Cross-table aggregation**: Joins 1B fact rows with date dimension\n",
    "- **Memory impact**: Will load DateKey column from billion-row table\n",
    "- **Performance**: Tests Direct Lake's ability to handle large-scale aggregations\n",
    "\n",
    "##### **COUNTROWS(fact_myevents_1bln)**:\n",
    "- **Row counting**: Counts all 1 billion rows efficiently\n",
    "- **Optimization**: Should leverage table metadata rather than scanning all rows\n",
    "- **Expected performance**: Fast due to columnar storage optimization\n",
    "\n",
    "##### **[Sum of Sales (1bln)]**:\n",
    "- **Measure evaluation**: Aggregates Quantity_ThisYear from 1B rows\n",
    "- **Memory loading**: Will bring billion-row numeric column into memory\n",
    "- **Critical test**: Primary test of Direct Lake's billion-row aggregation capability\n",
    "\n",
    "### Expected Performance Characteristics\n",
    "\n",
    "#### Direct Lake Success Scenario:\n",
    "- **Execution time**: 5-30 seconds depending on memory and data distribution\n",
    "- **Memory usage**: DateKey + Quantity_ThisYear columns loaded\n",
    "- **Result accuracy**: Exact aggregation across all billion rows\n",
    "- **Temperature change**: Related columns become \"HOT\"\n",
    "\n",
    "#### Potential Fallback Scenarios:\n",
    "- **Memory exhaustion**: If Quantity_ThisYear column too large for available memory\n",
    "- **Cardinality issues**: If unique values exceed Direct Lake limits\n",
    "- **Resource contention**: If multiple users accessing simultaneously\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "#### Success Indicators:\n",
    "- ‚úÖ **Query completes successfully** with reasonable performance\n",
    "- ‚úÖ **Server traces show DirectLake** execution path\n",
    "- ‚úÖ **DMV shows columns loaded** with \"HOT\" temperature\n",
    "- ‚úÖ **Results show aggregated data** by month\n",
    "\n",
    "#### Performance Insights:\n",
    "- üìä **Monthly aggregation results**: Business insights from billion-row dataset\n",
    "- ‚è±Ô∏è **Execution timing**: Baseline for subsequent larger queries\n",
    "- üß† **Memory impact**: See exact memory footprint of billion-row columns\n",
    "\n",
    "**Expected outcome**: Successful billion-row aggregation demonstrating Direct Lake's capability with large-scale datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbd807-3013-40a4-b584-fda3428ab4be",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "    \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "               \n",
    "                dim_Date[FirstDateofMonth] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(fact_myevents_1bln) ,\n",
    "                \"Sum of Sales\" , [Sum of Sales (1bln)] \n",
    "        )\n",
    "        ORDER BY [FirstDateofMonth]\n",
    "\n",
    "\"\"\",workspaceName,SemanticModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4efa57f-c533-4c4b-ab38-a982e962640f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### 16.2 Stress Test: 2 Billion Row Analytics\n",
    "\n",
    "### Pushing Direct Lake to Its Limits\n",
    "This query tests **Direct Lake's maximum scale capabilities** with 2 billion rows:\n",
    "\n",
    "```dax\n",
    "EVALUATE\n",
    "SUMMARIZECOLUMNS(\n",
    "    dim_Date[FirstDateofMonth],\n",
    "    \"Count of Transactions\", COUNTROWS(fact_myevents_2bln),\n",
    "    \"Sum of Sales\", [Sum of Sales (2bln)]\n",
    ")\n",
    "ORDER BY [FirstDateofMonth]\n",
    "```\n",
    "\n",
    "#### Critical Scale Factors:\n",
    "\n",
    "##### **2 Billion Row Impact**:\n",
    "- **Memory pressure**: Quantity_ThisYear column from 2B rows may be 8-16GB\n",
    "- **Guardrail testing**: Likely to approach or exceed Direct Lake memory limits\n",
    "- **Fallback probability**: Higher chance of SQL Endpoint fallback\n",
    "\n",
    "##### **Performance Comparison**:\n",
    "| Metric | 1B Rows | 2B Rows | Expected Impact |\n",
    "|--------|---------|---------|-----------------|\n",
    "| **Memory usage** | 4-8GB | 8-16GB | 2x increase |\n",
    "| **Execution time** | 5-30s | 10-60s or fallback | Variable |\n",
    "| **Fallback risk** | Low | Moderate-High | Depends on capacity |\n",
    "\n",
    "### Expected Scenarios\n",
    "\n",
    "#### Scenario 1: Direct Lake Success (Best Case)\n",
    "- **Memory sufficient**: Available memory can handle 2B-row column\n",
    "- **Performance**: Slower than 1B but still reasonable (30-60 seconds)\n",
    "- **Traces show**: DirectLake execution path maintained\n",
    "- **DMV impact**: Large memory allocation for Quantity_ThisYear column\n",
    "\n",
    "#### Scenario 2: SQL Endpoint Fallback (Common Case)\n",
    "- **Memory exhaustion**: 2B-row column exceeds available memory\n",
    "- **Automatic fallback**: Direct Lake gracefully falls back to SQL Endpoint\n",
    "- **Performance**: May be slower but still functional\n",
    "- **Traces show**: Fallback events and SQL execution\n",
    "\n",
    "#### Scenario 3: Resource Limits (Stress Case)\n",
    "- **Capacity constraints**: Multiple factors trigger protective fallback\n",
    "- **Error handling**: System protects against memory exhaustion\n",
    "- **Learning opportunity**: Understand Direct Lake boundaries\n",
    "\n",
    "### Fallback Analysis Opportunity\n",
    "This query is **designed to explore fallback behavior**:\n",
    "\n",
    "#### Key Learning Points:\n",
    "- **When fallback occurs**: Exact conditions that trigger SQL Endpoint usage\n",
    "- **Performance impact**: How fallback affects query execution time\n",
    "- **User experience**: Fallback is transparent to end users\n",
    "- **Resource protection**: How Fabric protects against memory exhaustion\n",
    "\n",
    "### Monitoring Strategy\n",
    "With `DMV=False` parameter:\n",
    "- **Focus on traces**: Detailed execution path analysis\n",
    "- **Reduced output**: Cleaner focus on performance characteristics\n",
    "- **Fallback detection**: Clear visibility into execution mode changes\n",
    "\n",
    "**Expected outcome**: Either successful 2-billion row Direct Lake aggregation or educational fallback scenario demonstrating system limits and protection mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4111c0-e717-49c7-9bdc-3d4a96ea96b9",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "                dim_Date[FirstDateofMonth] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(fact_myevents_2bln) ,\n",
    "                \"Sum of Sales\" , [Sum of Sales (2bln)]\n",
    "        )\n",
    "        ORDER BY [FirstDateofMonth]\n",
    "\n",
    "\"\"\",workspaceName,SemanticModelName,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2391da99-a7d1-417c-86cb-600cd7ae92e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### 16.3 Ultimate Stress Test: Multi-Billion Row Cross-Table Analysis\n",
    "\n",
    "### Maximum Scale Multi-Table Query\n",
    "This advanced query represents the **ultimate Direct Lake stress test** by combining both billion-row tables in a single analysis:\n",
    "\n",
    "```dax\n",
    "EVALUATE\n",
    "SUMMARIZECOLUMNS(\n",
    "    dim_Date[FirstDateofMonth],\n",
    "    \"Count of Transactions\", COUNTROWS(fact_myevents_1bln),\n",
    "    \"Sum of Sales (1bln)\", [Sum of Sales (1bln)],\n",
    "    \"Sum of Sales (2bln)\", [Sum of Sales (2bln)]\n",
    ")\n",
    "ORDER BY [FirstDateofMonth]\n",
    "```\n",
    "\n",
    "#### Multi-Table Complexity Analysis:\n",
    "\n",
    "##### **Memory Amplification**:\n",
    "- **1B + 2B columns**: Potentially 12-24GB total memory requirement\n",
    "- **Concurrent loading**: Both Quantity_ThisYear columns may load simultaneously\n",
    "- **Relationship columns**: Multiple DateKey columns from fact tables\n",
    "\n",
    "##### **Cross-Table Performance**:\n",
    "- **Parallel processing**: Direct Lake may optimize multi-table access\n",
    "- **Memory coordination**: Smart loading of shared dimension data\n",
    "- **Resource management**: Advanced memory allocation strategies\n",
    "\n",
    "### Expected Execution Scenarios\n",
    "\n",
    "#### Scenario 1: Direct Lake Optimization Success\n",
    "- **Intelligent caching**: Shared dimensions loaded once\n",
    "- **Parallel aggregation**: Both fact tables processed efficiently\n",
    "- **Memory management**: Optimal allocation across multiple large columns\n",
    "- **Performance**: Reasonable execution time despite scale\n",
    "\n",
    "#### Scenario 2: Selective Fallback\n",
    "- **Partial fallback**: 2B table falls back while 1B remains Direct Lake\n",
    "- **Mixed execution**: Demonstrates Direct Lake's adaptive behavior\n",
    "- **Performance variation**: Different execution paths for different tables\n",
    "\n",
    "#### Scenario 3: Complete Fallback\n",
    "- **System protection**: Total memory requirements exceed capacity\n",
    "- **Full SQL Endpoint**: All tables processed via SQL Analytics Endpoint\n",
    "- **Consistent results**: Same analytical output via different execution path\n",
    "\n",
    "### Business Value Demonstration\n",
    "\n",
    "#### Comparative Analysis Capability:\n",
    "This query demonstrates **real-world business scenarios**:\n",
    "- **Trend comparison**: Compare current (1B) vs. historical (2B) data patterns\n",
    "- **Scale analysis**: Understand business growth impact on performance\n",
    "- **Performance baselines**: Establish benchmarks for production workloads\n",
    "\n",
    "#### Enterprise Readiness Validation:\n",
    "- **Concurrent workload**: Tests realistic multi-user scenarios\n",
    "- **Resource scaling**: Validates Direct Lake behavior under pressure\n",
    "- **Fallback reliability**: Confirms transparent degradation when needed\n",
    "\n",
    "### Advanced Monitoring Insights\n",
    "\n",
    "#### Memory Orchestration:\n",
    "Watch for:\n",
    "- **Load sequencing**: Order in which large columns are loaded\n",
    "- **Memory sharing**: How dimension tables are cached across fact table queries\n",
    "- **Resource coordination**: System-level memory management decisions\n",
    "\n",
    "#### Performance Patterns:\n",
    "- **Execution parallelism**: Whether fact tables are processed concurrently\n",
    "- **Optimization strategies**: How Direct Lake handles multi-billion row scenarios\n",
    "- **Fallback coordination**: If fallback occurs, how it's managed across tables\n",
    "\n",
    "**Expected outcome**: Ultimate demonstration of Direct Lake's enterprise-scale capabilities or intelligent fallback behavior when approaching system limits.\n",
    "\n",
    "üéØ **Maximum scale checkpoint**: Multi-billion row analytics showcasing Direct Lake's ultimate capabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b3970-3eb3-46ae-a3e3-94ad6072c682",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "                dim_Date[FirstDateofMonth] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(fact_myevents_1bln) ,\n",
    "                \"Sum of Sales (1bln)\" , [Sum of Sales (1bln)] ,\n",
    "                \"Sum of Sales (2bln)\" , [Sum of Sales (2bln)]\n",
    "        )\n",
    "        ORDER BY [FirstDateofMonth]\n",
    "\n",
    "\"\"\",workspaceName,SemanticModelName,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463abb6d",
   "metadata": {},
   "source": [
    "## 17. Lab 2 Completion and Big Data Insights\n",
    "\n",
    "### Congratulations! Big Data Mastery Achieved üéâ\n",
    "\n",
    "You've successfully completed the most challenging Direct Lake lab, working with **billion-row datasets** and pushing the technology to its limits.\n",
    "\n",
    "#### üöÄ **What You've Accomplished**:\n",
    "- ‚úÖ **Cross-workspace data access** via OneLake shortcuts\n",
    "- ‚úÖ **Billion-row semantic model** creation and configuration\n",
    "- ‚úÖ **Advanced performance monitoring** with tracing and DMVs\n",
    "- ‚úÖ **Fallback scenario analysis** understanding Direct Lake limits\n",
    "- ‚úÖ **Multi-table billion-row analytics** stress testing\n",
    "\n",
    "### Key Direct Lake Big Data Learnings\n",
    "\n",
    "#### üìä **Scale Capabilities**:\n",
    "- **Direct Lake can handle billion-row tables** when properly configured\n",
    "- **OneLake shortcuts enable zero-copy big data access** across workspaces\n",
    "- **Intelligent fallback protects against memory exhaustion** while maintaining functionality\n",
    "- **Performance monitoring tools** provide deep insights into large-scale operations\n",
    "\n",
    "#### üõ°Ô∏è **Guardrails Understanding**:\n",
    "- **Memory limits protect system stability** while maximizing performance\n",
    "- **Automatic fallback to SQL Endpoint** ensures query reliability\n",
    "- **Column temperature tracking** shows memory usage patterns\n",
    "- **Cross-workspace shortcuts** maintain performance with proper configuration\n",
    "\n",
    "#### ‚ö° **Performance Insights**:\n",
    "- **V-Order optimization** significantly improves billion-row query performance\n",
    "- **Partitioning strategies** can enhance large-scale analytics\n",
    "- **Memory management** becomes critical at enterprise scale\n",
    "- **Query design patterns** impact Direct Lake vs. fallback behavior\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "#### Enterprise Scenarios You're Now Ready For:\n",
    "- **üìà Historical trend analysis** across years of transactional data\n",
    "- **üåç Global analytics** combining data from multiple regions/workspaces\n",
    "- **üìä Real-time dashboards** over massive operational datasets\n",
    "- **üîç Detailed forensic analysis** of billion-row audit logs\n",
    "\n",
    "### Next Steps in Your Big Data Journey\n",
    "\n",
    "#### üéØ **Immediate Exploration**:\n",
    "- Experiment with different query patterns to understand fallback triggers\n",
    "- Create Power BI reports using your billion-row semantic model\n",
    "- Test concurrent user scenarios to understand multi-user performance\n",
    "\n",
    "#### üìö **Advanced Learning Path**:\n",
    "- **Lab 3**: Delta table analysis and optimization techniques\n",
    "- **Lab 4**: Deep dive into fallback behaviors and troubleshooting\n",
    "- **Lab 5**: Framing and refresh strategies for big data\n",
    "- **Lab 6-7**: Performance optimization techniques for billion-row scenarios\n",
    "\n",
    "### Resource Cleanup Importance\n",
    "Stopping the Spark session is crucial after big data operations to:\n",
    "- **üí∞ Release expensive compute resources** used for billion-row processing\n",
    "- **üßπ Free memory** allocated for large column dictionaries\n",
    "- **‚úÖ Clean up cross-workspace connections** properly\n",
    "\n",
    "### Big Data Direct Lake Mastery Certificate üèÜ\n",
    "You now understand:\n",
    "- ‚úÖ **Enterprise-scale Direct Lake** capabilities and limitations\n",
    "- ‚úÖ **Performance monitoring** for billion-row scenarios\n",
    "- ‚úÖ **Fallback behavior** and system protection mechanisms\n",
    "- ‚úÖ **Cross-workspace big data** architecture with OneLake shortcuts\n",
    "\n",
    "**Ready for production big data analytics with Direct Lake!** üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
