{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b3d893-db98-4929-97a7-40148f6269a4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 2: Direct Lake with Big Data - Billion Row Analytics\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "This lab demonstrates Direct Lake's **enterprise-scale capabilities** by working with **billion-row datasets**. You'll learn how Direct Lake handles massive data volumes, create OneLake shortcuts for cross-workspace data access, and understand when Direct Lake falls back to SQL Endpoint mode.\n",
    "\n",
    "### What You'll Build\n",
    "\n",
    "**Workshop Flow:**\n",
    "```\n",
    "1. Setup Big Data Environment\n",
    "   ‚Üì\n",
    "2. Create OneLake Shortcuts\n",
    "   ‚Üì\n",
    "3. Build Billion-Row Semantic Model\n",
    "   ‚Üì\n",
    "4. Performance Testing & Tracing\n",
    "   ‚Üì\n",
    "5. Analyze Fallback Scenarios\n",
    "   ‚Üì\n",
    "6. Optimize for Scale\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "- **OneLake Shortcuts**: Access data across workspaces without duplication\n",
    "- **Direct Lake Guardrails**: Understanding billion-row table limits\n",
    "- **Fallback Behavior**: When Direct Lake uses SQL Endpoint mode\n",
    "- **Column Temperature**: Memory management for large datasets\n",
    "\n",
    "### Learning Objectives\n",
    "By completing this lab, you'll be able to:\n",
    "- ‚úÖ Create OneLake shortcuts for cross-workspace data access\n",
    "- ‚úÖ Build semantic models with billion-row fact tables\n",
    "- ‚úÖ Monitor Direct Lake performance with advanced tracing\n",
    "- ‚úÖ Understand and troubleshoot fallback scenarios\n",
    "- ‚úÖ Optimize queries for massive datasets\n",
    "\n",
    "### Dataset Scale\n",
    "| Table | Rows | Purpose |\n",
    "|:------|:-----|:--------|\n",
    "| **fact_myevents_1bln** | 1 billion | Standard fact table |\n",
    "| **fact_myevents_2bln** | 2 billion | Stress test limits |\n",
    "| **fact_myevents_1bln_partitioned_datekey** | 1 billion | Optimized with partitioning |\n",
    "| **dim_Date** | ~3,650 | Date dimension |\n",
    "| **dim_Geography** | ~200 | Geography dimension |\n",
    "\n",
    "**Estimated Time**: 60-90 minutes  \n",
    "**Prerequisites**: Lab 1 completion, access to Big Data workspace\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd099e87-89f6-45b2-b286-d7f8a12b6a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "Install Semantic Link Labs with enhanced big data capabilities for billion-row analytics and OneLake shortcut management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ed5ca-a6ba-478a-b029-17b6db9b6308",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc44ce-395c-4db3-8979-b06acf9f8ecf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2. Import Libraries and Set Variables\n",
    "\n",
    "Import required Python libraries and configure environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e841d6-a757-4029-aa71-88d4bd286c30",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as labs\n",
    "from sempy import fabric\n",
    "import sempy\n",
    "import pandas\n",
    "import json\n",
    "import time\n",
    "\n",
    "LakehouseName = \"BigData\"\n",
    "SemanticModelName = f\"{LakehouseName}_model\"\n",
    "\n",
    "capacity_name = labs.get_capacity_name()\n",
    "\n",
    "Shortcut_LakehouseName = \"BigDemoDB\"\n",
    "Shortcut_WorkspaceName = \"DL Labs - Data [North Central US]\"\n",
    "if capacity_name == \"FabConUS8-P1\":\n",
    "    Shortcut_WorkspaceName = \"DL Labs - Data [West US 3]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99f94c-a564-4aca-b670-31da354b7b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3. Create Lakehouse for Big Data\n",
    "\n",
    "Create a lightweight lakehouse that will use OneLake shortcuts to access billion-row tables without data duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760df625-543c-4289-b6cc-f043290d5879",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "lakehouses=labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "if LakehouseName in lakehouses.values:\n",
    "    lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]\n",
    "else:\n",
    "    lakehouseId = fabric.create_lakehouse(LakehouseName)\n",
    "\n",
    "workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "workspaceName = sempy.fabric.resolve_workspace_name(workspaceId)\n",
    "print(f\"WorkspaceId = {workspaceId}, LakehouseID = {lakehouseId}, Workspace Name = {workspaceName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d9d90-0444-472b-a870-ed4a2425227a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Create OneLake Shortcuts for Big Data Access\n",
    "\n",
    "Creates shortcuts to billion-row fact tables and dimension tables across workspaces using OneLake shortcut functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3367b-c08a-4104-b04c-1410d8850f51",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#1. Remove any existing shortcuts\n",
    "for index, row in labs.lakehouse.list_shortcuts(lakehouse=LakehouseName).iterrows():\n",
    "    labs.lakehouse.delete_shortcut(shortcut_name=row[\"Shortcut Name\"],lakehouse=LakehouseName)\n",
    "    print(f\"Deleted shortcut {row['Shortcut Name']}\")\n",
    "\n",
    "#2. Creates correct shortcuts\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"fact_myevents_1bln\"                      ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"fact_myevents_1bln_no_vorder\"            ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"fact_myevents_1bln_partitioned_datekey\"  ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"fact_myevents_2bln\"                      ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"dim_Date\"                                ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"dim_Geography\"                           ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "\n",
    "print('Adding shortcuts complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff2996-0b1c-4792-99cd-8d61315e65da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 5. Synchronize Big Data Table Metadata\n",
    "\n",
    "Forces lakehouse metadata refresh to recognize billion-row shortcuts and their schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb2e8e-a2b9-411f-b69d-3ea129e06c10",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Triggers REST API call to refresh lakehouse metadata and table schemas.\n",
    "3. **üìä Progress Monitoring**: Poll batch status every second until success\n",
    "4. **‚úÖ Completion Validation**: Confirm all billion-row tables are properly cataloged\n",
    "\n",
    "**Expected behavior**: Periodic \"running\" status updates followed by \"success\" for complete metadata sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54e1aa-a94e-4efd-b40e-f497f54589e8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "##https://medium.com/@sqltidy/delays-in-the-automatically-generated-schema-in-the-sql-analytics-endpoint-of-the-lakehouse-b01c7633035d\n",
    "\n",
    "def triggerMetadataRefresh():\n",
    "    client = fabric.FabricRestClient()\n",
    "    response = client.get(f\"/v1/workspaces/{workspaceId}/lakehouses/{lakehouseId}\")\n",
    "    sqlendpoint = response.json()['properties']['sqlEndpointProperties']['id']\n",
    "\n",
    "    # trigger sync\n",
    "    uri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}\"\n",
    "    payload = {\"commands\":[{\"$type\":\"MetadataRefreshExternalCommand\"}]}\n",
    "    response = client.post(uri,json= payload)\n",
    "    batchId = response.json()['batchId']\n",
    "\n",
    "    # Monitor Progress\n",
    "    statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "    statusresponsedata = client.get(statusuri).json()\n",
    "    progressState = statusresponsedata['progressState']\n",
    "    print(f\"Metadata refresh : {progressState}\")\n",
    "    while progressState != \"success\":\n",
    "        statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "        statusresponsedata = client.get(statusuri).json()\n",
    "        progressState = statusresponsedata['progressState']\n",
    "        print(f\"Metadata refresh : {progressState}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    print('Metadata refresh complete')\n",
    "\n",
    "triggerMetadataRefresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e16b1-9893-46d3-ac7d-93a4d1453c5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 6. Create \"Big Data\" Direct Lake Semantic Model\n",
    "\n",
    "Creates semantic model using billion-row shortcut tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e9cc7-de53-4c81-8408-9fb18ef4383a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sempy import fabric\n",
    "#1. Generate list of ALL table names from lakehouse to add to Semantic Model\n",
    "lakehouseTables:list = labs.lakehouse.get_lakehouse_tables(lakehouse=LakehouseName)[\"Table Name\"]\n",
    "\n",
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        #2 Create the semantic model (check if exists first)\n",
    "        if sempy.fabric.list_items().query(f\"`Display Name`=='{LakehouseName}_model' & Type=='SemanticModel'  \").shape[0] ==0:\n",
    "            labs.directlake.generate_direct_lake_semantic_model(dataset=f\"{LakehouseName}_model\",lakehouse_tables=lakehouseTables,workspace=workspaceName,lakehouse=lakehouseId,refresh=False,overwrite=True)\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error creating model... trying again.')\n",
    "        time.sleep(3)\n",
    "        triggerMetadataRefresh()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df3373-52ca-4e45-83a6-40e1b20c184d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 7. Configure Relationships for Big Data semantic model\n",
    "\n",
    "Create relationships in the new semantic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616193a-57df-4139-91ef-c73830331555",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing relationships\n",
    "            for r in tom.model.Relationships:\n",
    "                tom.model.Relationships.Remove(r)\n",
    "\n",
    "            #2. Creates correct relationships\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln\"                    , from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln\"                    , from_column=\"GeographyID\" , to_table=\"dim_Geography\"  , to_column=\"GeographyID\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "\n",
    "            tom.add_relationship(from_table=\"fact_myevents_2bln\"                    , from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"fact_myevents_2bln\"                    , from_column=\"GeographyID\" , to_table=\"dim_Geography\"  , to_column=\"GeographyID\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln_partitioned_datekey\", from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln_partitioned_datekey\", from_column=\"GeographyID\" , to_table=\"dim_Geography\"  , to_column=\"GeographyID\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding relationships... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba5536-018c-402b-aa0f-663ee5e8d07f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 8. Create Measures for Big Data Semantic Model\n",
    "\n",
    "Add measures to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666631c-4221-4638-b8ff-10fee8a4f7df",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing measures\n",
    "            for t in tom.model.Tables:\n",
    "                for m in t.Measures:\n",
    "                    tom.remove_object(m)\n",
    "                    print(m.Name)\n",
    "\n",
    "            tom.add_measure(table_name=\"fact_myevents_2bln\",measure_name=\"Sum of Sales (2bln)\",expression=\"SUM(fact_myevents_2bln[Quantity_ThisYear])\",format_string=\"#,0\")\n",
    "            tom.add_measure(table_name=\"fact_myevents_1bln\",measure_name=\"Sum of Sales (1bln)\",expression=\"SUM(fact_myevents_1bln[Quantity_ThisYear])\",format_string=\"#,0\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding measures... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ff91f-0362-4c3e-bf2e-60c64867cec8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 9. Configure Date Intelligence for Big Data Analytics\n",
    "\n",
    "Marks dim_Date as date table to enable time intelligence functions for efficient billion-row time-based filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f48bf-893b-4a8f-92a1-e5e80b968840",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            tom.mark_as_date_table(table_name=\"dim_Date\",column_name=\"DateKey\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error with date table... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb148df-8490-47fc-8e9c-6b8cbcb5fb6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 10. Configure Logical Sorting for Big Data Visualizations\n",
    "\n",
    "Sets logical column sorting for date dimensions to ensure proper chronological ordering in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e3959-51e3-481c-b730-bde8482bd746",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        tom = labs.tom.TOMWrapper(dataset=SemanticModelName, workspace=workspaceName, readonly=False)\n",
    "        tom.set_sort_by_column(table_name=\"dim_Date\",column_name=\"MonthName\"       ,sort_by_column=\"Month\")\n",
    "        tom.set_sort_by_column(table_name=\"dim_Date\",column_name=\"WeekDayName\"     ,sort_by_column=\"Weekday\")\n",
    "        tom.model.SaveChanges()\n",
    "\n",
    "        #Show BIM data for dim_Date table\n",
    "        i:int=0\n",
    "        for t in tom.model.Tables:\n",
    "            if t.Name==\"dim_Date\":\n",
    "                bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "                print(bim)\n",
    "            i=i+1\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error with sort by cols... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7babd-5053-4dc5-97e1-4398e67dba26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 11. Optimize Big Data Model by Hiding Fact Table Columns\n",
    "\n",
    "Hides fact table columns for better user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e49ec-bd01-467f-ad6f-5bff8f068fcc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        i:int=0\n",
    "        for t in tom.model.Tables:\n",
    "            if t.Name in [\"fact_myevents_1bln\",\"fact_myevents_2bln\",\"fact_myevents_1bln_partitioned_datekey\"]:\n",
    "                for c in t.Columns:\n",
    "                    c.IsHidden=True\n",
    "\n",
    "                bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "                print(bim)\n",
    "            i=i+1\n",
    "        tom.model.SaveChanges()\n",
    "        completedOK=True\n",
    "    except:\n",
    "        print('Error with hiding cols... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc341d-8682-4a4d-8b00-300d8723bcfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 12. Frame Big Data Model\n",
    "\n",
    "Frame semantic model to be ready for queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed6756-a8b0-4250-9062-3266ae563054",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "reframeOK:bool=False\n",
    "while not reframeOK:\n",
    "    try:\n",
    "        result:pandas.DataFrame = labs.refresh_semantic_model(dataset=SemanticModelName)\n",
    "        reframeOK=True\n",
    "    except:\n",
    "        print('Error with reframe... trying again.')\n",
    "        triggerMetadataRefresh()\n",
    "        time.sleep(3)\n",
    "\n",
    "print('Custom Semantic Model reframe OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f25b5-949d-4794-b565-f4d8fea1c089",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 13. Create helper functions for later steps\n",
    "\n",
    "Creates enhanced tracing function for comprehensive big data performance monitoring and fallback detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab34d2-784f-4a4a-bf61-afdabb4c2b69",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "\n",
    "#### Generate Unique Trace Name - Start ####\n",
    "import json, base64\n",
    "token = notebookutils.credentials.getToken(\"pbi\")\n",
    "payload = token.split(\".\")[1]\n",
    "payload += \"=\" * (4 - len(payload) % 4)\n",
    "upn = json.loads(base64.b64decode(payload)).get(\"upn\")\n",
    "\n",
    "# Extract just the user part (e.g. \"SQLKDL.user39\")\n",
    "user_id = upn.split(\"@\")[0]\n",
    "lab_number = 2  # set per lab\n",
    "\n",
    "trace_name = f\"Lab{lab_number}_{user_id}\"\n",
    "#### Generate Unique Trace Name - End ####\n",
    "\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)\n",
    "\n",
    "def filter_func(e):\n",
    "    retVal:bool=True\n",
    "    if e.EventSubclass.ToString() == \"VertiPaqScanInternal\":\n",
    "        retVal=False      \n",
    "    #     #if e.EventSubClass.ToString() == \"VertiPaqScanInternal\":\n",
    "    #     retVal=False\n",
    "    return retVal\n",
    "\n",
    "# define events to trace and their corresponding columns\n",
    "def runQueryWithTrace (expr:str,workspaceName:str,SemanticModelName:str,Result:Optional[bool]=True,Trace:Optional[bool]=True,DMV:Optional[bool]=True,ClearCache:Optional[bool]=True) -> pandas.DataFrame :\n",
    "    event_schema = fabric.Trace.get_default_query_trace_schema()\n",
    "    event_schema.update({\"ExecutionMetrics\":[\"EventClass\",\"TextData\"]})\n",
    "    del event_schema['VertiPaqSEQueryBegin']\n",
    "    del event_schema['VertiPaqSEQueryCacheMatch']\n",
    "    del event_schema['DirectQueryBegin']\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    WorkspaceName = workspaceName\n",
    "    SemanticModelName = SemanticModelName\n",
    "\n",
    "    if ClearCache:\n",
    "        labs.clear_cache(SemanticModelName)\n",
    "\n",
    "    with fabric.create_trace_connection(SemanticModelName,WorkspaceName) as trace_connection:\n",
    "        # create trace on server with specified events\n",
    "        with trace_connection.create_trace(\n",
    "            event_schema=event_schema, \n",
    "            name=trace_name,\n",
    "            filter_predicate=filter_func,\n",
    "            stop_event=\"QueryEnd\"\n",
    "            ) as trace:\n",
    "\n",
    "            trace.start()\n",
    "\n",
    "            df=sempy.fabric.evaluate_dax(\n",
    "                dataset=SemanticModelName, \n",
    "                dax_string=expr)\n",
    "\n",
    "            if Result:\n",
    "                displayHTML(f\"<H2>####### DAX QUERY RESULT #######</H2>\")\n",
    "                display(df)\n",
    "\n",
    "            # Wait 5 seconds for trace data to arrive\n",
    "            time.sleep(5)\n",
    "\n",
    "            # stop Trace and collect logs\n",
    "            final_trace_logs = trace.stop()\n",
    "\n",
    "    if Trace:\n",
    "        displayHTML(f\"<H2>####### SERVER TIMINGS #######</H2>\")\n",
    "        display(final_trace_logs)\n",
    "    \n",
    "    if DMV:\n",
    "        displayHTML(f\"<H2>####### SHOW DMV RESULTS #######</H2>\")\n",
    "        runDMV()\n",
    "    \n",
    "    return final_trace_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4e8d4",
   "metadata": {},
   "source": [
    "## 14. Review Table Traits for issues\n",
    "\n",
    "Validates model configuration and Direct Lake mode operation for billion-row tables using TABLETRAITS and displays guardrails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf138f4-df36-41c4-bb58-a6af71edd179",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=sempy.fabric.evaluate_dax(\n",
    "    dataset=SemanticModelName, \n",
    "    dax_string=\"\"\"\n",
    "    \n",
    "    evaluate tabletraits()\n",
    "    \n",
    "    \"\"\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3394566-f8a8-4ee3-885b-766db5e8a615",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=labs.directlake.get_direct_lake_guardrails()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae67a8-b41c-4c18-8898-aaf4e478e599",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 15. Establish Big Data column residency baseline\n",
    "\n",
    "Establishes baseline for billion-row analytics by capturing initial column states and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223acea6-6ac3-4f3a-b04d-603309daf706",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2dcd3-fe61-4466-b7dd-f746bac1e05a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 16. Execute Billion-Row Analytics with Performance Monitoring\n",
    "\n",
    "Executes billion-row analytics with detailed performance monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235fe7b5-d8b4-4ed6-9874-6875962859b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### 16.1 Baseline Performance: 1 Billion Row Analytics\n",
    "\n",
    "Executes baseline query against 1 billion row table to establish Direct Lake performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbd807-3013-40a4-b584-fda3428ab4be",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "    \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "               \n",
    "                dim_Date[FirstDateofMonth] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(fact_myevents_1bln) ,\n",
    "                \"Sum of Sales\" , [Sum of Sales (1bln)] \n",
    "        )\n",
    "        ORDER BY [FirstDateofMonth]\n",
    "\n",
    "\"\"\",workspaceName,SemanticModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4efa57f-c533-4c4b-ab38-a982e962640f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### 16.2 Baseline Performance: 2 Billion Row Analytics\n",
    "\n",
    "Tests Direct Lake limits with 2 billion row query to explore fallback behavior and performance boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4111c0-e717-49c7-9bdc-3d4a96ea96b9",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "                dim_Date[FirstDateofMonth] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(fact_myevents_2bln) ,\n",
    "                \"Sum of Sales\" , [Sum of Sales (2bln)]\n",
    "        )\n",
    "        ORDER BY [FirstDateofMonth]\n",
    "\n",
    "\"\"\",workspaceName,SemanticModelName,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2391da99-a7d1-417c-86cb-600cd7ae92e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### 16.3 Ultimate Stress Test: Multi-Billion Row Cross-Table Analysis\n",
    "\n",
    "Executes query combining both billion-row tables to test Direct Lake capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b3970-3eb3-46ae-a3e3-94ad6072c682",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "                dim_Date[FirstDateofMonth] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(fact_myevents_1bln) ,\n",
    "                \"Sum of Sales (1bln)\" , [Sum of Sales (1bln)] ,\n",
    "                \"Sum of Sales (2bln)\" , [Sum of Sales (2bln)]\n",
    "        )\n",
    "        ORDER BY [FirstDateofMonth]\n",
    "\n",
    "\"\"\",workspaceName,SemanticModelName,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69f4342",
   "metadata": {},
   "source": [
    "## 17. Lab 2 Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463abb6d",
   "metadata": {},
   "source": [
    "### Congratulations! Big Data Mastery Achieved üéâ\n",
    "\n",
    "You've successfully completed the most challenging Direct Lake lab, working with **billion-row datasets** and pushing the technology to its limits.\n",
    "\n",
    "#### üöÄ **What You've Accomplished**:\n",
    "- ‚úÖ **Cross-workspace data access** via OneLake shortcuts\n",
    "- ‚úÖ **Billion-row semantic model** creation and configuration\n",
    "- ‚úÖ **Advanced performance monitoring** with tracing and DMVs\n",
    "- ‚úÖ **Fallback scenario analysis** understanding Direct Lake limits\n",
    "- ‚úÖ **Multi-table billion-row analytics** stress testing\n",
    "\n",
    "### Key Direct Lake Big Data Learnings\n",
    "\n",
    "#### üìä **Scale Capabilities**:\n",
    "- **Direct Lake can handle billion-row tables** when properly configured\n",
    "- **OneLake shortcuts enable zero-copy big data access** across workspaces\n",
    "- **Intelligent fallback protects against memory exhaustion** while maintaining functionality\n",
    "- **Performance monitoring tools** provide deep insights into large-scale operations\n",
    "\n",
    "#### üõ°Ô∏è **Guardrails Understanding**:\n",
    "- **Memory limits protect system stability** while maximizing performance\n",
    "- **Automatic fallback to SQL Endpoint** ensures query reliability\n",
    "- **Column temperature tracking** shows memory usage patterns\n",
    "- **Cross-workspace shortcuts** maintain performance with proper configuration\n",
    "\n",
    "#### ‚ö° **Performance Insights**:\n",
    "- **V-Order optimization** significantly improves billion-row query performance\n",
    "- **Partitioning strategies** can enhance large-scale analytics\n",
    "- **Memory management** becomes critical at enterprise scale\n",
    "- **Query design patterns** impact Direct Lake vs. fallback behavior\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "#### Enterprise Scenarios You're Now Ready For:\n",
    "- **üìà Historical trend analysis** across years of transactional data\n",
    "- **üåç Global analytics** combining data from multiple regions/workspaces\n",
    "- **üìä Real-time dashboards** over massive operational datasets\n",
    "- **üîç Detailed forensic analysis** of billion-row audit logs\n",
    "\n",
    "### Next Steps in Your Big Data Journey\n",
    "\n",
    "#### üéØ **Immediate Exploration**:\n",
    "- Experiment with different query patterns to understand fallback triggers\n",
    "- Create Power BI reports using your billion-row semantic model\n",
    "- Test concurrent user scenarios to understand multi-user performance\n",
    "\n",
    "#### üìö **Advanced Learning Path**:\n",
    "- **Lab 3**: Delta table analysis and optimization techniques\n",
    "- **Lab 4**: Deep dive into fallback behaviors and troubleshooting\n",
    "- **Lab 5**: Framing and refresh strategies for big data\n",
    "- **Lab 6-7**: Performance optimization techniques for billion-row scenarios\n",
    "\n",
    "### Resource Cleanup Importance\n",
    "Stopping the Spark session is crucial after big data operations to:\n",
    "- **üí∞ Release expensive compute resources** used for billion-row processing\n",
    "- **üßπ Free memory** allocated for large column dictionaries\n",
    "- **‚úÖ Clean up cross-workspace connections** properly\n",
    "\n",
    "### Big Data Direct Lake Mastery Certificate üèÜ\n",
    "You now understand:\n",
    "- ‚úÖ **Enterprise-scale Direct Lake** capabilities and limitations\n",
    "- ‚úÖ **Performance monitoring** for billion-row scenarios\n",
    "- ‚úÖ **Fallback behavior** and system protection mechanisms\n",
    "- ‚úÖ **Cross-workspace big data** architecture with OneLake shortcuts\n",
    "\n",
    "**Ready for production big data analytics with Direct Lake!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
