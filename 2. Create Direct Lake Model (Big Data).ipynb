{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b3d893-db98-4929-97a7-40148f6269a4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 2: Direct Lake with Big Data\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lab demonstrates Direct Lake at enterprise scale by working with billion-row datasets. You will create OneLake shortcuts for cross-workspace data access, build a semantic model over massive tables, and use tracing to observe how Direct Lake handles queries that exceed its guardrails.\n",
    "\n",
    "### Workshop Flow\n",
    "\n",
    "1. Create a lakehouse with OneLake shortcuts to billion-row tables\n",
    "2. Build a Direct Lake semantic model over the shortcut tables\n",
    "3. Define relationships and measures for the big data model\n",
    "4. Run queries against billion-row tables with performance tracing\n",
    "5. Observe fallback behaviour when guardrails are exceeded\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **OneLake shortcuts** provide access to data in other workspaces without copying it\n",
    "- **Direct Lake guardrails** define the row and memory limits for each capacity SKU\n",
    "- **Fallback behaviour** is the automatic switch to SQL Endpoint mode when limits are exceeded\n",
    "- **Column temperature** indicates whether a column is loaded into memory (hot) or not (cold)\n",
    "\n",
    "### Dataset Scale\n",
    "\n",
    "| Table | Rows | Purpose |\n",
    "|:------|:-----|:--------|\n",
    "| fact_myevents_1bln | 1 billion | Standard fact table |\n",
    "| fact_myevents_2bln | 2 billion | Exceeds guardrails to trigger fallback |\n",
    "| fact_myevents_1bln_partitioned_datekey | 1 billion | Partitioned variant for later labs |\n",
    "| dim_Date | ~3,650 | Date dimension |\n",
    "| dim_Geography | ~200 | Geography dimension |\n",
    "\n",
    "**Estimated duration:** 60-90 minutes  \n",
    "**Prerequisites:** Lab 1 completed, access to the Big Data workspace\n",
    "\n",
    "---\n",
    "\n",
    "*Deutsche Version:*\n",
    "\n",
    "# Lab 2: Direct Lake mit Big Data\n",
    "\n",
    "## Uebersicht\n",
    "\n",
    "Dieses Lab demonstriert Direct Lake im Unternehmensmassstab mit Milliarden-Zeilen-Datensaetzen. Sie erstellen OneLake-Shortcuts fuer workspace-uebergreifenden Datenzugriff, bauen ein Semantic Model ueber massiven Tabellen auf und verwenden Tracing, um zu beobachten, wie Direct Lake Abfragen behandelt, die seine Guardrails ueberschreiten.\n",
    "\n",
    "### Wichtige Konzepte\n",
    "\n",
    "- **OneLake-Shortcuts** bieten Zugriff auf Daten in anderen Workspaces, ohne sie zu kopieren\n",
    "- **Direct Lake Guardrails** definieren die Zeilen- und Speichergrenzen fuer jede Kapazitaets-SKU\n",
    "- **Fallback-Verhalten** ist der automatische Wechsel zum SQL Endpoint-Modus bei Ueberschreitung der Grenzen\n",
    "- **Spaltentemperatur** gibt an, ob eine Spalte in den Speicher geladen ist (hot) oder nicht (cold)\n",
    "\n",
    "**Geschaetzte Dauer:** 60-90 Minuten  \n",
    "**Voraussetzungen:** Lab 1 abgeschlossen, Zugang zum Big Data Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd099e87-89f6-45b2-b286-d7f8a12b6a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "Install the Semantic Link Labs library for Direct Lake model creation and OneLake shortcut management.\n",
    "\n",
    "---\n",
    "\n",
    "*Installieren Sie die Semantic Link Labs-Bibliothek fuer die Erstellung von Direct Lake-Modellen und die Verwaltung von OneLake-Shortcuts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ed5ca-a6ba-478a-b029-17b6db9b6308",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc44ce-395c-4db3-8979-b06acf9f8ecf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Import Libraries and Set Variables\n",
    "\n",
    "Import the required Python libraries and configure environment variables for the big data lakehouse and semantic model names.\n",
    "\n",
    "---\n",
    "\n",
    "*Importieren Sie die erforderlichen Python-Bibliotheken und konfigurieren Sie Umgebungsvariablen fuer den Big-Data-Lakehouse- und Semantic Model-Namen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e841d6-a757-4029-aa71-88d4bd286c30",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as labs\n",
    "from sempy import fabric\n",
    "import sempy\n",
    "import pandas\n",
    "import json\n",
    "import time\n",
    "\n",
    "LakehouseName = \"BigData\"\n",
    "SemanticModelName = f\"{LakehouseName}_model\"\n",
    "\n",
    "capacity_name = labs.get_capacity_name()\n",
    "\n",
    "Shortcut_LakehouseName = \"BigDemoDB\"\n",
    "Shortcut_WorkspaceName = \"DL Labs - Data [North Central US]\"\n",
    "if capacity_name == \"FabConUS8-P1\":\n",
    "    Shortcut_WorkspaceName = \"DL Labs - Data [West US 3]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99f94c-a564-4aca-b670-31da354b7b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Create Lakehouse for Big Data\n",
    "\n",
    "Create a new lakehouse that will host OneLake shortcuts to the billion-row tables. No data is physically copied; the shortcuts reference the source tables directly.\n",
    "\n",
    "---\n",
    "\n",
    "*Erstellen Sie ein neues Lakehouse, das OneLake-Shortcuts zu den Milliarden-Zeilen-Tabellen enthaelt. Es werden keine Daten physisch kopiert; die Shortcuts verweisen direkt auf die Quelltabellen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760df625-543c-4289-b6cc-f043290d5879",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "lakehouses=labs.list_lakehouses()[\"Lakehouse Name\"]\n",
    "if LakehouseName in lakehouses.values:\n",
    "    lakehouseId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"id\"]\n",
    "else:\n",
    "    lakehouseId = fabric.create_lakehouse(LakehouseName)\n",
    "\n",
    "workspaceId = notebookutils.lakehouse.getWithProperties(LakehouseName)[\"workspaceId\"]\n",
    "workspaceName = sempy.fabric.resolve_workspace_name(workspaceId)\n",
    "print(f\"WorkspaceId = {workspaceId}, LakehouseID = {lakehouseId}, Workspace Name = {workspaceName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d9d90-0444-472b-a870-ed4a2425227a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 4: Create OneLake Shortcuts\n",
    "\n",
    "Create shortcuts from the big data workspace to the current lakehouse. This gives the lakehouse access to billion-row fact tables and dimension tables without duplicating storage.\n",
    "\n",
    "---\n",
    "\n",
    "*Erstellen Sie Shortcuts vom Big-Data-Workspace zum aktuellen Lakehouse. Dadurch erhaelt das Lakehouse Zugriff auf Milliarden-Zeilen-Faktentabellen und Dimensionstabellen, ohne den Speicher zu duplizieren.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3367b-c08a-4104-b04c-1410d8850f51",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#1. Remove any existing shortcuts\n",
    "for index, row in labs.lakehouse.list_shortcuts(lakehouse=LakehouseName).iterrows():\n",
    "    labs.lakehouse.delete_shortcut(shortcut_name=row[\"Shortcut Name\"],lakehouse=LakehouseName)\n",
    "    print(f\"Deleted shortcut {row['Shortcut Name']}\")\n",
    "\n",
    "#2. Creates correct shortcuts\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"fact_myevents_1bln\"                      ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"fact_myevents_1bln_no_vorder\"            ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"fact_myevents_1bln_partitioned_datekey\"  ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"fact_myevents_2bln\"                      ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"dim_Date\"                                ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "labs.lakehouse.create_shortcut_onelake(table_name=\"dim_Geography\"                           ,source_lakehouse=Shortcut_LakehouseName,source_workspace=Shortcut_WorkspaceName,destination_lakehouse=LakehouseName)\n",
    "\n",
    "print('Adding shortcuts complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff2996-0b1c-4792-99cd-8d61315e65da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 5: Synchronise Table Metadata\n",
    "\n",
    "Trigger a metadata refresh so the SQL Analytics Endpoint recognises the newly created shortcuts and their schemas.\n",
    "\n",
    "---\n",
    "\n",
    "*Loesen Sie eine Metadaten-Aktualisierung aus, damit der SQL Analytics Endpoint die neu erstellten Shortcuts und deren Schemata erkennt.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb2e8e-a2b9-411f-b69d-3ea129e06c10",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "The metadata sync polls until the operation completes. Expect periodic \"running\" status messages followed by \"success\".\n",
    "\n",
    "---\n",
    "\n",
    "*Die Metadaten-Synchronisation fragt ab, bis der Vorgang abgeschlossen ist. Erwarten Sie periodische \"running\"-Statusmeldungen, gefolgt von \"success\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54e1aa-a94e-4efd-b40e-f497f54589e8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "##https://medium.com/@sqltidy/delays-in-the-automatically-generated-schema-in-the-sql-analytics-endpoint-of-the-lakehouse-b01c7633035d\n",
    "\n",
    "def triggerMetadataRefresh():\n",
    "    client = fabric.FabricRestClient()\n",
    "    response = client.get(f\"/v1/workspaces/{workspaceId}/lakehouses/{lakehouseId}\")\n",
    "    sqlendpoint = response.json()['properties']['sqlEndpointProperties']['id']\n",
    "\n",
    "    # trigger sync\n",
    "    uri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}\"\n",
    "    payload = {\"commands\":[{\"$type\":\"MetadataRefreshExternalCommand\"}]}\n",
    "    response = client.post(uri,json= payload)\n",
    "    batchId = response.json()['batchId']\n",
    "\n",
    "    # Monitor Progress\n",
    "    statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "    statusresponsedata = client.get(statusuri).json()\n",
    "    progressState = statusresponsedata['progressState']\n",
    "    print(f\"Metadata refresh : {progressState}\")\n",
    "    while progressState != \"success\":\n",
    "        statusuri = f\"/v1.0/myorg/lhdatamarts/{sqlendpoint}/batches/{batchId}\"\n",
    "        statusresponsedata = client.get(statusuri).json()\n",
    "        progressState = statusresponsedata['progressState']\n",
    "        print(f\"Metadata refresh : {progressState}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    print('Metadata refresh complete')\n",
    "\n",
    "triggerMetadataRefresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e16b1-9893-46d3-ac7d-93a4d1453c5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 6: Create the Big Data Semantic Model\n",
    "\n",
    "Generate a Direct Lake semantic model from the shortcut tables. This model will reference billion-row fact tables directly.\n",
    "\n",
    "---\n",
    "\n",
    "*Erstellen Sie ein Direct Lake Semantic Model aus den Shortcut-Tabellen. Dieses Modell verweist direkt auf Milliarden-Zeilen-Faktentabellen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e9cc7-de53-4c81-8408-9fb18ef4383a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from sempy import fabric\n",
    "#1. Generate list of ALL table names from lakehouse to add to Semantic Model\n",
    "lakehouseTables:list = labs.lakehouse.get_lakehouse_tables(lakehouse=LakehouseName)[\"Table Name\"]\n",
    "\n",
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        #2 Create the semantic model (check if exists first)\n",
    "        if sempy.fabric.list_items().query(f\"`Display Name`=='{LakehouseName}_model' & Type=='SemanticModel'  \").shape[0] ==0:\n",
    "            labs.directlake.generate_direct_lake_semantic_model(dataset=f\"{LakehouseName}_model\",lakehouse_tables=lakehouseTables,workspace=workspaceName,lakehouse=lakehouseId,refresh=False,overwrite=True)\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error creating model... trying again.')\n",
    "        time.sleep(3)\n",
    "        triggerMetadataRefresh()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df3373-52ca-4e45-83a6-40e1b20c184d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 7: Configure Relationships\n",
    "\n",
    "Define relationships between the billion-row fact tables and the dimension tables to enable cross-table filtering in queries.\n",
    "\n",
    "---\n",
    "\n",
    "*Definieren Sie Beziehungen zwischen den Milliarden-Zeilen-Faktentabellen und den Dimensionstabellen, um tabellenuebergreifendes Filtern in Abfragen zu ermoeglichen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616193a-57df-4139-91ef-c73830331555",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing relationships\n",
    "            for r in tom.model.Relationships:\n",
    "                tom.model.Relationships.Remove(r)\n",
    "\n",
    "            #2. Creates correct relationships\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln\"                    , from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln\"                    , from_column=\"GeographyID\" , to_table=\"dim_Geography\"  , to_column=\"GeographyID\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "\n",
    "            tom.add_relationship(from_table=\"fact_myevents_2bln\"                    , from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"fact_myevents_2bln\"                    , from_column=\"GeographyID\" , to_table=\"dim_Geography\"  , to_column=\"GeographyID\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln_partitioned_datekey\", from_column=\"DateKey\"     , to_table=\"dim_Date\"       , to_column=\"DateKey\"       , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            tom.add_relationship(from_table=\"fact_myevents_1bln_partitioned_datekey\", from_column=\"GeographyID\" , to_table=\"dim_Geography\"  , to_column=\"GeographyID\"   , from_cardinality=\"Many\" , to_cardinality=\"One\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding relationships... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba5536-018c-402b-aa0f-663ee5e8d07f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 8: Add Measures\n",
    "\n",
    "Create DAX measures on each fact table for use in performance testing queries later in this lab.\n",
    "\n",
    "---\n",
    "\n",
    "*Erstellen Sie DAX-Measures fuer jede Faktentabelle zur Verwendung in Leistungstestabfragen spaeter in diesem Lab.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666631c-4221-4638-b8ff-10fee8a4f7df",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            #1. Remove any existing measures\n",
    "            for t in tom.model.Tables:\n",
    "                for m in t.Measures:\n",
    "                    tom.remove_object(m)\n",
    "                    print(m.Name)\n",
    "\n",
    "            tom.add_measure(table_name=\"fact_myevents_2bln\",measure_name=\"Sum of Sales (2bln)\",expression=\"SUM(fact_myevents_2bln[Quantity_ThisYear])\",format_string=\"#,0\")\n",
    "            tom.add_measure(table_name=\"fact_myevents_1bln\",measure_name=\"Sum of Sales (1bln)\",expression=\"SUM(fact_myevents_1bln[Quantity_ThisYear])\",format_string=\"#,0\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error adding measures... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ff91f-0362-4c3e-bf2e-60c64867cec8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 9: Configure Date Table\n",
    "\n",
    "Mark dim_Date as a date table to enable time intelligence functions for filtering and aggregating billion-row tables by date.\n",
    "\n",
    "---\n",
    "\n",
    "*Markieren Sie dim_Date als Datumstabelle, um Zeitintelligenzfunktionen zum Filtern und Aggregieren von Milliarden-Zeilen-Tabellen nach Datum zu aktivieren.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f48bf-893b-4a8f-92a1-e5e80b968840",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        with labs.tom.connect_semantic_model(dataset=SemanticModelName, readonly=False) as tom:\n",
    "            tom.mark_as_date_table(table_name=\"dim_Date\",column_name=\"DateKey\")\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error with date table... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb148df-8490-47fc-8e9c-6b8cbcb5fb6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 10: Configure Column Sorting\n",
    "\n",
    "Set sort-by-column properties on date dimension columns to ensure chronological ordering in visualisations.\n",
    "\n",
    "---\n",
    "\n",
    "*Legen Sie Sortierungseigenschaften fuer Datumsdimensionsspalten fest, um die chronologische Reihenfolge in Visualisierungen sicherzustellen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e3959-51e3-481c-b730-bde8482bd746",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        tom = labs.tom.TOMWrapper(dataset=SemanticModelName, workspace=workspaceName, readonly=False)\n",
    "        tom.set_sort_by_column(table_name=\"dim_Date\",column_name=\"MonthName\"       ,sort_by_column=\"Month\")\n",
    "        tom.set_sort_by_column(table_name=\"dim_Date\",column_name=\"WeekDayName\"     ,sort_by_column=\"Weekday\")\n",
    "        tom.model.SaveChanges()\n",
    "\n",
    "        #Show BIM data for dim_Date table\n",
    "        i:int=0\n",
    "        for t in tom.model.Tables:\n",
    "            if t.Name==\"dim_Date\":\n",
    "                bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "                print(bim)\n",
    "            i=i+1\n",
    "            completedOK=True\n",
    "    except:\n",
    "        print('Error with sort by cols... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7babd-5053-4dc5-97e1-4398e67dba26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 11: Hide Fact Table Columns\n",
    "\n",
    "Hide raw columns on the fact tables so that report authors are guided towards using the defined measures.\n",
    "\n",
    "---\n",
    "\n",
    "*Blenden Sie Rohspalten der Faktentabellen aus, damit Berichtsautoren die definierten Measures verwenden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e49ec-bd01-467f-ad6f-5bff8f068fcc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "completedOK:bool=False\n",
    "while not completedOK:\n",
    "    try:\n",
    "        i:int=0\n",
    "        for t in tom.model.Tables:\n",
    "            if t.Name in [\"fact_myevents_1bln\",\"fact_myevents_2bln\",\"fact_myevents_1bln_partitioned_datekey\"]:\n",
    "                for c in t.Columns:\n",
    "                    c.IsHidden=True\n",
    "\n",
    "                bim = json.dumps(tom.get_bim()[\"model\"][\"tables\"][i],indent=4)\n",
    "                print(bim)\n",
    "            i=i+1\n",
    "        tom.model.SaveChanges()\n",
    "        completedOK=True\n",
    "    except:\n",
    "        print('Error with hiding cols... trying again.')\n",
    "        time.sleep(3)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc341d-8682-4a4d-8b00-300d8723bcfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 12: Frame the Model\n",
    "\n",
    "Trigger framing to prepare the semantic model to serve queries against the billion-row tables.\n",
    "\n",
    "---\n",
    "\n",
    "*Loesen Sie das Framing aus, um das Semantic Model fuer die Abfrage der Milliarden-Zeilen-Tabellen vorzubereiten.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed6756-a8b0-4250-9062-3266ae563054",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "reframeOK:bool=False\n",
    "while not reframeOK:\n",
    "    try:\n",
    "        result:pandas.DataFrame = labs.refresh_semantic_model(dataset=SemanticModelName)\n",
    "        reframeOK=True\n",
    "    except:\n",
    "        print('Error with reframe... trying again.')\n",
    "        triggerMetadataRefresh()\n",
    "        time.sleep(3)\n",
    "\n",
    "print('Custom Semantic Model reframe OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f25b5-949d-4794-b565-f4d8fea1c089",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 13: Create Tracing Helper Functions\n",
    "\n",
    "Define functions for capturing trace events during query execution. These will be used to detect Direct Lake mode, SQL Endpoint fallback, and query timing.\n",
    "\n",
    "---\n",
    "\n",
    "*Definieren Sie Funktionen zum Erfassen von Trace-Ereignissen waehrend der Abfrageausfuehrung. Diese werden verwendet, um Direct Lake-Modus, SQL Endpoint-Fallback und Abfragezeiten zu erkennen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab34d2-784f-4a4a-bf61-afdabb4c2b69",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from Microsoft.AnalysisServices.Tabular import TraceEventArgs\n",
    "from typing import Dict, List, Optional, Callable\n",
    "\n",
    "\n",
    "#### Generate Unique Trace Name - Start ####\n",
    "import json, base64, re\n",
    "token = notebookutils.credentials.getToken(\"pbi\")\n",
    "payload = token.split(\".\")[1]\n",
    "payload += \"=\" * (4 - len(payload) % 4)\n",
    "upn = json.loads(base64.b64decode(payload)).get(\"upn\")\n",
    "\n",
    "# Extract just the user part (e.g. \"SQLKDL.user39\")\n",
    "user_id = upn.split(\"@\")[0]\n",
    "lab_number = 2  # set per lab\n",
    "\n",
    "# Remove characters not allowed in trace names: . , ; ' ` : / \\ * | ? \" & % $ ! + = ( ) [ ] { } < >\n",
    "user_id_clean = re.sub(r\"[.,;'`:/\\\\*|?\\\"&%$!+=(){}\\[\\]<>]\", \"_\", user_id)\n",
    "trace_name = f\"Lab{lab_number}_{user_id_clean}\"\n",
    "#### Generate Unique Trace Name - End ####\n",
    "\n",
    "\n",
    "def runDMV():\n",
    "    df = sempy.fabric.evaluate_dax(\n",
    "        dataset=SemanticModelName, \n",
    "        dax_string=\"\"\"\n",
    "        \n",
    "        SELECT \n",
    "            MEASURE_GROUP_NAME AS [TABLE],\n",
    "            ATTRIBUTE_NAME AS [COLUMN],\n",
    "            DATATYPE ,\n",
    "            DICTIONARY_SIZE \t\t    AS SIZE ,\n",
    "            DICTIONARY_ISPAGEABLE \t\tAS PAGEABLE ,\n",
    "            DICTIONARY_ISRESIDENT\t\tAS RESIDENT ,\n",
    "            DICTIONARY_TEMPERATURE\t\tAS TEMPERATURE,\n",
    "            DICTIONARY_LAST_ACCESSED\tAS LASTACCESSED \n",
    "        FROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMNS \n",
    "        ORDER BY \n",
    "            [DICTIONARY_TEMPERATURE] DESC\n",
    "        \n",
    "        \"\"\")\n",
    "    display(df)\n",
    "\n",
    "def filter_func(e):\n",
    "    retVal:bool=True\n",
    "    if e.EventSubclass.ToString() == \"VertiPaqScanInternal\":\n",
    "        retVal=False      \n",
    "    #     #if e.EventSubClass.ToString() == \"VertiPaqScanInternal\":\n",
    "    #     retVal=False\n",
    "    return retVal\n",
    "\n",
    "# define events to trace and their corresponding columns\n",
    "def runQueryWithTrace (expr:str,workspaceName:str,SemanticModelName:str,Result:Optional[bool]=True,Trace:Optional[bool]=True,DMV:Optional[bool]=True,ClearCache:Optional[bool]=True) -> pandas.DataFrame :\n",
    "    event_schema = fabric.Trace.get_default_query_trace_schema()\n",
    "    event_schema.update({\"ExecutionMetrics\":[\"EventClass\",\"TextData\"]})\n",
    "    del event_schema['VertiPaqSEQueryBegin']\n",
    "    del event_schema['VertiPaqSEQueryCacheMatch']\n",
    "    del event_schema['DirectQueryBegin']\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    WorkspaceName = workspaceName\n",
    "    SemanticModelName = SemanticModelName\n",
    "\n",
    "    if ClearCache:\n",
    "        labs.clear_cache(SemanticModelName)\n",
    "\n",
    "    with fabric.create_trace_connection(SemanticModelName,WorkspaceName) as trace_connection:\n",
    "        # create trace on server with specified events\n",
    "        with trace_connection.create_trace(\n",
    "            event_schema=event_schema, \n",
    "            name=trace_name,\n",
    "            filter_predicate=filter_func,\n",
    "            stop_event=\"QueryEnd\"\n",
    "            ) as trace:\n",
    "\n",
    "            trace.start()\n",
    "\n",
    "            df=sempy.fabric.evaluate_dax(\n",
    "                dataset=SemanticModelName, \n",
    "                dax_string=expr)\n",
    "\n",
    "            if Result:\n",
    "                displayHTML(f\"<H2>####### DAX QUERY RESULT #######</H2>\")\n",
    "                display(df)\n",
    "\n",
    "            # Wait 5 seconds for trace data to arrive\n",
    "            time.sleep(5)\n",
    "\n",
    "            # stop Trace and collect logs\n",
    "            final_trace_logs = trace.stop()\n",
    "\n",
    "    if Trace:\n",
    "        displayHTML(f\"<H2>####### SERVER TIMINGS #######</H2>\")\n",
    "        display(final_trace_logs)\n",
    "    \n",
    "    if DMV:\n",
    "        displayHTML(f\"<H2>####### SHOW DMV RESULTS #######</H2>\")\n",
    "        runDMV()\n",
    "    \n",
    "    return final_trace_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4e8d4",
   "metadata": {},
   "source": [
    "## Step 14: Review Table Traits and Guardrails\n",
    "\n",
    "Validate that the model is in Direct Lake mode and display the guardrail limits for the current capacity SKU. This helps set expectations for which queries will succeed in Direct Lake mode and which will fall back.\n",
    "\n",
    "---\n",
    "\n",
    "*Validieren Sie, dass das Modell im Direct Lake-Modus ist, und zeigen Sie die Guardrail-Grenzen fuer die aktuelle Kapazitaets-SKU an. Dies hilft bei der Einschaetzung, welche Abfragen im Direct Lake-Modus erfolgreich sind und welche zurueckfallen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf138f4-df36-41c4-bb58-a6af71edd179",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=sempy.fabric.evaluate_dax(\n",
    "    dataset=SemanticModelName, \n",
    "    dax_string=\"\"\"\n",
    "    \n",
    "    evaluate tabletraits()\n",
    "    \n",
    "    \"\"\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3394566-f8a8-4ee3-885b-766db5e8a615",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df=labs.directlake.get_direct_lake_guardrails()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae67a8-b41c-4c18-8898-aaf4e478e599",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 15: Establish Column Residency Baseline\n",
    "\n",
    "Capture the current column loading states and memory usage as a baseline before running billion-row queries.\n",
    "\n",
    "---\n",
    "\n",
    "*Erfassen Sie die aktuellen Spaltenladezustaende und die Speichernutzung als Basislinie, bevor Milliarden-Zeilen-Abfragen ausgefuehrt werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223acea6-6ac3-4f3a-b04d-603309daf706",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "runDMV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2dcd3-fe61-4466-b7dd-f746bac1e05a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 16: Execute Billion-Row Queries with Tracing\n",
    "\n",
    "Run a series of queries against the billion-row tables with trace monitoring enabled. The trace output reveals whether each query ran in Direct Lake mode or fell back to the SQL Endpoint.\n",
    "\n",
    "---\n",
    "\n",
    "*Fuehren Sie eine Reihe von Abfragen gegen die Milliarden-Zeilen-Tabellen mit aktivierter Trace-Ueberwachung aus. Die Trace-Ausgabe zeigt, ob jede Abfrage im Direct Lake-Modus oder ueber den SQL Endpoint ausgefuehrt wurde.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235fe7b5-d8b4-4ed6-9874-6875962859b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Step 16.1: Query the 1 Billion Row Table\n",
    "\n",
    "Run a baseline query against the 1 billion row fact table to observe Direct Lake performance at scale.\n",
    "\n",
    "---\n",
    "\n",
    "*Fuehren Sie eine Basisabfrage gegen die 1-Milliarden-Zeilen-Faktentabelle aus, um die Direct Lake-Leistung im grossen Massstab zu beobachten.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbd807-3013-40a4-b584-fda3428ab4be",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "    \n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "               \n",
    "                dim_Date[FirstDateofMonth] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(fact_myevents_1bln) ,\n",
    "                \"Sum of Sales\" , [Sum of Sales (1bln)] \n",
    "        )\n",
    "        ORDER BY [FirstDateofMonth]\n",
    "\n",
    "\"\"\",workspaceName,SemanticModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4efa57f-c533-4c4b-ab38-a982e962640f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Step 16.2: Query the 2 Billion Row Table\n",
    "\n",
    "Run the same query pattern against the 2 billion row table to observe whether Direct Lake falls back to the SQL Endpoint.\n",
    "\n",
    "---\n",
    "\n",
    "*Fuehren Sie dasselbe Abfragemuster gegen die 2-Milliarden-Zeilen-Tabelle aus, um zu beobachten, ob Direct Lake zum SQL Endpoint zurueckfaellt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4111c0-e717-49c7-9bdc-3d4a96ea96b9",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "                dim_Date[FirstDateofMonth] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(fact_myevents_2bln) ,\n",
    "                \"Sum of Sales\" , [Sum of Sales (2bln)]\n",
    "        )\n",
    "        ORDER BY [FirstDateofMonth]\n",
    "\n",
    "\"\"\",workspaceName,SemanticModelName,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2391da99-a7d1-417c-86cb-600cd7ae92e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Step 16.3: Cross-Table Query Combining Both Billion-Row Tables\n",
    "\n",
    "Run a query that references both billion-row tables simultaneously to test Direct Lake under maximum load.\n",
    "\n",
    "---\n",
    "\n",
    "*Fuehren Sie eine Abfrage aus, die beide Milliarden-Zeilen-Tabellen gleichzeitig referenziert, um Direct Lake unter maximaler Last zu testen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b3970-3eb3-46ae-a3e3-94ad6072c682",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df = runQueryWithTrace(\"\"\"\n",
    "\n",
    "    EVALUATE\n",
    "        SUMMARIZECOLUMNS(\n",
    "                dim_Date[FirstDateofMonth] ,\n",
    "                \"Count of Transactions\" , COUNTROWS(fact_myevents_1bln) ,\n",
    "                \"Sum of Sales (1bln)\" , [Sum of Sales (1bln)] ,\n",
    "                \"Sum of Sales (2bln)\" , [Sum of Sales (2bln)]\n",
    "        )\n",
    "        ORDER BY [FirstDateofMonth]\n",
    "\n",
    "\"\"\",workspaceName,SemanticModelName,DMV=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69f4342",
   "metadata": {},
   "source": [
    "## Step 17: Stop the Spark Session\n",
    "\n",
    "---\n",
    "\n",
    "*Spark-Sitzung beenden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463abb6d",
   "metadata": {},
   "source": [
    "## Lab 2 Summary\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "- **Cross-workspace data access:** Created OneLake shortcuts to billion-row tables without duplicating storage\n",
    "- **Billion-row semantic model:** Built and configured a Direct Lake model over 1 and 2 billion row fact tables\n",
    "- **Performance tracing:** Used trace events to monitor query execution mode and timing\n",
    "- **Fallback observation:** Identified conditions under which Direct Lake falls back to the SQL Endpoint\n",
    "- **Stress testing:** Ran queries combining multiple billion-row tables to test system limits\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Direct Lake can handle billion-row tables when the data fits within the capacity's guardrails\n",
    "- OneLake shortcuts enable zero-copy access to data across workspaces\n",
    "- When a query exceeds guardrail limits, Direct Lake automatically falls back to the SQL Endpoint to ensure the query still completes\n",
    "- Trace events provide clear visibility into whether a query ran in Direct Lake mode or via fallback\n",
    "\n",
    "### Next Lab\n",
    "\n",
    "Continue to **Lab 3** to analyse Delta table structure using the Delta Analyzer tool.\n",
    "\n",
    "---\n",
    "\n",
    "*Deutsche Version:*\n",
    "\n",
    "### Was Sie erreicht haben\n",
    "\n",
    "- **Workspace-uebergreifender Datenzugriff:** OneLake-Shortcuts zu Milliarden-Zeilen-Tabellen ohne Speicherduplizierung erstellt\n",
    "- **Milliarden-Zeilen Semantic Model:** Ein Direct Lake-Modell ueber 1- und 2-Milliarden-Zeilen-Faktentabellen erstellt und konfiguriert\n",
    "- **Leistungs-Tracing:** Trace-Ereignisse zur Ueberwachung des Abfrageausfuehrungsmodus und der Zeitmessung verwendet\n",
    "- **Fallback-Beobachtung:** Bedingungen identifiziert, unter denen Direct Lake zum SQL Endpoint zurueckfaellt\n",
    "- **Stresstests:** Abfragen ueber mehrere Milliarden-Zeilen-Tabellen ausgefuehrt, um Systemgrenzen zu testen\n",
    "\n",
    "### Wichtige Erkenntnisse\n",
    "\n",
    "- Direct Lake kann Milliarden-Zeilen-Tabellen verarbeiten, wenn die Daten innerhalb der Kapazitaets-Guardrails liegen\n",
    "- OneLake-Shortcuts ermoeglichen kopierlosen Zugriff auf Daten ueber Workspaces hinweg\n",
    "- Bei Ueberschreitung der Guardrail-Grenzen faellt Direct Lake automatisch zum SQL Endpoint zurueck\n",
    "- Trace-Ereignisse bieten klare Sichtbarkeit, ob eine Abfrage im Direct Lake-Modus oder ueber Fallback ausgefuehrt wurde\n",
    "\n",
    "### Naechstes Lab\n",
    "\n",
    "Weiter zu **Lab 3**, um die Delta-Tabellenstruktur mit dem Delta Analyzer zu analysieren."
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
